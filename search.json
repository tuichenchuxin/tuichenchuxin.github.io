[{"categories":null,"content":"背景 在优化全局二级索引的过程中，有一个优化点是如果二级索引的数据量太大，那么带回主表再查可能引起一些问题，例如效率不高，OOM 或者 SQL 超长等问题，因此考虑二级索引表数据量大的时候，直接查询主表。 因此需要查询二级索引表的数据量。 这里考虑能否直接查询二级索引表的同时获取本次查询结果的数据量。\n实践 方式一 通过 ResultSet.TYPE_SCROLL_INSENSITIVE 可以将游标放到最后，然后根据 getRow() 方法获取\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public static void main(String[] args) throws ClassNotFoundException, SQLException { Class.forName(\"com.mysql.jdbc.Driver\"); Connection connection = DriverManager.getConnection(\"jdbc:mysql://127.0.0.1:3306/demo_ds_0?serverTimezone=UTC\u0026useSSL=false\u0026allowPublicKeyRetrieval=true\", \"root\", \"123456\"); // 设置游标可以滚动，默认是 forward only try (PreparedStatement preparedStatement = connection.prepareStatement(\"select * from t_single\", ResultSet.TYPE_SCROLL_INSENSITIVE, ResultSet.CONCUR_READ_ONLY); ResultSet resultSet = preparedStatement.executeQuery()) { int count = 0; // 移动到最后 if (resultSet.last()) { // 获取最后一行的行号 count = resultSet.getRow(); } System.out.println(\"total:\" + count); resultSet.beforeFirst(); int columnIndex = 1; while (resultSet.next()) { System.out.println(resultSet.getObject(columnIndex ++)); } } } 但是为什么之前的很多框架都采用的 count sql，而不用这种方式呢？例如 pageHelper 等 https://cloud.tencent.com/developer/article/1433806 搜索到相关文章，发现 TYPE_SCROLL_INSENSITIVE 会将结果集缓存起来，容易 OOM。那么这种方式可能问题就比较大了。 如果二级索引表查询出的数据量比较大，缓存结果集的风险就比较大了。\nhttps://forums.mysql.com/read.php?39,435275,435275 该主题也在说使用这种resultSet type 会导致性能很差\nHow many rows are we talking about here? In general, it’s -never- a good idea to select more rows than you need, and use fetch hints to scroll the result set. Pretty much every database now has options like MySQL’s LIMIT clause that can (and often will) involve the optimizer to make it so your query examines as few rows as possible.\nThe MySQL protocol itself doesn’t have the notion of fetch batch sizes, and the driver is optimized for OLTP type queries. Leaving large result sets open while a user paginates is very, very sub-optimal because of the locks and other resources that are needlessly taken.\nYou may want to consider re-thinking your overall strategy, because it’s more than likely consuming more resources than you think, even on databases other than MySQL.\n-Mark\nMark Matthews Consulting Member Technical Staff - MySQL Enterprise Tools Oracle http://www.mysql.com/products/enterprise/monitor.html\n看官方的回复也不建议在大数据中使用滚动游标的方式。\n结论 可以在查询结果的同时获取 count，但是性能可能不高，也有占用过多资源的可能。\n","description":"","tags":null,"title":"是否可以在 jdbc 查询数据的同时获取本次查询的数据量？","uri":"/posts/result_set_type/"},{"categories":null,"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 create table ENCRYPT.TEST1 ( A VARCHAR2(200) PRIMARY KEY, B VARCHAR2(200), C VARCHAR2(200) ) / create table ENCRYPT.TEST2 ( A VARCHAR2(200) PRIMARY KEY, B VARCHAR2(200), C VARCHAR2(200) ) / insert into TEST1 values (1,2,3); insert into TEST1 values (2,3,4); insert into TEST2 values (4,2,3); merge into TEST2 fi using TEST1 se on (fi.A = se.A and fi.A = 3) when matched then update set fi.B = se.B, fi.C = se.C when not matched then insert (A, B, C) values (1,2,3); 执行如上 SQL 发现一直报 ORA-00001: unique constraint (ENCRYPT.SYS_C007033) violated\n原来是因为我理解错了 merge insert 的含义。 这是将 TEST2 的数据痛 TEST1 进行比对，如果 TEST1 中的数据满足 ON 的条件，那么会进行 update，否则会进行 insert. 因为 TEST1 中不符合条件的有两条，所以会依次执行 insert (1,2,3) 所以就冲突了。（相当于 insert 执行了两次）\n","description":"","tags":null,"title":"Oracle merge insert 问题","uri":"/posts/oracle_merge/"},{"categories":["Tech"],"content":"背景 用户使用 shardingsphere 发现 No suitable driver 异常。但是用户的 jar 包中是有该驱动的。\n调查 发现如果增加这段代码后，可以正常执行\n1 Class.forName(\"com.mysql.cj.jdbc.Driver\"); 正常而言，按照 JDBC4.0 标准，驱动是通过 spi 机制加载。不需要手动加载。 通过 debug DriverManager 发现其中有个驱动的名称中包含了 - 不是合法的 java 名称。 但是会将异常吞掉，所以没有展示出来。 修复名称后，可以正常使用。\n相关知识 JDBC4.0 是通过 spi 加载。 - 不是合法名称，使用 shade 插件替换名称时要注意。 DriverManager 加载多个驱动过程中，如果有任何一个失败，都不会再继续加载，并不会抛出相关异常。\n","description":"","tags":null,"title":"No suitable driver 问题调查","uri":"/posts/driverclassname/"},{"categories":null,"content":"https://github.com/julianhyde/sqlline sqlline 是一个通过 jdbc 的连接工具。\n下载代码并编译 git clone git://github.com/julianhyde/sqlline.git cd sqlline ./mvnw package 将 shardingsphere-jdbc 打包，之后最好再使用 sharde PLUGIN 打成一个 jar 包\n修改 sqlline bin 目录下的 sqlline 脚本，将相关依赖添加进去包括数据库驱动、shardingsphere shade jar、 sqlline-VERSION-jar-with-dependencies.jar\n#!/bin/bash # sqlline - Script to launch SQL shell on Unix, Linux or Mac OS BINPATH=$(dirname $0) exec java -cp \"$BINPATH/../target/*\":\"/Users/chenchuxin/Documents/sqlline-sqlline-1.12.0/bin/*\" sqlline.SqlLine \"$@\" # End sqlline 使用 sqlline sqlline -d org.apache.shardingsphere.driver.ShardingSphereDriver -u jdbc:shardingsphere:absolutepath:/Users/chenchuxin/Documents/GitHub/sw-test/src/main/resources/META-INF/oracle.yaml ","description":"","tags":null,"title":"使用 sqlline 连接 sharding_jdbc","uri":"/posts/use_sql_line/"},{"categories":["Tech"],"content":"初步了解 opendal 查看 examples 了解到 opendal 是各种数据源的通用连接器\n查看 good first issue 的相关 pr ","description":"","tags":null,"title":"Open_dal","uri":"/posts/open_dal/"},{"categories":["Tech"],"content":"如何才能实现分布式 join？\n有三种方式：\n将数据全部抽取到内存中，进行内存计算。 将数据全部抽取到一个数据源中，直接进行查询。 内存和存储并用。 对于分析型的 SQL 而言，不需要考虑事务和性能的话，可以采用数据挪动的方式来实现。\n如下图所示，列出了可能的实现方式。 该方案的具体实现需要考虑 2 点。\n语义分析 （考虑通过 calcite 或者自行解析提取条件） 获取表之间的依赖关系，例如 A left join B on condition，那么 B 依赖 A, 可以先将 A 抽取出来，再依据 A 来抽取 B 增加条件到表中 支持数据迁移（需要支持数据迁移的管理） ","description":"","tags":null,"title":"分布式 join 实现","uri":"/posts/distributed-join/"},{"categories":["Tech"],"content":"Target 说清楚 calcite 是什么，能做什么 画出 calcite 的大体架构 明白 calcite 是如何跟 SS 整合的 未来 SS 还可以在哪些方面进一步利用 calcite calcite 优化逻辑源码级理解 在 calcite 上提交 pr Process calcite 是什么，能做什么 calcite 是动态数据的管理框架。我理解，就是数据库的计算层，去除掉了存储的部分。 calcite 可以接入任何形式的数据，只需要我们将数据注册成表的形式，那么就可以使用 SQL 来对它进行查询。\ncalcite 的大体架构 calcite 如何跟 SS 整合 ShardingSphere 可以支持分库分表，所以也自然会有跨实例查询的 SQL。 例如跨实例的 join。这些 SQL 是没办法直接下推到数据库去执行的，因此利用 calcite 来实现复杂子查询或者是跨实例 join。 我们可以从一个跨实例 join 的 select debug 来查看整合 calcite 的方式。 1. 执行查询前，会先构造 OptimizerContext，里面包含一些数据库的配置，例如 caseSensitive -\u003e false conformance -\u003e MYSQL_5 timeZone -\u003e UTC lex -\u003e MYSQL fun -\u003e mysql 以及一些默认的规则 HepPlanner\n注册 schema 将表转化为 translatable table 创建 CalciteCatalogReader 对象（参数： rootSchema, schemaName, RelDataTypeFactory, CalciteConnectionConfig） 创建 SqlValidator 创建 SqlToRelConverter 拿到之前的 RelOptPlanner （HepPlanner） 将 shardingsphere 的 sqlstatement 转化成 SqlNode 利用 SqlToRelConverter 将 sqlNode 转化为 RelNode 然后分别进行RBO 和 CBO 优化 将优化结果与 dataContext 绑定 在 FilterableTableScanExecutor 执行下推逻辑，下推到各个逻辑库上执行。 将结果集封装到 ResultSet 中 同普通查询后续流程一致 如何规划执行？默认每个表下推一次吗？ 从目前代码的 TranslatableTableScanExecutor 来看，确实是单表的下推。\n未来 SS 还可以在哪些方面进一步利用 calcite 内存的优化，目前全部都是抽取到内存中，大数据会 OOM 自定义算子的实现 利用数据的收集做代价优化 calcite 优化逻辑源码级理解 关系代数的理解 https://blog.csdn.net/QuinnNorris/article/details/70739094 ","description":"","tags":null,"title":"calcite 学习","uri":"/posts/learn_calcite/"},{"categories":["Tech"],"content":"","description":"","tags":null,"title":"Learnhadoop","uri":"/posts/learnhadoop/"},{"categories":["Tech"],"content":"","description":"","tags":null,"title":"LearnSpark","uri":"/posts/learnspark/"},{"categories":null,"content":"文档信息提取 https://developer.aliyun.com/article/783392 本文首先介绍了 MySQL 的 join 实现，包括以下方式\nNested-Loop Join (NL Join) Batched Key Access Join (BKA Join) Block Nested-Loop Join（版本 \u003c 8.0.20） Hash Join (版本 \u003e= 8.0.18) polardb-x 支持多种 join 方式，包括 Lookup Join、Nested-Loop Join、Hash Join、Sort-Merge Join 等，本文主要讲的是 Lookup Join\nlookup join Lookup Join 的执行过程如下（非索引回表情形）：\n从驱动侧拉取一批数据。通常情况下数据量不会很多，如果数据较多，那么每个批的大小受到 lookup 端的分片数量以及是否可以进行分片裁剪限制。批大小的选择会直接影响查询性能，如果批特别小会导致 RPC 次数太高，批太大则会导致内存中暂存的数据量膨胀，高并发情况下可能导致 OOM。默认情况下我们尽可能让每个分片平均查询 50 个值、最多不超过 300 个值。 计算 batch 内每行数据所在分片，由于 lookup 侧是一个分区表，驱动表的每行数据要 lookup 的数据位于不同的分区中。只有包含数据的分片才需要参与 Join，如果没有任何值被路由到某个分片上，那么这个分片也无需被 Lookup。 并发请求所有需要 lookup 的分片，并将查到的数据行以 Join Key 为 Key 构建成哈希表，缓存在内存中。 类似于 Hash Join，利用哈希表为驱动侧的每行找到与其 Join 的行，取决于 Join 类型，可能 Join 出 0 行、1 行或多行。 1 2 3 4 /* Query 1 */ SELECT o_orderkey, o_custkey, c_name FROM orders JOIN customer ON o_cutkey = c_custkey WHERE o_orderkey BETWEEN 1001 AND 1005 依赖全局索引的 Join 则更为复杂一些，回忆下 MySQL 的 BKA Join，需要进行两次 lookup：\n第一次用 Join key 查询全局索引表（用于 Join） 第二次用全局索引表中的主键查询主表（用于索引回表） 将回表结果以 PK 为 key 构建哈希表，与2中的查询结果 Join，得到完整的 Join 右侧数据 将完整的 Join 右侧数据以 Join Key 为 key 构建哈希表，与 1 的数据 Join，得到最终 Join 结果 1 2 3 4 /* Query 2 */ SELECT c_name, c_custkey, o_orderkey, o_totalprice FROM customer JOIN orders ON c_cutkey = o_custkey WHERE c_custkey BETWEEN 13 AND 15 文档信息提取 https://help.aliyun.com/document_detail/316601.html\nNested-Loop Join (NLJoin) 一般是含有不等式条件场景下会使用 流程如下：\n拉取内表（右表，通常是数据量较小的一边）的全部数据，缓存到内存中。 遍历外表数据，对于外表的每行： 对于每一条缓存在内存中的内表数据。 构造结果行，并检查是否满足JOIN条件，如果满足条件则输出。 支持通过 hint 来定义顺序\nHash Join Hash Join是等值JOIN最常用的算法之一。它的原理如下所示：\n拉取内表（右表，通常是数据量较小的一边）的全部数据，写进内存中的哈希表。 遍历外表数据，对于外表的每行： 根据等值条件JOIN Key查询哈希表，取出0-N匹配的行（JOIN Key相同）。 构造结果行，并检查是否满足JOIN条件，如果满足条件则输出。 当索引无法满足，不能使用 lookup join 的时候，可以使用 hash join\nSort-Merge Join Sort-Merge Join是另一种等值JOIN算法，它依赖左右两边输入的顺序，必须按JOIN Key排序。它的原理如下：\n开始Sort-Merge Join之前，输入端必须排序（借助MergeSort或MemSort）。 比较当前左右表输入的行，并按以下方式操作，不断消费左右两边的输入： 如果左表的JOIN Key较小，则消费左表的下一条数据。 如果右表的JOIN Key较小，则消费右表的下一条数据。 如果左右表JOIN Key相等，说明获得了1条或多条匹配，检查是否满足JOIN条件并输出。 关于 Join 顺序以及各种 join 的使用场景 文档信息提取 https://zhuanlan.zhihu.com/p/379967662\n面对 OLTP 场景，可以使用大表和小表进行 join ，使用前文提到的 Lookup Join 算法 但是对于 OLAP 的场景，我们可能就需要将数据从内存拉出来进行计算。\n等值 JOIN 那么一般就是 hash-join 和 sort-merge join 两种，Hash join 根据是否支持大数据量，又分为 内存 Hash-join 和 落盘版的 Hash-join\n内存版 Hash join 的实现 根据统计信息选取较小的表根据 join 条件对给定的列 build hash table，然后流式遍历，去 hash table 里面找 key，如果一致，那么就拿出一整行，然后直接向下游输出，这个过程又叫 probe table\n//build Table for row in t1: hashValue = hash_func(row) put (hashValue, row) into hash-table; //probe Table for row in t2: hashValue = hash_func(row) t1_row = lookup from hash-table if (t1_row != null) { join(t1_row, row) } polardb-x 与一般的 hash join 最大的不同，是采用了内存友好的 vector 重新实现了哈希表，对 CPU CACHE 更友好，可以提高 JOIN 性能。 具体实现没太明白。。。大概是搞了一些缓存索引结构？\n如果数据量太大，无法全部放到内存中的时候，那么就需要使用 HybridHashJoin 算法。\nHybirdHashJoin Hybird hash join 从我的角度理解，就是将原表按照关联的键做 partition，然后分别在每个分区做 hash join. SortMergeJoin sort t1, sort t2 R1 = t1.next() R2 = t2.next() while (R1 != null \u0026\u0026 R2 != null) { if R1 joins with R2 output (R1, R2) else if R1 \u003c R2 R1 = t1.next() else R2 = t2.next() } 这种场景一般外排序会比较消耗 IO，一般 hash join.不过某些极端场景下， hash join 分桶之后，数据量还是超限，那么这种场景可能 sort merge join 就比较优势了。\nShuffle join TODO\n","description":"","tags":null,"title":"Polardb-x 的 join 实现","uri":"/posts/polardbxjoin/"},{"categories":["Tech"],"content":"分布式 join 调研 Citus Data Citus Data 是一个开源的分布式数据库管理系统，它是基于 PostgreSQL 架构之上，能够允许数据库在多个服务器之间进行分布式运算，以便应对大型数据处理和高流量负载的需求。利用 Citus Data 可以为 PostgreSQL 提供横向扩展的能力，使其可以更好地应对应用中的大量数据请求，具有较高的性能和可扩展性。Citus Data 的特点还包括良好的可用性、容错性、可管理性和可扩展性。Citrus Data 可以作为云端数据库的解决方案提供，同时也适用于在本地运行的企业数据库和分析场景中。\n分布式 join 支持方式（数据复制） 单表 join 分表\n下图介绍了 单表和 sharding 表 join 时 citus 的处理方式\n它两种方式来对 local 表和 distributed 表进行关联查询\n将 distributed 表的数据从 workers 节点移动到协调器 将 local 表数据从协调器移动到 workers 节点 两种方式的选择有多种模式可以配置，例如自动模式： 因为一般来说 distributed 表的数据量比较大，所以只有当满足以下条件时才会将数据从 distributed 移动到 local 中\n分布式表包含唯一键，如主键。 唯一键包含一个常量等式过滤器，可以直接或通过传递性实现。 具体模式如下：\nAuto (default): Distributed table will be moved to coordinator if the distributed table contains a constant equality filter on a unique column, which ensures less data movement from workers to the coordinator. If not, then the local table will be moved from coordinator to workers. Prefer-local: The filtered data of local table will be moved to the workers from the coordinator, then the JOIN will be executed on the workers. Prefer-distributed: The filtered data of the distributed table will be moved to the coordinator from the workers, then the JOIN will be executed on the coordinator. Never: Citus will give an error for local and distributed table JOINs, same as before Citus 10. 分表 join 分表\n这里展示的是表的分片数量相同，并且关联条件是分片键的场景，类似于 binding table\n该图展示了 citus 的 join 方式，有本地 join，refrence table join （就是广播表），还有 repartition joins，里面提到会使用数据混洗的方式，性能不高。另外这个方式需要开关打开。\n根据作者对于 repartition joins https://github.com/citusdata/citus/issues/2321 的回复来看，目前该功能只能用于某些场景并且不建议默认打开使用。\nVitess vitess 是可扩展的兼容 mysql 的云原生数据库。\n分布式 join 支持方式（内存计算 + 数据复制） 没有找到官方明确的说法，但是从一些公开的演讲中发现了一些信息。 它实现分布式 join 的方式也是两种，一种是在内存中计算，另外一种就是数据复制，不过它的复制是通过 MySQL binlog。\nhttps://www.infoq.com/presentations/vitess/ https://vitess.io/docs/16.0/reference/vreplication/vreplication/\nPresto Presto 是一个分布式 SQL 查询引擎，可以在云计算和大数据领域广泛应用。Presto 主要用于处理大数据查询和分析，它支持从多个数据源中进行高速查询，包括 Hadoop，MySQL，Cassandra，PostgreSQL 等。 Presto 的设计目的是为了在处理大量数据时提供快速的查询和分析功能。Presto 支持 SQL 查询和复杂的分析，可以处理 PB 级别的数据，并且可以在数秒内返回查询结果。Presto 的一个优点是它可以与多个数据存储系统集成，而不需要将数据迁移到一个中心位置，从而降低数据分析的成本和复杂性。它主要处理 OLAP 场景。\n有点儿像 calcite。\n分布式 join 支持方式（内存计算 + 数据复制） 从上述文档可以看到 presto 是基于代价的查询优化器。一般也是两种方式实现分布式 join\nPartitioned: each node participating in the query builds a hash table from only a fraction of the data Broadcast: each node participating in the query builds a hash table from all of the data (data is replicated to each node) partitioned 方式是输入数据的部分生成 hash 表然后去各个节点上迭代。\nbroadcast 的方式则是基于全部数据生成 hash 表然后 join。\n参考文档 https://www.citusdata.com/blog/2021/07/02/citus-tips-joins-between-local-and-distributed-postgres-tables/\nhttps://docs.citusdata.com/en/stable/articles/outer_joins.html\nhttps://blog.bigdataboutique.com/2020/05/querying-multiple-data-sources-with-a-single-query-using-prestos-query-federation-veulwi\nhttps://prestodb.io/docs/current/optimizer/cost-based-optimizations.html\nhttps://docs.treasuredata.com/display/public/PD/About+Presto+Distributed+Query+Engine\n","description":"","tags":null,"title":"分布式 join 调研","uri":"/posts/join_learn/"},{"categories":["Tech"],"content":"Apache ShardingSphere 元数据介绍 Apache ShardingSphere 的元数据主要包括规则、数据源、表结构等信息。规则信息可能包含分片、加密、读写分离、事务、高可用等。 数据源信息存储的是需要通过 ShardingSphere 来进行管理的底层数据库资源。表结构信息主要就是底层数据源的表结构，包括表的 column 信息、索引信息等。\nApache ShardingSphere 通过这些元数据信息配合治理中心的能力，例如 zookeeper、etcd 的存储和通知能力，可以实现集群内配置的共享和变更，从而实现计算节点的水平扩展。同时元数据信息对于 ShardingSphere 而言也是至关重要的，以表的数据结构为例，ShardingSphere 利用表的数据结构可以对采用了加密规则的 SQL 进行正确的改写，内核中的 federation 引擎也会利用表结构信息进行 SQL 优化。\n既然 ShardingSphere 的元数据如此重要，那么我们该怎么入手了解元数据呢？\nApache ShardingSphere 三层元数据结构 ShardingSphere 的三层元数据结构是个了解元数据信息的很好入口。我们可以启动 ShardingSphere–proxy 的 cluster 模式，这样可以在 zookeeper 中直观的看到 ShardingSphere 的三层元数据结构。 如下结构展示了 ShardingSphere 元数据在 zookeeper 中的结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 governance_ds --metadata (元数据信息) ----sharding_db (逻辑库名称) ------active_version (当前生效的版本) ------versions --------0 ----------data_sources (底层数据库信息) ----------rules (逻辑库的规则信息，例如分片规则，加密规则等) ------schemas (表、视图信息) --------sharding_db ----------tables ------------t_order ------------t_single ----------views ----shardingsphere (内置元数据库) ------schemas --------shardingsphere ----------tables ------------sharding_table_statics (分片统计信息表) ------------cluster_information （版本信息） ----performance_schema (模拟 mysql 数据库) ------schemas --------performance_schema ----------tables ------------accounts ----information_schema (模拟 mysql 数据库) ------schemas --------information_schema ----------tables ------------tables ------------schemata ------------columns ------------engines ------------routines ------------parameters ------------views ----mysql ----sys --sys_data (内置元数据库的具体行信息) ----shardingsphere ------schemas --------shardingsphere ----------tables ------------sharding_table_statistics --------------79ff60bc40ab09395bed54cfecd08f94 --------------e832393209c9a4e7e117664c5ff8fc61 ------------cluster_information --------------d387c4f7de791e34d206f7dd59e24c1c 上图展示了 ShardingSphere 的元数据结构信息，内容非常丰富，不过我们也不用担心，只要了解了大的节点信息，就可以根据逻辑推测出其它相关信息。\n我们首先关注下 metadata 节点，下方的 active_version 中展示了当前生效的元数据版本，然后在 versions 节点下我们可以找到生效的版本 0 ， 在这个生效版本节点下存储了规则和数据库的连接信息。 schemas 节点下存储了该逻辑库下的表和视图的具体信息，值得一提的是，ShardingSphere 存储的表结构信息是经过规则装饰后的表结构信息，例如分片表只会根据其中一片真实表获取结构，然后替换表名称，如果是加密规则，也不会在表结构中展示真实的加密列信息。这样做的目的是为了让用户完全面向逻辑库来进行相关操作。\n对于 metadata 下的 shardingsphere 节点，该节点也跟逻辑库的结构类似，只不过这里存储的是 ShardingSphere 内置的一些表结构，例如 sharding_table_statics （分片统计信息表）、cluster_information （版本信息表）等。这一块内容后面的内置元数据库会再进一步展开讲讲。\n至于 performance_schema、information_schema、mysql、sys 等节点都是用来模拟 mysql 的数据字典而建立的。当然如果用户的前端协议是 PostgreSQL， 那么这些数据字典也会变更为 PostgreSQL 的数据字典。目前数据字典这边主要用于支持各种客户端工具直接连接 proxy，未来会进一步增加数据收集从而支持对于这些数据字典的查询。这一块也会在后面的元数据库内容中做进一步介绍。\n可能有同学看到我们的数据结构会比较好奇，为什么在 sharding_db 的逻辑库下还有一层 sharding_db 呢？为什么不直接把 tables 节点放置在 sharding_db 下呢？其实这也就是 ShardingSphere 三层元数据结构的由来，ShardingSphere 是一个数据库的上层平台，因此需要兼容多种数据库格式，MySQL 的确是两层结构的，但是对于 PostgreSQL 而言，它是三层结构的，由 实例 和 database 以及 schema 组成，因此为了兼容性，ShardingSphere 使用了三层数据库结构，对于 MySQL，ShardingSphere 增加了一个相同的逻辑 schema 层，从而保证逻辑的统一性。\n在了解了 metadata 的结构之后，相信细心的同学也发现了 sys_data 节点。那么这个节点的作用是什么呢？让我们走进 ShardingSphere 的内置元数据库。\nApache ShardingSphere 内置元数据库 内置元数据库简介 内置元数据库是什么？其实我们从上面的节点中就能猜出来，在 metadata 节点下有一个 shardingsphere 节点，该节点下目前存在着两张表，sharding_table_statics （分片信息收集表），cluster_information （版本信息表）。可能有同学会说他们都是 ShardingSphere 内部信息的收集表，的确 ShardingSphere 内置元数据库的设计目标之一就是存储内部收集信息服务于功能和用户，另外，内置元数据库还可以用来存储用户设置的信息（暂未实现）。\n说了这么多，肯定有同学好奇，sys_data 节点的作用是什么。ShardingSphere 将表的结构信息存储在 metadata 中，表的行信息存储在 sys_data 中。我们可以看到 sys_data 节点下的多个 id，其实他们就是该表的每一行，具体行的内容就存储在该节点之下。\n通过在 metadata 中存储表结构，在 sys_data 中存储表的内容，我们就可以实现通过 sql 直接查询内置元数据库表的信息了。例如我们可以通过如下 SQL 直接查询出当前的版本信息。\nmysql\u003e select * from shardingsphere.cluster_information; +----------------+ | version | +----------------+ | 5.3.2-SNAPSHOT | +----------------+ 1 row in set (3.35 sec) 我们还可以直接查询出某个真实表的统计信息。\nmysql\u003e select * from shardingsphere.sharding_table_statistics where actual_table_name = \"t_order_1\"\\G *************************** 1. row *************************** id: 2 logic_database_name: sharding_db logic_table_name: t_order actual_database_name: ds_0 actual_table_name: t_order_1 row_count: 0 size: 16384 *************************** 2. row *************************** id: 4 logic_database_name: sharding_db logic_table_name: t_order actual_database_name: ds_1 actual_table_name: t_order_1 row_count: 0 size: 16384 2 rows in set (0.39 sec) 那么元数据库的功能是如何实现的呢？\n内置数据库的工作原理 内置元数据库功能的实现主要依赖两个方面，一是数据收集，二是查询实现。\n数据收集主要需要考虑如何将数据采集到内存，同时还需要考虑如何将内存信息同步到治理中心来保证集群间的同步。如何将数据采集到内存中，以 sharding_table_statistics 表的数据采集为例。从 ShardingSphereDataCollector 接口出发，我们看到该类有一个数据收集的方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * ShardingSphere data collector. */ @SingletonSPI public interface ShardingSphereDataCollector extends TypedSPI { /** * Collect. * * @param databaseName database name * @param table table * @param shardingSphereDatabases ShardingSphere databases * @return ShardingSphere table data * @throws SQLException sql exception */ Optional\u003cShardingSphereTableData\u003e collect(String databaseName, ShardingSphereTable table, Map\u003cString, ShardingSphereDatabase\u003e shardingSphereDatabases) throws SQLException; 查询该接口的调用方，我们可以看到是 ShardingSphereDataCollectorRunnable 定时任务在调用。没错，当前的实现方式是在 proxy 端启动定时任务来进行数据收集，根据内置元数据表来区分不同的数据收集器进行数据采集。未来会根据社区用户反馈，可能将这一部分做成 e-job 触发的方式来进行收集。 ShardingStatisticsTableCollector 类中具体展示了收集信息的逻辑。主要就是利用底层数据源和分片规则去查询数据库信息从而获取统计信息。\n在数据收集完成后，ShardingSphereDataScheduleCollector 类会根据收集到的信息跟内存中的信息做比对，如果发现不一致，那么会通过 EVENTBUS 发送事件给治理中心模块，治理中心收到事件后，会更新其它节点的信息，并做内存同步。监听事件类的代码如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /** * ShardingSphere schema data registry subscriber. */ @SuppressWarnings(\"UnstableApiUsage\") public final class ShardingSphereSchemaDataRegistrySubscriber { private final ShardingSphereDataPersistService persistService; private final GlobalLockPersistService lockPersistService; public ShardingSphereSchemaDataRegistrySubscriber(final ClusterPersistRepository repository, final GlobalLockPersistService globalLockPersistService, final EventBusContext eventBusContext) { persistService = new ShardingSphereDataPersistService(repository); lockPersistService = globalLockPersistService; eventBusContext.register(this); } /** * Update when ShardingSphere schema data altered. * * @param event schema altered event */ @Subscribe public void update(final ShardingSphereSchemaDataAlteredEvent event) { String databaseName = event.getDatabaseName(); String schemaName = event.getSchemaName(); GlobalLockDefinition lockDefinition = new GlobalLockDefinition(\"sys_data_\" + event.getDatabaseName() + event.getSchemaName() + event.getTableName()); if (lockPersistService.tryLock(lockDefinition, 10_000)) { try { persistService.getTableRowDataPersistService().persist(databaseName, schemaName, event.getTableName(), event.getAddedRows()); persistService.getTableRowDataPersistService().persist(databaseName, schemaName, event.getTableName(), event.getUpdatedRows()); persistService.getTableRowDataPersistService().delete(databaseName, schemaName, event.getTableName(), event.getDeletedRows()); } finally { lockPersistService.unlock(lockDefinition); } } } } 如上述代码所示，在事件接受后，当前节点会更新治理中心的信息，当节点信息发生变更后，依赖 zookeeper/etcd 的通知能力，集群中的其它节点会收到治理中心的变更，并在如下代码中更新自己节点的内存信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 /** * ShardingSphere data changed watcher. */ public final class ShardingSphereDataChangedWatcher implements GovernanceWatcher\u003cGovernanceEvent\u003e { @Override public Collection\u003cString\u003e getWatchingKeys(final String databaseName) { return Collections.singleton(ShardingSphereDataNode.getShardingSphereDataNodePath()); } @Override public Collection\u003cType\u003e getWatchingTypes() { return Arrays.asList(Type.ADDED, Type.UPDATED, Type.DELETED); } @Override public Optional\u003cGovernanceEvent\u003e createGovernanceEvent(final DataChangedEvent event) { if (isDatabaseChanged(event)) { return createDatabaseChangedEvent(event); } if (isSchemaChanged(event)) { return createSchemaChangedEvent(event); } if (isTableChanged(event)) { return createTableChangedEvent(event); } if (isTableRowDataChanged(event)) { return createRowDataChangedEvent(event); } return Optional.empty(); } private boolean isDatabaseChanged(final DataChangedEvent event) { return ShardingSphereDataNode.getDatabaseName(event.getKey()).isPresent(); } private boolean isSchemaChanged(final DataChangedEvent event) { return ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()).isPresent() \u0026\u0026 ShardingSphereDataNode.getSchemaName(event.getKey()).isPresent(); } private boolean isTableChanged(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaNameBySchemaPath(event.getKey()); Optional\u003cString\u003e tableName = ShardingSphereDataNode.getTableName(event.getKey()); return databaseName.isPresent() \u0026\u0026 schemaName.isPresent() \u0026\u0026 tableName.isPresent(); } private boolean isTableRowDataChanged(final DataChangedEvent event) { return ShardingSphereDataNode.getRowUniqueKey(event.getKey()).isPresent(); } private Optional\u003cGovernanceEvent\u003e createDatabaseChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseName(event.getKey()); Preconditions.checkState(databaseName.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType()) { return Optional.of(new DatabaseDataAddedEvent(databaseName.get())); } if (Type.DELETED == event.getType()) { return Optional.of(new DatabaseDataDeletedEvent(databaseName.get())); } return Optional.empty(); } private Optional\u003cGovernanceEvent\u003e createSchemaChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Preconditions.checkState(databaseName.isPresent()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaName(event.getKey()); Preconditions.checkState(schemaName.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType()) { return Optional.of(new SchemaDataAddedEvent(databaseName.get(), schemaName.get())); } if (Type.DELETED == event.getType()) { return Optional.of(new SchemaDataDeletedEvent(databaseName.get(), schemaName.get())); } return Optional.empty(); } private Optional\u003cGovernanceEvent\u003e createTableChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Preconditions.checkState(databaseName.isPresent()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaNameBySchemaPath(event.getKey()); Preconditions.checkState(schemaName.isPresent()); return doCreateTableChangedEvent(event, databaseName.get(), schemaName.get()); } private Optional\u003cGovernanceEvent\u003e doCreateTableChangedEvent(final DataChangedEvent event, final String databaseName, final String schemaName) { Optional\u003cString\u003e tableName = ShardingSphereDataNode.getTableName(event.getKey()); Preconditions.checkState(tableName.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType()) { return Optional.of(new TableDataChangedEvent(databaseName, schemaName, tableName.get(), null)); } if (Type.DELETED == event.getType()) { return Optional.of(new TableDataChangedEvent(databaseName, schemaName, null, tableName.get())); } return Optional.empty(); } private Optional\u003cGovernanceEvent\u003e createRowDataChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Preconditions.checkState(databaseName.isPresent()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaNameBySchemaPath(event.getKey()); Preconditions.checkState(schemaName.isPresent()); Optional\u003cString\u003e tableName = ShardingSphereDataNode.getTableNameByRowPath(event.getKey()); Preconditions.checkState(tableName.isPresent()); Optional\u003cString\u003e rowPath = ShardingSphereDataNode.getRowUniqueKey(event.getKey()); Preconditions.checkState(rowPath.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType() \u0026\u0026 !Strings.isNullOrEmpty(event.getValue())) { YamlShardingSphereRowData yamlShardingSphereRowData = YamlEngine.unmarshal(event.getValue(), YamlShardingSphereRowData.class); return Optional.of(new ShardingSphereRowDataChangedEvent(databaseName.get(), schemaName.get(), tableName.get(), yamlShardingSphereRowData)); } if (Type.DELETED == event.getType()) { return Optional.of(new ShardingSphereRowDataDeletedEvent(databaseName.get(), schemaName.get(), tableName.get(), rowPath.get())); } return Optional.empty(); } } 经过上述流程，我们就完成了元数据库信息的收集。\n那么内置元数据库是如何支持用户查询的呢？其实就是利用了 ShardingSphere 内置的 federation 引擎实现的。federation 引擎借助 calcite 的能力，可以将内存中的数据结构注册到 calcite 上，通过 calcite 将 sql 查询转化为内存的查询，从而实现 SQL 语句直接返回结果。 在 FilterableTableScanExecutor 类中，我们可以看到如果用户查询的表在内置元数据库中，那么会采用内存查询。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 private Enumerable\u003cObject[]\u003e executeByShardingSphereData(final String databaseName, final String schemaName, final ShardingSphereTable table) { Optional\u003cShardingSphereTableData\u003e tableData = Optional.ofNullable(data.getDatabaseData().get(databaseName)).map(optional -\u003e optional.getSchemaData().get(schemaName)) .map(ShardingSphereSchemaData::getTableData).map(shardingSphereData -\u003e shardingSphereData.get(table.getName())); return tableData.map(this::createMemoryEnumerator).orElseGet(this::createEmptyEnumerable); } private Enumerable\u003cObject[]\u003e createMemoryEnumerator(final ShardingSphereTableData tableData) { return new AbstractEnumerable\u003cObject[]\u003e() { @Override public Enumerator\u003cObject[]\u003e enumerator() { return new MemoryEnumerator\u003c\u003e(tableData.getRows()); } }; } 当然 federation 引擎还提供了其它更强大的功能，例如跨库查询、复杂子查询等，当前 federation 也在快速迭代中，欢迎社区小伙伴前来贡献。\n在了解了内置元数据库的基本实现原理后，我们就可以利用内置元数据库实现更多更丰富的功能了。例如支持 PostgreSQL 客户端的 \\d 查询。\nPostgreSQL \\d 的查询支持 PostgreSQL \\d 是 PG 客户端常用的命令之一，要实现 \\d 的查询，其实就是需要实现它对应的 SQL，并且需要对数据做一定的装饰，例如分片表替换成逻辑表。\n\\d 实际的执行语句如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT n.nspname as \"Schema\", c.relname as \"Name\", CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", c.reloptions as \"Storage\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 需要实现该语句的查询，我们需要收集 pg_catalog.pg_class pg_catalog.pg_namespace 两张表的信息。另外我们还需要模拟返回如下两个函数的返回结果 pg_catalog.pg_get_userbyid(c.relowner), pg_catalog.pg_table_is_visible(c.oid)。\n表的收集同上面的 sharding_table_statistics 表的收集逻辑类似，这里就不再赘述，由于 pg_class 内容比较多，所以我们只收集 \\d 涉及到的一些信息。另外在数据收集阶段，由于分片规则的存在，我们需要展示逻辑表名，因此需要对收集的信息做进一步的装饰，例如表名的替换。\n查询的过程我们只需要模拟函数结果就可以，很幸运，calcite 提供了注册函数的能力。当然目前只是单纯的 mock，未来可以进一步扩展成为真实数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 /** * Create catalog reader. * * @param schemaName schema name * @param schema schema * @param relDataTypeFactory rel data type factory * @param connectionConfig connection config * @return calcite catalog reader */ public static CalciteCatalogReader createCatalogReader(final String schemaName, final Schema schema, final RelDataTypeFactory relDataTypeFactory, final CalciteConnectionConfig connectionConfig) { CalciteSchema rootSchema = CalciteSchema.createRootSchema(true); rootSchema.add(schemaName, schema); registryUserDefinedFunction(schemaName, rootSchema.plus()); return new CalciteCatalogReader(rootSchema, Collections.singletonList(schemaName), relDataTypeFactory, connectionConfig); } private static void registryUserDefinedFunction(final String schemaName, final SchemaPlus schemaPlus) { if (!\"pg_catalog\".equalsIgnoreCase(schemaName)) { return; } schemaPlus.add(\"pg_catalog.pg_table_is_visible\", ScalarFunctionImpl.create(SQLFederationPlannerUtil.class, \"pgTableIsVisible\")); schemaPlus.add(\"pg_catalog.pg_get_userbyid\", ScalarFunctionImpl.create(SQLFederationPlannerUtil.class, \"pgGetUserById\")); } /** * Mock pg_table_is_visible function. * * @param oid oid * @return true */ @SuppressWarnings(\"unused\") public static boolean pgTableIsVisible(final Long oid) { return true; } /** * Mock pg_get_userbyid function. * * @param oid oid * @return user name */ @SuppressWarnings(\"unused\") public static String pgGetUserById(final Long oid) { return \"mock user\"; } 其它流程就基本跟 sharding_table_statistics 一致。那么我们来看下我们的成果吧。\n我们执行 \\d 结果如下（t_order 为分片表，t_single 为普通单表）。\n1 2 3 4 5 6 7 sharding_db=\u003e \\d List of relations Schema | Name | Type | Owner --------+----------+-------------------+----------- public | t_order | table | mock user public | t_single | table | mock user (2 rows) 目前 ShardingSphere 内置元数据库功能是试验性的功能，很多流程和实现方式还需要进一步完善，它是 ShardingSphere 社区对于元数据库的一次探索，它的进一步发展还依赖社区小伙伴的支持。读完了全完的你是不是也跃跃欲试了？欢迎来社区贡献，我们准备了丰富的任务列表等你来挑战。 https://github.com/apache/shardingsphere/issues/24378\n","description":"","tags":null,"title":"ShardingSphere 元数据能力增强解读与实战","uri":"/posts/shardingsphere_meta_learn_action/"},{"categories":["Tech"],"content":"最开始是通过官方网站上这本书来学习 https://doc.rust-lang.org/book/\n通读之后开始学习 https://github.com/rust-lang/rustlings/\n中间参与了一个开源项目 https://github.com/database-mesh/pisanix/tree/master\n最近开始学习 https://exercism.org/tracks/rust\n","description":"","tags":null,"title":"rust 学习","uri":"/posts/learn_rust/"},{"categories":["Tech"],"content":"TiDB 使用 创建表时可以设置数据过期时间，然后后台定时任务删除相关数据\n产品文档：https://docs.pingcap.com/zh/tidb/dev/time-to-live\n功能列表 功能 备注 create 语句中增加 TTL 属性，指定表中数据的过期时间 create 语句中增加注释信息(兼容 MySQL)，包含 TTL 属性 alter 语句修改 TTL 属性 TTL 可以配合表中其它列的属性来使用 可以指定 TTL 任务时间间隔 PolarDB-X 创建 TTL 表，按照时间分区，定期删除和创建相关分区表\n产品文档： https://help.aliyun.com/document_detail/403528.html\n功能列表 功能 备注 对按照时间进行 range 分区的表，定时失效过期分区，定时提前创建分区 仅用在自动模式下的分区表上 支持通过 DDL 语句来定义相关分区的 TTL TTL表支持的时间分区列类型为：date、datetime； 所有的唯一键（包括主键）必须包含TTL表的local partition by range时间分区列；所有的唯一键（包括主键）必须包含TTL表的local partition by range时间分区列 支持查看分区信息以及过期时间等 information_schema.local_partitions 支持校验物理表的物理分区的完整性 支持 Alter 语句手动创建新分区表/删除过期分区表 支持普通表和 TTL 表互相转换 支持创建、查看、删除 TTL 定时任务 Google Spanner 创建表时可以设置行删除策略，后台任务扫描表并删除相关行\n产品文档：https://cloud.google.com/spanner/docs/ttl?hl=zh-cn\n功能列表 功能 备注 create 语句中增加行删除策略 alter 语句修改行删除策略 查看表的 TTL 删除情况 ","description":"","tags":null,"title":"DataTTL 相关调研","uri":"/posts/datattl/"},{"categories":["Tech"],"content":"目标 新增 SS 默认系统库，初步支持全局静态元数据（编码、事务隔离级别）管理 设计元数据调度收集功能，支持 ShardingSphere 动态元数据管理（数据分布等信息） 现状 当前 ss 缺乏自己的元数据信息，例如分片的数据分布、编码等等。\n之前为了解决各个客户端连接报错的问题，设计了各个数据库方言的模拟库。 只有用户表会从真实数据库获取，其他表都是通过 yaml 文件来模拟存储到 zk 的。\n元数据存储调研 MySQL Mysql 将相关参数放在 variables_info 并将相关值设置在全局或 session 级别对应的表中\n1 2 3 4 5 6 7 8 9 10 11 mysql\u003e show tables like '%variables%'; +--------------------------------------------+ | Tables_in_performance_schema (%variables%) | +--------------------------------------------+ | global_variables | | persisted_variables | | session_variables | | user_variables_by_thread | | variables_by_thread | | variables_info | +--------------------------------------------+ 查询隔离级别\n1 2 3 4 5 6 7 8 9 10 11 mysql\u003e SELECT VI.VARIABLE_NAME, GV.VARIABLE_VALUE, VI.MIN_VALUE, VI.MAX_VALUE FROM performance_schema.variables_info AS VI INNER JOIN performance_schema.global_variables AS GV USING (VARIABLE_NAME) where variable_name = 'transaction_isolation' ORDER BY VARIABLE_NAME; +-----------------------+-----------------+-----------+-----------+ | VARIABLE_NAME | VARIABLE_VALUE | MIN_VALUE | MAX_VALUE | +-----------------------+-----------------+-----------+-----------+ | transaction_isolation | REPEATABLE-READ | 0 | 0 | +-----------------------+-----------------+-----------+-----------+ 1 row in set (0.01 sec) Postgres pg_settings 表\n1 2 3 4 5 postgres=# select * from pg_settings where name = 'default_transaction_isolation'; name | setting | unit | category | short_desc | extra_desc | context | vartype | source | min_val | max_val | enumvals | boot_val | reset_val | sourcefile | sourceline | pending_restart -------------------------------+----------------+------+-------------------------------------------------+---------------------------------------------------------------+------------+---------+---------+---------+---------+---------+----------------------------------------------------------------------+----------------+----------------+------------+------------+----------------- default_transaction_isolation | read committed | | Client Connection Defaults / Statement Behavior | Sets the transaction isolation level of each new transaction. | | user | enum | default | | | {serializable,\"repeatable read\",\"read committed\",\"read uncommitted\"} | read committed | read committed | | | f (1 row) PolarDB-X Polardb-x 的元数据库 Polardb-x 中维护了自己的元数据库用于内部流程的使用 它的元数据中几乎维护了所有需要的信息，例如 tables，columns，lock，ddl_job, schedule job 等。 另外在内存中也维护了一份元数据信息，主要是表、列。内存中元数据来源于存储在 mysql 中的元数据，每次 ddl 执行后，都会生成刷新对应表的任务，通过任务触发 cn 从 mysql 中查询并缓存。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 mysql\u003e show tables; +---------------------------------------+ | Tables_in_polardbx_meta_db_polardbx | +---------------------------------------+ | __test_sequence | | __test_sequence_opt | | audit_log | | backfill_objects | | baseline_info | | binlog_polarx_command | | character_sets | | checker_reports | | collation_character_set_applicability | | collations | | column_metas | | column_statistics | | columns | | complex_task_outline | | concurrency_control_rule | | concurrency_control_trigger | | config_listener | | db_group_info | | db_info | | db_priv | | ddl_engine | | ddl_engine_archive | | ddl_engine_task | | ddl_engine_task_archive | | ddl_plan | | default_role_state | | engines | | feature_usage_statistics | | file_storage_files_meta | | file_storage_info | | files | | fired_scheduled_jobs | | global_variables | | group_detail_info | | indexes | | inst_config | | inst_lock | | key_column_usage | | locality_info | | node_info | | partition_group | | partition_group_delta | | partitions | | plan_info | | quarantine_config | | read_write_lock | | recycle_bin | | referential_constraints | | role_priv | | scaleout_backfill_objects | | scaleout_checker_reports | | scaleout_outline | | scheduled_jobs | | schema_change | | schemata | | sequence | | sequence_opt | | server_info | | session_variables | | storage_info | | table_constraints | | table_group | | table_local_partitions | | table_partitions | | table_partitions_delta | | table_priv | | table_statistics | | tablegroup_outline | | tables | | tables_ext | | user_login_error_limit | | user_priv | | variable_config | | views | +---------------------------------------+ 74 rows in set (0.02 sec) 从DDL的执行来看 polardb-x 的元数据使用 简述：整个 ddl 过程中涉及了 ddl-job 任务相关表的使用（ddl_engine 、ddl_engine_task ）,表相关元数据的使用（tables、tables_ext、columns）以及锁 （read_write_lock）等。 以 DDL 语句为例，我们看一下 polardb-x 是怎么使用 metadata 的\n1 CREATE TABLE t1(id bigint not null auto_increment, name varchar(30), primary key(id)) dbpartition by hash(id); SQL语句进入PolarDB-X的CN后，将经历协议层、优化器、执行器的完整处理流程。首先经过解析、鉴权、校验，被解析为关系代数树后，在优化器中经历RBO和CBO生成执行计划，最终在DN上执行完成。 由于 DDL 涉及元数据的变更，所以可能会造成系统状态的不一致。所以 polardb-x 通过 ddl job 配合 metadataDb 以及 双版本元数据 + MDL锁来解决这个问题。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Override public Cursor handle(RelNode logicalPlan, ExecutionContext executionContext) { BaseDdlOperation logicalDdlPlan = (BaseDdlOperation) logicalPlan; initDdlContext(logicalDdlPlan, executionContext); // Validate the plan first and then return immediately if needed. boolean returnImmediately = validatePlan(logicalDdlPlan, executionContext); boolean isNewPartDb = DbInfoManager.getInstance().isNewPartitionDb(logicalDdlPlan.getSchemaName()); if (isNewPartDb) { setPartitionDbIndexAndPhyTable(logicalDdlPlan); } else { setDbIndexAndPhyTable(logicalDdlPlan); } // Build a specific DDL job by subclass. DdlJob ddlJob = returnImmediately? new TransientDdlJob(): buildDdlJob(logicalDdlPlan, executionContext); // Validate the DDL job before request. validateJob(logicalDdlPlan, ddlJob, executionContext); // Handle the client DDL request on the worker side. handleDdlRequest(ddlJob, executionContext); if (executionContext.getDdlContext().isSubJob()){ return buildSubJobResultCursor(ddlJob, executionContext); } return buildResultCursor(logicalDdlPlan, executionContext); } 将 sql 转化为执行计划后，就会创建 ddl job, ddl job 中包含了新增表信息到元数据库、创建物理表、同步元数据信息等任务。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Override protected ExecutableDdlJob doCreate() { CreateTableValidateTask validateTask = new CreateTableValidateTask(schemaName, logicalTableName, physicalPlanData.getTablesExtRecord()); CreateTableAddTablesExtMetaTask addExtMetaTask = new CreateTableAddTablesExtMetaTask(schemaName, logicalTableName, physicalPlanData.isTemporary(), physicalPlanData.getTablesExtRecord(), autoPartition); CreateTablePhyDdlTask phyDdlTask = new CreateTablePhyDdlTask(schemaName, logicalTableName, physicalPlanData); CdcDdlMarkTask cdcDdlMarkTask = new CdcDdlMarkTask(schemaName, physicalPlanData); CreateTableAddTablesMetaTask addTableMetaTask = new CreateTableAddTablesMetaTask(schemaName, logicalTableName, physicalPlanData.getDefaultDbIndex(), physicalPlanData.getDefaultPhyTableName(), physicalPlanData.getSequence(), physicalPlanData.getTablesExtRecord(), physicalPlanData.isPartitioned(), physicalPlanData.isIfNotExists(), physicalPlanData.getKind(), hasTimestampColumnDefault, binaryColumnDefaultValues); LocalityDesc locality = physicalPlanData.getLocalityDesc(); StoreTableLocalityTask storeLocalityTask = locality == null ? null : new StoreTableLocalityTask(schemaName, logicalTableName, locality.toString(), false); CreateTableShowTableMetaTask showTableMetaTask = new CreateTableShowTableMetaTask(schemaName, logicalTableName); TableSyncTask tableSyncTask = new TableSyncTask(schemaName, logicalTableName); ExecutableDdlJob4CreateTable result = new ExecutableDdlJob4CreateTable(); ... result.setCreateTableValidateTask(validateTask); result.setCreateTableAddTablesExtMetaTask(addExtMetaTask); result.setCreateTablePhyDdlTask(phyDdlTask); result.setCreateTableAddTablesMetaTask(addTableMetaTask); result.setCdcDdlMarkTask(cdcDdlMarkTask); result.setCreateTableShowTableMetaTask(showTableMetaTask); result.setTableSyncTask(tableSyncTask); return result; 创建 ddl job 之后，就会下发执行任务，storeJob 会将相关 job 写入ddl_engine_task,ddl_engine 表中。 notifyLeader 会通知相关 cn 节点执行 ddl job。ddl job 会更新元数据库中的 tables columns 等表，如果有多个 cn 节点，这里会触发节点间同步，收到同步信息的节点会从元数据中获取相关信息，并更新 cn 内缓存的元数据信息。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void execute() { ddlContext.setResources(ddlJob.getExcludeResources()); // Create a new job and put it in the queue. ddlJobManager.storeJob(ddlJob, ddlContext); // Request the leader to perform the job. DdlRequest ddlRequest = notifyLeader(ddlContext.getSchemaName(), Lists.newArrayList(ddlContext.getJobId())); // Wait for response from the leader, then respond to the client. if (ddlContext.isAsyncMode()) { return; } respond(ddlRequest, ddlJobManager, executionContext, true); } 关于元数据 GMS 三副本节点 polarDb-x 的元数据存储于内置的 GMS 三副本节点中，提供了全局时间戳来提供外部一致性读，具体可以参考：PolarDB-X 全局时间戳服务的设计 PolarDB-X 一致性共识协议 (X-Paxos)-阿里云开发者社区\nCockroachDB Cockroach db 的内置元数据库 Cockroach 的底层存储结构如下： 内置元数据设计 方案：表结构和存储分开，yaml 模拟表结构 流程图 Zk 结构 以 分片表数据量统计表 为例\ncreate table sharding_table_statistics ( id int, logic_database_name varchar(100), logic_table_name varchar(100), actual_database_name varchar(100), actual_table_name varchar(100), row_count BIGINT, size BIGINT ) 对应 zk 存储结构 红色为修改，黄色为新增，蓝色为 PG 特有（pg 同一个实例上的库是不共享元数据信息的，所以 pg 的 shardingsphere schema 增加在用户创建的逻辑库下，当然 postgres 库下也会模拟一个 shardingsphere schema）\n初始化\n通过 yaml 模拟生成表结构（同现有 information_schema 流程） 从 zk 获取 ShardingSphereData 对象 如果未获取到，根据表结构初始化 ShardingSphereData 对象 注册表结构到 calcite 方便进行查询使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public static MetaDataContexts create(final MetaDataPersistService persistService, final ContextManagerBuilderParameter parameter, final InstanceContext instanceContext) throws SQLException { Collection\u003cString\u003e databaseNames = instanceContext.getInstance().getMetaData() instanceof JDBCInstanceMetaData ? parameter.getDatabaseConfigs().keySet() : persistService.getDatabaseMetaDataService().loadAllDatabaseNames(); Map\u003cString, DatabaseConfiguration\u003e effectiveDatabaseConfigs = createEffectiveDatabaseConfigurations(databaseNames, parameter.getDatabaseConfigs(), persistService); Collection\u003cRuleConfiguration\u003e globalRuleConfigs = persistService.getGlobalRuleService().load(); ConfigurationProperties props = new ConfigurationProperties(persistService.getPropsService().load()); // 增加元数据库表结构的初始化 Map\u003cString, ShardingSphereDatabase\u003e databases = ShardingSphereDatabasesFactory.create(effectiveDatabaseConfigs, props, instanceContext); databases.putAll(reloadDatabases(databases, persistService)); ShardingSphereRuleMetaData globalMetaData = new ShardingSphereRuleMetaData(GlobalRulesBuilder.buildRules(globalRuleConfigs, databases, instanceContext, props)); ShardingSphereMetaData metaData = new ShardingSphereMetaData(databases, globalMetaData, props); ShardingSphereData shardingSphereData = initShardingSphereData(persistService, metaData, instanceContext); return new MetaDataContexts(persistService, metaData, shardingSphereData); } 1 2 3 4 5 6 7 private static ShardingSphereData initShardingSphereData(final MetaDataPersistService persistService, final ShardingSphereMetaData metaData, final InstanceContext instanceContext) { // 先从 zk 加载，没有的话就依赖之前的表结构进行初始化 ShardingSphereData result = persistService.getShardingSphereDataPersistService().load().orElse(ShardingSphereDataFactory.getInstance(metaData)); // 注册表结构到 calcite 方便后续查询使用 result.registerShardingSphereDataQueryEngine(metaData, instanceContext.getEventBusContext()); return result; } 初始化数据收集任务 （目前考虑采用监听 zk 的方式来触发相关任务的收集 + 定时任务收集） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @Slf4j public final class ShardingSphereDataContextManagerLifecycleListener implements ContextManagerLifecycleListener { @Override public void onInitialized(final ModeConfiguration modeConfig, final ContextManager contextManager) { ShardingSphereDataJobWorker.initialize(contextManager); } } public final class ShardingSphereDataJobWorker { private static final AtomicBoolean WORKER_INITIALIZED = new AtomicBoolean(false); /** * Initialize job worker. * @param contextManager context manager */ public static void initialize(final ContextManager contextManager) { if (WORKER_INITIALIZED.get()) { return; } synchronized (WORKER_INITIALIZED) { if (WORKER_INITIALIZED.get()) { return; } log.info(\"start worker initialization\"); // 开启定时收集线程 startScheduleThread(contextManager); // 监听 收集任务 zk 节点 ShardingSphereDataNodeWatcher.getInstance(); WORKER_INITIALIZED.set(true); log.info(\"worker initialization done\"); } } private static void startScheduleThread(final ContextManager contextManager) { // TODO start thread to collect data } } 内存中数据结构如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public final class MetaDataContexts implements AutoCloseable { private final MetaDataPersistService persistService; private final ShardingSphereMetaData metaData; private final ShardingSphereData shardingSphereData; /** * Sharding sphere data. */ @Getter public final class ShardingSphereData { .. // key: table name value: table data private final Map\u003cString, ShardingSphereTableData\u003e tableData = new LinkedHashMap\u003c\u003e(); .. } @RequiredArgsConstructor @Getter public class ShardingSphereTableData { private final String name; private final List\u003cShardingSphereRowData\u003e rows = new LinkedList\u003c\u003e(); } @RequiredArgsConstructor @Getter public class ShardingSphereRowData { private final List\u003cObject\u003e row; } 内存中元数据的变化通过 event 发送，并同步到 zk 中，通过 zk 监听同步刷新其它节点的元数据库 元数据库的使用 利用 calcite 进行查询 增删改，通过内存对象操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @Getter public final class ShardingSphereData { private final Map\u003cString, ShardingSphereTableData\u003e tableData = new LinkedHashMap\u003c\u003e(); private ShardingSphereDataQueryEngine queryEngine; /** * Query. * * @param sql sql * @return result set */ public ResultSet query(final String sql) { return queryEngine.query(sql); } /** * Register. * * @param metaData meta data * @param eventBusContext event bus */ public void registerShardingSphereDataQueryEngine(final ShardingSphereMetaData metaData, final EventBusContext eventBusContext) { ShardingSphereDataQueryEngine queryEngine = ShardingSphereDataQueryEngineFactory.getShardingSphereDataQueryEngine(); // 注册到 calcite queryEngine.init(metaData, eventBusContext, tableData); this.queryEngine = queryEngine; } } 查询的实现类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 public class ShardingSphereDataFederationQueryEngine implements ShardingSphereDataQueryEngine { private OptimizerContext optimizerContext; private ShardingSphereRuleMetaData globalRuleMetaData; private ConfigurationProperties props; private EventBusContext eventBusContext; private ShardingSphereMetaData metaData; private Map\u003cString, ShardingSphereTableData\u003e tableData; @Override public void init(final ShardingSphereMetaData metaData, final EventBusContext eventBusContext, final Map\u003cString, ShardingSphereTableData\u003e tableData) { this.optimizerContext = OptimizerContextFactory.create(metaData.getDatabases(), metaData.getGlobalRuleMetaData()); this.globalRuleMetaData = metaData.getGlobalRuleMetaData(); this.props = metaData.getProps(); this.eventBusContext = eventBusContext; this.tableData = tableData; this.metaData = metaData; } @SneakyThrows private Connection createConnection() { MemorySchema memorySchema = new MemorySchema(tableData, metaData.getDatabase(\"ShardingSphereData\").getSchema(\"ShardingSphereData\")); Properties info = new Properties(); info.setProperty(CalciteConnectionProperty.DEFAULT_NULL_COLLATION.camelName(), NullCollation.LAST.name()); info.setProperty(CalciteConnectionProperty.CASE_SENSITIVE.camelName(), \"false\"); Connection result = DriverManager.getConnection(\"jdbc:calcite:\", info); CalciteConnection calciteConnection = result.unwrap(CalciteConnection.class); SchemaPlus rootSchema = calciteConnection.getRootSchema(); rootSchema.add(\"memory\", memorySchema); return result; } @SneakyThrows @Override public ResultSet query(final String sql) { try (Connection connection = createConnection(); Statement statement = connection.createStatement()) { return statement.executeQuery(sql); } } @Override public boolean isDefault() { return true; } } Tasks use yaml to simulate ShardingSphere built in table add ShardingSphere data and persist add schedule thread or zk watch to trigger collect data support to use federation to query shardingsphere data ","description":"","tags":null,"title":"ShardingSphere 内置数据库设计","uri":"/posts/shardingspheresysdatabase/"},{"categories":["Tech"],"content":"PG \\d 支持 \\d 的现状 \\d 实际执行的语句如下\nSELECT n.nspname as \"Schema\", c.relname as \"Name\", CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", c.reloptions as \"Storage\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 查询结果如下\nSchema | Name | Type | Owner | Storage --------+----------------------+-------+---------+---------------------------------- public | new_t_single_view | view | gaussdb | public | student10w | table | gaussdb | {orientation=row,compression=no} public | t_broadcast_table | table | gaussdb | {orientation=row,compression=no} public | t_broadcast_view_new | view | gaussdb | public | t_encrypt | table | gaussdb | {orientation=row,compression=no} public | t_order_0 | table | gaussdb | {orientation=row,compression=no} public | t_order_1 | table | gaussdb | {orientation=row,compression=no} public | t_order_view_new_0 | view | gaussdb | public | t_order_view_new_1 | view | gaussdb | public | t_single | table | gaussdb | {orientation=row,compression=no} (10 rows) 涉及的系统表有\npg_catalog.pg_class pg_catalog.pg_namespace\n涉及的函数有\npg_catalog.pg_get_userbyid(c.relowner) pg_catalog.pg_table_is_visible(c.oid)\n方案设计 方案概述 利用之前的ShardingSphere 内置数据库设计 在系统库中增加对应的表，表结构和字段类型同原生的数据库一致，并做数据收集并装饰，通过 federation 执行内存查询\n治理中心结构 表的数据收集 SELECT -- 数据库收集值 n.nspname as \"Schema\", -- 数据库收集值 c.relname as \"Name\", -- 数据库收集值 CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", -- 通过 calcite 注册函数，返回 ss 中的用户信息 pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", -- 数据库收集值 c.reloptions as \"Storage\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' -- 通过 calcite 注册函数，mock 返回 true AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 需要收集 pg_class 和 pg_namespace 的信息，并做一些值的装饰，例如 分片表需要替换表名。\npg_class 表\nhttps://opengauss.org/zh/docs/3.1.0/docs/Developerguide/PG_CLASS.html\n黄色表示 \\d 使用到的字段\npg_namespace 表\nhttps://opengauss.org/zh/docs/3.1.0/docs/Developerguide/PG_NAMESPACE.html\n关键问题及解决方法 查询语句中的函数无法下推，如何使用 通过 federation 注册函数解决 pg_table_is_visible(c.oid) mock 返回 true，收集数据时就只收集 true 的部分；\npg_get_userbyid mock 返回 ss 中配置的用户；\npg_class 数据量比较大，只收集 \\d 使用到的数据行内容。 查询中需要根据 pg_class 的 relnamespace 关联到 pg_namespace 中的 oid， 但是由于分布式系统，因此可能存在 pg_namespace oid 相同但是 nspname 却不同。 需要新建一张系统 oid 映射关系表，通过 ss 生成 id 映射不同实例的 oid，在表存储时，需要存储 ss 生成的 oid。（需要进一步设计）\n针对 \\d+ 是否能够支持 SELECT n.nspname as \"Schema\", c.relname as \"Name\", CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", -- 该值暂时无法支持显示，需要获取大小 pg_catalog.pg_size_pretty(pg_catalog.pg_table_size(c.oid)) as \"Size\", c.reloptions as \"Storage\", -- 该值依赖 pg_catalog.pg_description 表的收集以及 oid 的映射维护 -- select description from pg_catalog.pg_description where objoid = $1 and classoid = (select oid from pg_catalog.pg_class where relname = $2 and relnamespace = 11) and objsubid = 0 pg_catalog.obj_description(c.oid, 'pg_class') as \"Description\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 附录 系统函数分析 pg_get_userbyid\n对应源码为\n/* ---------- * pg_get_userbyid - Get a user name by roleid and * fallback to 'unknown (OID=n)' * ---------- */ Datum pg_get_userbyid(PG_FUNCTION_ARGS) { Oid roleid = PG_GETARG_OID(0); Name result; HeapTuple roletup; Form_pg_authid role_rec; /* * Allocate space for the result */ result = (Name) palloc(NAMEDATALEN); memset(NameStr(*result), 0, NAMEDATALEN); /* * Get the pg_authid entry and print the result */ roletup = SearchSysCache1(AUTHOID, ObjectIdGetDatum(roleid)); if (HeapTupleIsValid(roletup)) { role_rec = (Form_pg_authid) GETSTRUCT(roletup); *result = role_rec-\u003erolname; ReleaseSysCache(roletup); } else sprintf(NameStr(*result), \"unknown (OID=%u)\", roleid); PG_RETURN_NAME(result); } 可以用以下语句获取\n\u003cstrong\u003eselect\u003c/strong\u003e oid,rolname \u003cstrong\u003efrom\u003c/strong\u003e pg_authid where oid = 1234 pg_table_is_visible(c.oid) 函数对应源码如下\n/* * RelationIsVisible * Determine whether a relation (identified by OID) is visible in the * current search path. Visible means \"would be found by searching * for the unqualified relation name\". */ bool RelationIsVisible(Oid relid) { HeapTuple reltup; Form_pg_class relform; Oid relnamespace; bool visible; reltup = SearchSysCache1(\u003cem\u003eRELOID\u003c/em\u003e, ObjectIdGetDatum(relid)); if (!HeapTupleIsValid(reltup)) elog(ERROR, \"cache lookup failed for relation %u\", relid); relform = (Form_pg_class) GETSTRUCT(reltup); recomputeNamespacePath(); /* * Quick check: if it ain't in the path at all, it ain't visible. Items in * the system namespace are surely in the path and so we needn't even do * list_member_oid() for them. */ relnamespace = relform-\u003erelnamespace; if (relnamespace != PG_CATALOG_NAMESPACE \u0026\u0026 !list_member_oid(activeSearchPath, relnamespace)) visible = false; else { /* * If it is in the path, it might still not be visible; it could be * hidden by another relation of the same name earlier in the path. So * we must do a slow check for conflicting relations. */ char *relname = NameStr(relform-\u003erelname); ListCell *l; visible = false; foreach(l, activeSearchPath) { Oid namespaceId = lfirst_oid(l); if (namespaceId == relnamespace) { /* Found it first in path */ visible = true; break; } if (OidIsValid(get_relname_relid(relname, namespaceId))) { /* Found something else first in path */ break; } } } ReleaseSysCache(reltup); return visible; } ","description":"","tags":null,"title":"ShardingSphere PostgreSQL openGauss \\d 支持方案","uri":"/posts/shardingsphere_sys_data_d/"},{"categories":["Tech"],"content":"","description":"","tags":null,"title":"Pisanix protocol","uri":"/posts/join-pisanix/"},{"categories":["Tech"],"content":"https://yeasy.gitbook.io/docker_practice/\n使用 dockerFile 创建镜像 FROM nginx RUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html 容器的使用 ","description":"","tags":null,"title":"Docker","uri":"/posts/docker/"},{"categories":["Tech"],"content":"构建 yat 下载源码\ngit clone https://gitee.com/opengauss/Yat.git 构建 yat\ncd Yat/yat-master chmod +x gradlew ./gradlew pack cd pkg chmod +x install ./install -F 根据源码中的 dockerFile 构建 dockerImage\n使用 yat 测试 测试 shardingSphere proxy 利用构建的 yat image 运行相关测试 需要挂在到对应目录\ndocker run --name yat0 -i -t -v /Users/chenchuxin/Documents/GitHub/Yat/openGaussBase:/root/openGaussBase -w /root/openGaussBase --entrypoint=bash --privileged=true yat-v1 修改 yat 项目 openGaussBase/conf 下的 configure.yml 文件\nyat.limit.case.size.max: '200K' yat.limit.case.count.max: 100000 yat.limit.case.depth.max: 10 yat.limit.case.name.pattern: .* 修改 yat 项目 openGaussBase/conf 下的 nodes.yml 文件\ndefault: host: 'host.docker.internal' db: url: 'jdbc:opengauss://host.docker.internal:3307/sharding_db?loggerLevel=OFF' driver: 'org.opengauss.Driver' username: 'root' password: 'root' port: 3307 name: 'sharding_db' ssh: port: 22 username: root password: '********' 创建 conf/env.sh\ntouch conf/env.sh 在 openGaussBase/schedule 目录创建 schedule.schd 文件，增加你需要测试的 case\ntest: SQL/DDL/view/Opengauss_Function_DDL_View_Case0034 进入 docker bash 模式 运行 yat suite\n[+] [2022-06-22 09:06:15] [05.394 s] [v] SQL/DDL/view/Opengauss_Function_DDL_View_Case0034 .... : ok ################# Testing Result 1/1 Using Time PT5.46011S At 2022-06-22 09:06:20 ################## Run test suite finished 也可能出现相关异常，可参考 https://gitee.com/opengauss/Yat/blob/master/yat-master/docs/index.md#yat-quick-start 来解决\n","description":"","tags":null,"title":"Add Yat Test for ShardingSphere","uri":"/posts/add-yat-test-for-shardingsphere/"},{"categories":["Tech"],"content":"https://zhuanlan.zhihu.com/p/515688555\nmac 开发环境启动 galaxysql https://hub.docker.com/r/polardbx/polardb-x\ndocker pull polardbx/polardb-x docker run -d --name some-dn-and-gms --env mode=dev -p 4886:4886 -p 32886:32886 polardbx/polardb-x docker exec -it 41d8a027 bash mysql -h127.0.0.1 -P4886 -uroot -padmin -D polardbx_meta_db_polardbx -e \"select passwd_enc from storage_info where inst_kind=2\" 获取密码后修改 server.properties\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 serverPort=8528 managerPort=3406 rpcPort=9090 charset=utf-8 processors=4 processorHandler=16 processorKillExecutor=128 timerExecutor=8 managerExecutor=256 serverExecutor=1024 idleTimeout= trustedIps=127.0.0.1 slowSqlTime=1000 maxConnection=20000 allowManagerLogin=1 allowCrossDbQuery=true galaxyXProtocol=1 metaDbAddr=127.0.0.1:4886 metaDbXprotoPort=32886 metaDbUser=my_polarx # 前文查看的存储节点密码 metaDbPasswd=qEJWtCdgsOIie4j4mKP2Bvg2dsFHzdIhTaqMiq86N1QQU1HHL7olKb60pxz5hp/4 #?? E2+jB0*0\u0026gM9)9$4+6)E4@1$lO9%G8+jA4_ metaDbName=polardbx_meta_db_polardbx instanceId=polardbx-polardbx 注释掉 CobarServer.java 中 tryStartCdcManager(); 代码\n启动配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u003ccomponent name=\"ProjectRunConfigurationManager\"\u003e \u003cconfiguration default=\"false\" name=\"TddlLauncher\" type=\"Application\" factoryName=\"Application\" singleton=\"false\" nameIsGenerated=\"true\"\u003e \u003cenvs\u003e \u003cenv name=\"dnPasswordKey\" value=\"asdf1234ghjk5678\" /\u003e \u003c/envs\u003e \u003coption name=\"MAIN_CLASS_NAME\" value=\"com.alibaba.polardbx.server.TddlLauncher\" /\u003e \u003cmodule name=\"polardbx-server\" /\u003e \u003coption name=\"VM_PARAMETERS\" value=\"-Dserver.conf=$PROJECT_DIR$/polardbx-server/src/main/conf/server.properties\" /\u003e \u003cextension name=\"coverage\"\u003e \u003cpattern\u003e \u003coption name=\"PATTERN\" value=\"com.alibaba.polardbx.server.*\" /\u003e \u003coption name=\"ENABLED\" value=\"true\" /\u003e \u003c/pattern\u003e \u003c/extension\u003e \u003cmethod v=\"2\"\u003e \u003coption name=\"Make\" enabled=\"true\" /\u003e \u003c/method\u003e \u003c/configuration\u003e \u003c/component\u003e 连接 polar-dbx cn\nmysql -h127.0.0.1 -P8528 -upolardbx_root -p123456 polardb-x debug ddl create ddl 一直不能成功原来是因为，docker 镜像中没有启动 cdc，DDL 流程中涉及到 notify cdc，所以注释 CdcManager.notifyDdl 方法中的代码可以执行成功。 这篇文档写得挺好，说明了 ddl 执行的流程 https://zhuanlan.zhihu.com/p/515688555\n","description":"","tags":null,"title":"Polar Db","uri":"/posts/polar-db/"},{"categories":["Tech"],"content":"Step 1: 克隆项目\nStep 2: 添加新的远程仓库 为了修改他人 Fork 的仓库，你需要将其添加到自己的远程仓库列表中\n$ git remote add sirmin https://github.com/SirMin/shardingsphere.git 现在，当你执行 git remote -v 指令时，就可以看到他人 Fork 的仓库，出现在你的远程仓库列表中：\n% git remote -v origin\thttps://github.com/tuichenchuxin/shardingsphere.git (fetch) origin\thttps://github.com/tuichenchuxin/shardingsphere.git (push) sirmin\thttps://github.com/SirMin/shardingsphere.git (fetch) sirmin\thttps://github.com/SirMin/shardingsphere.git (push) upstream\thttps://github.com/apache/shardingsphere.git (fetch) upstream\thttps://github.com/apache/shardingsphere.git (push) Step 3: 拉取新的远程仓库\n$ git fetch sirmin Step 4: 切换到对应的分支 你需要确认一下，贡献者提出 Pull Request 时所用的分支（如果你不确定他们使用的哪个分支，请看 Pull Request 顶部的信息）：\n在本地，给该分支起一个不重复的名字，例如 EvanOne0616-patch，然后切换到贡献者提出 Pull Request 所用的分支：\n$ git checkout -b sirmin-patch sirmin/sirmin/resultSet Step 5: 提交修改，推送远程\n$ git commit -m \"Fix the wrong spelling of the word\" $ git push sirmin HEAD:sirmin/resultSet ","description":"","tags":null,"title":"How to Modify Pr","uri":"/posts/modifypr/"},{"categories":["Tech"],"content":"docker pull registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g docker run --privileged --restart=always --name oracle_11g -p 1521:1521 -d registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g docker exec -it 容器ID /bin/bash source /home/oracle/.bash_profile sqlplus nologconnect as sysdba 1 2 3 4 create user oracle identified by oracle#123; alter user system identified by system; alter user system identified by 123456; grant connect,resource,dba to oracle; 参考：https://baijiahao.baidu.com/s?id=1709232831349390844\u0026wfr=spider\u0026for=pc\n","description":"","tags":null,"title":"mac docker 安装 oracle 11g","uri":"/posts/installoracle11g/"},{"categories":["Tech"],"content":"Freemarker freemarker 是一款开源的模版引擎，可以基于模版方便的生成结果。 https://freemarker.apache.org/\nFreemarker 的使用 编写 ftl 模版 以生成 postgres 查询的 sql 语句为例 编写 delete.ftl 文件，${} 中的字段是参数\n1 DROP TABLE IF EXISTS ${schema}.${name}; 当然实际使用中的模版可能复杂的多，以部分创建表模版为例 我们可以在模版中使用 import 引入其它模版 使用 assign 设置变量 使用 if, list 等 使用 ?? 判断是否为空，使用 !false 如果为空，默认 false\n\u003c#import \"../../macro/constraints.ftl\" as CONSTRAINTS\u003e \u003c#assign with_clause = false\u003e \u003c#if fillfactor!false || parallel_workers!false || toast_tuple_target!false || (autovacuum_custom!false \u0026\u0026 add_vacuum_settings_in_sql!false) || autovacuum_enabled == 't' || autovacuum_enabled == 'f' || (toast_autovacuum!false \u0026\u0026 add_vacuum_settings_in_sql!false) || toast_autovacuum_enabled == 't' || toast_autovacuum_enabled == 'f' \u003e \u003c#assign with_clause = true\u003e \u003c/#if\u003e \u003c#list columns as c \u003e 其它模版使用可以参考 https://freemarker.apache.org/docs/dgui_template.html\njava 使用 freemarker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @NoArgsConstructor(access = AccessLevel.PRIVATE) public final class FreemarkerManager { private static final FreemarkerManager INSTANCE = new FreemarkerManager(); private final Configuration templateConfig = createTemplateConfiguration(); /** * Get freemarker manager instance. * * @return freemarker manager instance */ public static FreemarkerManager getInstance() { return INSTANCE; } @SneakyThrows private Configuration createTemplateConfiguration() { Configuration result = new Configuration(Configuration.VERSION_2_3_31); result.setDirectoryForTemplateLoading(new File(Objects.requireNonNull(FreemarkerManager.class.getClassLoader().getResource(\"template\")).getFile())); result.setDefaultEncoding(\"UTF-8\"); return result; } /** * Get sql from template. * * @param data data * @param path path * @return sql */ @SneakyThrows public static String getSqlFromTemplate(final Map\u003cString, Object\u003e data, final String path) { Template template = FreemarkerManager.getInstance().templateConfig.getTemplate(path); try (StringWriter result = new StringWriter()) { template.process(data, result); return result.toString(); } } } ","description":"","tags":null,"title":"Freemarker 的使用","uri":"/posts/use_free_marker/"},{"categories":["Tech"],"content":"下载源码 https://github.com/postgres/pgadmin4\n安装环境 brew install node brew install yarn cd runtime yarn install node_modules/nw/nwjs/nwjs.app/Contents/MacOS/nwjs . sudo mkdir \"/var/log/pgadmin\" sudo chmod a+wrx \"/var/log/pgadmin\" sudo mkdir \"/var/lib/pgadmin\" sudo chmod a+wrx \"/var/lib/pgadmin\" pip install --upgrade pip pip install psycopg2-binary make install-node 最后运行 pgAdmin4.py 过程中会有一些问题，参考 https://github.com/postgres/pgadmin4 readme. 和 stack overflow\n","description":"","tags":null,"title":"How To Debug PgAdmin4","uri":"/posts/howtodebugpgadmin4/"},{"categories":["Tech"],"content":"PgAmdin4 展示 DDL 语句 通过 PgAdmin4 可以获取 table 的 DDL 语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 -- Table: public.t_order_0 -- DROP TABLE IF EXISTS public.t_order_0; CREATE TABLE IF NOT EXISTS public.t_order_0 ( order_id integer NOT NULL, user_id integer NOT NULL, status character varying(45) COLLATE pg_catalog.\"default\", CONSTRAINT t_order_0_pkey PRIMARY KEY (order_id) ) TABLESPACE pg_default; ALTER TABLE IF EXISTS public.t_order_0 OWNER to postgres; COMMENT ON TABLE public.t_order_0 IS 'haha'; COMMENT ON COLUMN public.t_order_0.order_id IS 'haha'; PgAdmin4 是如何展示对应的 DDL 语句的呢 https://github.com/postgres/pgadmin4 翻阅源码发现 DDL 语句的展示，主要是通过以下步骤来获取 SQL 语句的。\n查询表基础信息（区分 pg 不同版本） 获取相关权限信息 查询列相关信息 检查 of_type 和 继承表，如果存在需要获取 获取该表的所有列信息 对列做格式化 查询 constriant 信息，并添加到 列上 检查 table is partitions 查询 index 信息 查询 ROW SECURITY POLICY 查询 Triggers 查询 Rules 相关代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def sql(self, gid, sid, did, scid, tid): \"\"\" This function will creates reverse engineered sql for the table object Args: gid: Server Group ID sid: Server ID did: Database ID scid: Schema ID tid: Table ID \"\"\" main_sql = [] status, res = self._fetch_table_properties(did, scid, tid) if not status: return res if len(res['rows']) == 0: return gone(gettext(self.not_found_error_msg())) data = res['rows'][0] return BaseTableView.get_reverse_engineered_sql( self, did=did, scid=scid, tid=tid, main_sql=main_sql, data=data, add_not_exists_clause=True) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_reverse_engineered_sql(self, **kwargs): \"\"\" This function will creates reverse engineered sql for the table object Args: kwargs \"\"\" did = kwargs.get('did') scid = kwargs.get('scid') tid = kwargs.get('tid') main_sql = kwargs.get('main_sql') data = kwargs.get('data') json_resp = kwargs.get('json_resp', True) diff_partition_sql = kwargs.get('diff_partition_sql', False) if_exists_flag = kwargs.get('add_not_exists_clause', False) # Table \u0026 Schema declaration so that we can use them in child nodes schema = data['schema'] table = data['name'] is_partitioned = 'is_partitioned' in data and data['is_partitioned'] # Get Reverse engineered sql for Table self._get_resql_for_table(did, scid, tid, data, json_resp, main_sql, add_not_exists_clause=if_exists_flag) # Get Reverse engineered sql for Table self._get_resql_for_index(did, tid, main_sql, json_resp, schema, table, add_not_exists_clause=if_exists_flag) # Get Reverse engineered sql for ROW SECURITY POLICY self._get_resql_for_row_security_policy(scid, tid, json_resp, main_sql, schema, table) # Get Reverse engineered sql for Triggers self._get_resql_for_triggers(tid, json_resp, main_sql, schema, table) # Get Reverse engineered sql for Compound Triggers self._get_resql_for_compound_triggers(tid, main_sql, schema, table) # Get Reverse engineered sql for Rules self._get_resql_for_rules(tid, main_sql, table, json_resp) # Get Reverse engineered sql for Partitions partition_main_sql = \"\" if is_partitioned: sql = render_template(\"/\".join([self.partition_template_path, self._NODES_SQL]), scid=scid, tid=tid) status, rset = self.conn.execute_2darray(sql) if not status: return internal_server_error(errormsg=rset) self._get_resql_for_partitions(data, rset, json_resp, diff_partition_sql, main_sql, did) sql = '\\n'.join(main_sql) if not json_resp: return sql, partition_main_sql return ajax_response(response=sql.strip('\\n')) pgAdmin 展示表结构反向 SQL 执行了哪些 SQL 语句呢？ table propeties 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 SELECT rel.oid, rel.relname AS name, rel.reltablespace AS spcoid,rel.relacl AS relacl_str, (CASE WHEN length(spc.spcname::text) \u003e 0 OR rel.relkind = 'p' THEN spc.spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END) as spcname, (CASE rel.relreplident WHEN 'd' THEN 'default' WHEN 'n' THEN 'nothing' WHEN 'f' THEN 'full' WHEN 'i' THEN 'index' END) as replica_identity, (select nspname FROM pg_catalog.pg_namespace WHERE oid = 2200::oid ) as schema, pg_catalog.pg_get_userbyid(rel.relowner) AS relowner, rel.relkind, (CASE WHEN rel.relkind = 'p' THEN true ELSE false END) AS is_partitioned, rel.relhassubclass, rel.reltuples::bigint, des.description, con.conname, con.conkey, EXISTS(select 1 FROM pg_catalog.pg_trigger JOIN pg_catalog.pg_proc pt ON pt.oid=tgfoid AND pt.proname='logtrigger' JOIN pg_catalog.pg_proc pc ON pc.pronamespace=pt.pronamespace AND pc.proname='slonyversion' WHERE tgrelid=rel.oid) AS isrepl, (SELECT count(*) FROM pg_catalog.pg_trigger WHERE tgrelid=rel.oid AND tgisinternal = FALSE) AS triggercount, (SELECT ARRAY(SELECT CASE WHEN (nspname NOT LIKE 'pg\\_%') THEN pg_catalog.quote_ident(nspname)||'.'||pg_catalog.quote_ident(c.relname) ELSE pg_catalog.quote_ident(c.relname) END AS inherited_tables FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid ORDER BY inhseqno)) AS coll_inherits, (SELECT count(*) FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid) AS inherited_tables_cnt, (CASE WHEN rel.relpersistence = 'u' THEN true ELSE false END) AS relpersistence, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'fillfactor=([0-9]*)') AS fillfactor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'parallel_workers=([0-9]*)') AS parallel_workers, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'toast_tuple_target=([0-9]*)') AS toast_tuple_target, (substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS autovacuum_enabled, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS autovacuum_freeze_table_age, (substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS toast_autovacuum_enabled, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS toast_autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS toast_autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS toast_autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS toast_autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS toast_autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS toast_autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS toast_autovacuum_freeze_table_age, rel.reloptions AS reloptions, tst.reloptions AS toast_reloptions, rel.reloftype, CASE WHEN typ.typname IS NOT NULL THEN (select pg_catalog.quote_ident(nspname) FROM pg_catalog.pg_namespace WHERE oid = 2200::oid )||'.'||pg_catalog.quote_ident(typ.typname) ELSE typ.typname END AS typname, typ.typrelid AS typoid, rel.relrowsecurity as rlspolicy, rel.relforcerowsecurity as forcerlspolicy, (CASE WHEN rel.reltoastrelid = 0 THEN false ELSE true END) AS hastoasttable, (SELECT pg_catalog.array_agg(provider || '=' || label) FROM pg_catalog.pg_seclabels sl1 WHERE sl1.objoid=rel.oid AND sl1.objsubid=0) AS seclabels, (CASE WHEN rel.oid \u003c= 13756::oid THEN true ElSE false END) AS is_sys_table -- Added for partition table , (CASE WHEN rel.relkind = 'p' THEN pg_catalog.pg_get_partkeydef(16410::oid) ELSE '' END) AS partition_scheme FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=rel.oid AND des.objsubid=0 AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND NOT rel.relispartition AND rel.oid = 16410::oid ORDER BY rel.relname; 表总数 'SELECT COUNT(*)::text FROM public.t_order_1;' 权限 SELECT 'relacl' as deftype, COALESCE(gt.rolname, 'PUBLIC') grantee, g.rolname grantor, pg_catalog.array_agg(privilege_type) as privileges, pg_catalog.array_agg(is_grantable) as grantable FROM (SELECT d.grantee, d.grantor, d.is_grantable, CASE d.privilege_type WHEN 'CONNECT' THEN 'c' WHEN 'CREATE' THEN 'C' WHEN 'DELETE' THEN 'd' WHEN 'EXECUTE' THEN 'X' WHEN 'INSERT' THEN 'a' WHEN 'REFERENCES' THEN 'x' WHEN 'SELECT' THEN 'r' WHEN 'TEMPORARY' THEN 'T' WHEN 'TRIGGER' THEN 't' WHEN 'TRUNCATE' THEN 'D' WHEN 'UPDATE' THEN 'w' WHEN 'USAGE' THEN 'U' ELSE 'UNKNOWN' END AS privilege_type FROM (SELECT rel.relacl FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) acl, (SELECT (d).grantee AS grantee, (d).grantor AS grantor, (d).is_grantable AS is_grantable, (d).privilege_type AS privilege_type FROM (SELECT aclexplode(rel.relacl) as d FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) a ORDER BY privilege_type) d ) d LEFT JOIN pg_catalog.pg_roles g ON (d.grantor = g.oid) LEFT JOIN pg_catalog.pg_roles gt ON (d.grantee = gt.oid) GROUP BY g.rolname, gt.rolname reverse engine for table\nSELECT 'relacl' as deftype, COALESCE(gt.rolname, 'PUBLIC') grantee, g.rolname grantor, pg_catalog.array_agg(privilege_type) as privileges, pg_catalog.array_agg(is_grantable) as grantable FROM (SELECT d.grantee, d.grantor, d.is_grantable, CASE d.privilege_type WHEN 'CONNECT' THEN 'c' WHEN 'CREATE' THEN 'C' WHEN 'DELETE' THEN 'd' WHEN 'EXECUTE' THEN 'X' WHEN 'INSERT' THEN 'a' WHEN 'REFERENCES' THEN 'x' WHEN 'SELECT' THEN 'r' WHEN 'TEMPORARY' THEN 'T' WHEN 'TRIGGER' THEN 't' WHEN 'TRUNCATE' THEN 'D' WHEN 'UPDATE' THEN 'w' WHEN 'USAGE' THEN 'U' ELSE 'UNKNOWN' END AS privilege_type FROM (SELECT rel.relacl FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) acl, (SELECT (d).grantee AS grantee, (d).grantor AS grantor, (d).is_grantable AS is_grantable, (d).privilege_type AS privilege_type FROM (SELECT aclexplode(rel.relacl) as d FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) a ORDER BY privilege_type) d ) d LEFT JOIN pg_catalog.pg_roles g ON (d.grantor = g.oid) LEFT JOIN pg_catalog.pg_roles gt ON (d.grantee = gt.oid) GROUP BY g.rolname, gt.rolname 获取列\nSELECT att.attname as name, att.atttypid, att.attlen, att.attnum, att.attndims, att.atttypmod, att.attacl, att.attnotnull, att.attoptions, att.attstattarget, att.attstorage, att.attidentity, pg_catalog.pg_get_expr(def.adbin, def.adrelid) AS defval, pg_catalog.format_type(ty.oid,NULL) AS typname, pg_catalog.format_type(ty.oid,att.atttypmod) AS displaytypname, pg_catalog.format_type(ty.oid,att.atttypmod) AS cltype, CASE WHEN ty.typelem \u003e 0 THEN ty.typelem ELSE ty.oid END as elemoid, (SELECT nspname FROM pg_catalog.pg_namespace WHERE oid = ty.typnamespace) as typnspname, ty.typstorage AS defaultstorage, description, pi.indkey, (SELECT count(1) FROM pg_catalog.pg_type t2 WHERE t2.typname=ty.typname) \u003e 1 AS isdup, CASE WHEN length(coll.collname::text) \u003e 0 AND length(nspc.nspname::text) \u003e 0 THEN pg_catalog.concat(pg_catalog.quote_ident(nspc.nspname),'.',pg_catalog.quote_ident(coll.collname)) ELSE '' END AS collspcname, EXISTS(SELECT 1 FROM pg_catalog.pg_constraint WHERE conrelid=att.attrelid AND contype='f' AND att.attnum=ANY(conkey)) As is_fk, (SELECT pg_catalog.array_agg(provider || '=' || label) FROM pg_catalog.pg_seclabels sl1 WHERE sl1.objoid=att.attrelid AND sl1.objsubid=att.attnum) AS seclabels, (CASE WHEN (att.attnum \u003c 1) THEN true ElSE false END) AS is_sys_column, (CASE WHEN (att.attidentity in ('a', 'd')) THEN 'i' WHEN (att.attgenerated in ('s')) THEN 'g' ELSE 'n' END) AS colconstype, (CASE WHEN (att.attgenerated in ('s')) THEN pg_catalog.pg_get_expr(def.adbin, def.adrelid) END) AS genexpr, tab.relname as relname, (CASE WHEN tab.relkind = 'v' THEN true ELSE false END) AS is_view_only, seq.* FROM pg_catalog.pg_attribute att JOIN pg_catalog.pg_type ty ON ty.oid=atttypid LEFT OUTER JOIN pg_catalog.pg_attrdef def ON adrelid=att.attrelid AND adnum=att.attnum LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=att.attrelid AND des.objsubid=att.attnum AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN (pg_catalog.pg_depend dep JOIN pg_catalog.pg_class cs ON dep.classid='pg_class'::regclass AND dep.objid=cs.oid AND cs.relkind='S') ON dep.refobjid=att.attrelid AND dep.refobjsubid=att.attnum LEFT OUTER JOIN pg_catalog.pg_index pi ON pi.indrelid=att.attrelid AND indisprimary LEFT OUTER JOIN pg_catalog.pg_collation coll ON att.attcollation=coll.oid LEFT OUTER JOIN pg_catalog.pg_namespace nspc ON coll.collnamespace=nspc.oid LEFT OUTER JOIN pg_catalog.pg_sequence seq ON cs.oid=seq.seqrelid LEFT OUTER JOIN pg_catalog.pg_class tab on tab.oid = att.attrelid WHERE att.attrelid = 16410::oid AND att.attnum \u003e 0 AND att.attisdropped IS FALSE ORDER BY att.attnum; SELECT t.main_oid, pg_catalog.ARRAY_AGG(t.typname) as edit_types FROM (SELECT pc.castsource AS main_oid, pg_catalog.format_type(tt.oid,NULL) AS typname FROM pg_catalog.pg_type tt JOIN pg_catalog.pg_cast pc ON tt.oid=pc.casttarget WHERE pc.castsource IN (23,1043) AND pc.castcontext IN ('i', 'a') UNION SELECT tt.typbasetype AS main_oid, pg_catalog.format_type(tt.oid,NULL) AS typname FROM pg_catalog.pg_type tt WHERE tt.typbasetype IN (23,1043) ) t GROUP BY t.main_oid; SELECT 'attacl' as deftype, COALESCE(gt.rolname, 'PUBLIC') grantee, g.rolname grantor, pg_catalog.array_agg(privilege_type order by privilege_type) as privileges, pg_catalog.array_agg(is_grantable) as grantable FROM (SELECT d.grantee, d.grantor, d.is_grantable, CASE d.privilege_type WHEN 'CONNECT' THEN 'c' WHEN 'CREATE' THEN 'C' WHEN 'DELETE' THEN 'd' WHEN 'EXECUTE' THEN 'X' WHEN 'INSERT' THEN 'a' WHEN 'REFERENCES' THEN 'x' WHEN 'SELECT' THEN 'r' WHEN 'TEMPORARY' THEN 'T' WHEN 'TRIGGER' THEN 't' WHEN 'TRUNCATE' THEN 'D' WHEN 'UPDATE' THEN 'w' WHEN 'USAGE' THEN 'U' ELSE 'UNKNOWN' END AS privilege_type FROM (SELECT attacl FROM pg_catalog.pg_attribute att WHERE att.attrelid = 16410::oid AND att.attnum = 1::int ) acl, (SELECT (d).grantee AS grantee, (d).grantor AS grantor, (d).is_grantable AS is_grantable, (d).privilege_type AS privilege_type FROM (SELECT pg_catalog.aclexplode(attacl) as d FROM pg_catalog.pg_attribute att WHERE att.attrelid = 16410::oid AND att.attnum = 1::int) a) d ) d LEFT JOIN pg_catalog.pg_roles g ON (d.grantor = g.oid) LEFT JOIN pg_catalog.pg_roles gt ON (d.grantee = gt.oid) GROUP BY g.rolname, gt.rolname ORDER BY grantee SELECT cls.oid, cls.relname as name, indnkeyatts as col_count, CASE WHEN length(spcname::text) \u003e 0 THEN spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END as spcname, CASE contype WHEN 'p' THEN desp.description WHEN 'u' THEN desp.description WHEN 'x' THEN desp.description ELSE des.description END AS comment, condeferrable, condeferred, conislocal, substring(pg_catalog.array_to_string(cls.reloptions, ',') from 'fillfactor=([0-9]*)') AS fillfactor FROM pg_catalog.pg_index idx JOIN pg_catalog.pg_class cls ON cls.oid=indexrelid LEFT OUTER JOIN pg_catalog.pg_tablespace ta on ta.oid=cls.reltablespace LEFT JOIN pg_catalog.pg_depend dep ON (dep.classid = cls.tableoid AND dep.objid = cls.oid AND dep.refobjsubid = '0' AND dep.refclassid=(SELECT oid FROM pg_catalog.pg_class WHERE relname='pg_constraint') AND dep.deptype='i') LEFT OUTER JOIN pg_catalog.pg_constraint con ON (con.tableoid = dep.refclassid AND con.oid = dep.refobjid) LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=cls.oid AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_description desp ON (desp.objoid=con.oid AND desp.objsubid = 0 AND desp.classoid='pg_constraint'::regclass) WHERE indrelid = 16410::oid AND contype='u' ORDER BY cls.relname SELECT c.oid, conname as name, relname, nspname, description as comment, pg_catalog.pg_get_expr(conbin, conrelid, true) as consrc, connoinherit, NOT convalidated as convalidated, conislocal FROM pg_catalog.pg_constraint c JOIN pg_catalog.pg_class cl ON cl.oid=conrelid JOIN pg_catalog.pg_namespace nl ON nl.oid=relnamespace LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=c.oid AND des.classoid='pg_constraint'::regclass) WHERE contype = 'c' AND conrelid = 16410::oid SELECT cls.oid, cls.relname as name, indnkeyatts as col_count, amname, CASE WHEN length(spcname::text) \u003e 0 THEN spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END as spcname, CASE contype WHEN 'p' THEN desp.description WHEN 'u' THEN desp.description WHEN 'x' THEN desp.description ELSE des.description END AS comment, condeferrable, condeferred, substring(pg_catalog.array_to_string(cls.reloptions, ',') from 'fillfactor=([0-9]*)') AS fillfactor, pg_catalog.pg_get_expr(idx.indpred, idx.indrelid, true) AS indconstraint FROM pg_catalog.pg_index idx JOIN pg_catalog.pg_class cls ON cls.oid=indexrelid LEFT OUTER JOIN pg_catalog.pg_tablespace ta on ta.oid=cls.reltablespace JOIN pg_catalog.pg_am am ON am.oid=cls.relam LEFT JOIN pg_catalog.pg_depend dep ON (dep.classid = cls.tableoid AND dep.objid = cls.oid AND dep.refobjsubid = '0' AND dep.refclassid=(SELECT oid FROM pg_catalog.pg_class WHERE relname='pg_constraint') AND dep.deptype='i') LEFT OUTER JOIN pg_catalog.pg_constraint con ON (con.tableoid = dep.refclassid AND con.oid = dep.refobjid) LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=cls.oid AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_description desp ON (desp.objoid=con.oid AND desp.objsubid = 0 AND desp.classoid='pg_constraint'::regclass) WHERE indrelid = 16410::oid AND contype='x' ORDER BY cls.relname Table 结束，接下来是 index\nSELECT DISTINCT ON(cls.relname) cls.oid, cls.relname as name, (SELECT (CASE WHEN count(i.inhrelid) \u003e 0 THEN true ELSE false END) FROM pg_inherits i WHERE i.inhrelid = cls.oid) as is_inherited FROM pg_catalog.pg_index idx JOIN pg_catalog.pg_class cls ON cls.oid=indexrelid JOIN pg_catalog.pg_class tab ON tab.oid=indrelid LEFT OUTER JOIN pg_catalog.pg_tablespace ta on ta.oid=cls.reltablespace JOIN pg_catalog.pg_namespace n ON n.oid=tab.relnamespace JOIN pg_catalog.pg_am am ON am.oid=cls.relam LEFT JOIN pg_catalog.pg_depend dep ON (dep.classid = cls.tableoid AND dep.objid = cls.oid AND dep.refobjsubid = '0' AND dep.refclassid=(SELECT oid FROM pg_catalog.pg_class WHERE relname='pg_constraint') AND dep.deptype='i') LEFT OUTER JOIN pg_catalog.pg_constraint con ON (con.tableoid = dep.refclassid AND con.oid = dep.refobjid) WHERE indrelid = 16410::OID AND conname is NULL ORDER BY cls.relname ROW SECURITY POLICY\n'SELECT pl.oid AS oid, pl.polname AS name FROM pg_catalog.pg_policy pl WHERE pl.polrelid\t= 16410 ORDER BY pl.polname;' trigger\n1 2 3 4 5 SELECT t.oid, t.tgname as name, t.tgenabled AS is_enable_trigger FROM pg_catalog.pg_trigger t WHERE NOT tgisinternal AND tgrelid = 16410::OID ORDER BY tgname; rules\n1 2 3 4 5 6 7 8 9 10 11 12 SELECT rw.oid AS oid, rw.rulename AS name, CASE WHEN rw.ev_enabled != \\'D\\' THEN True ELSE False END AS enabled, rw.ev_enabled AS is_enable_rule FROM pg_catalog.pg_rewrite rw WHERE rw.ev_class = 16410 ORDER BY rw.rulename ","description":"","tags":null,"title":"PgAmdin4 展示 DDL 语句逻辑分析","uri":"/posts/analysispgamdin4ddl/"},{"categories":["Tech"],"content":"获取 postgres 的建表语句的几种方法 使用 pg_dump 可以使用 pg_dump 直接查看建表语句 以本机 docker 的 postgres 为例\ndocker run -it --rm postgres pg_dump -h host.docker.internal -p 5432 -U postgres -d demo_ds_0 -s -t t_order_0 结果展示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 -- -- PostgreSQL database dump -- -- Dumped from database version 14.2 (Debian 14.2-1.pgdg110+1) -- Dumped by pg_dump version 14.2 (Debian 14.2-1.pgdg110+1) SET statement_timeout = 0; SET lock_timeout = 0; SET idle_in_transaction_session_timeout = 0; SET client_encoding = 'UTF8'; SET standard_conforming_strings = on; SELECT pg_catalog.set_config('search_path', '', false); SET check_function_bodies = false; SET xmloption = content; SET client_min_messages = warning; SET row_security = off; SET default_tablespace = ''; SET default_table_access_method = heap; -- -- Name: t_order_0; Type: TABLE; Schema: public; Owner: postgres -- CREATE TABLE public.t_order_0 ( order_id integer NOT NULL, user_id integer NOT NULL, status character varying(45) ); ALTER TABLE public.t_order_0 OWNER TO postgres; -- -- Name: TABLE t_order_0; Type: COMMENT; Schema: public; Owner: postgres -- COMMENT ON TABLE public.t_order_0 IS 'haha'; -- -- Name: COLUMN t_order_0.order_id; Type: COMMENT; Schema: public; Owner: postgres -- COMMENT ON COLUMN public.t_order_0.order_id IS 'haha'; -- -- Name: t_order_0 t_order_0_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres -- ALTER TABLE ONLY public.t_order_0 ADD CONSTRAINT t_order_0_pkey PRIMARY KEY (order_id); -- -- PostgreSQL database dump complete -- 使用自定义函数 创建函数如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 CREATE OR REPLACE FUNCTION tabledef(oid) RETURNS text LANGUAGE sql STRICT AS $$ /* snatched from https://github.com/filiprem/pg-tools */ WITH attrdef AS ( SELECT n.nspname, c.relname, pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts, c.relpersistence, a.attnum, a.attname, pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype, (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault, a.attnotnull, (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation \u003c\u003e t.typcollation) as attcollation, a.attidentity, a.attgenerated FROM pg_catalog.pg_attribute a JOIN pg_catalog.pg_class c ON a.attrelid = c.oid JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid) WHERE a.attrelid = $1 AND a.attnum \u003e 0 AND NOT a.attisdropped ORDER BY a.attnum ), coldef AS ( SELECT attrdef.nspname, attrdef.relname, attrdef.relopts, attrdef.relpersistence, pg_catalog.format( '%I %s%s%s%s%s', attrdef.attname, attrdef.atttype, case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %I', attrdef.attcollation) end, case when attrdef.attnotnull then ' NOT NULL' else '' end, case when attrdef.attdefault is null then '' else case when attrdef.attgenerated = 's' then pg_catalog.format(' GENERATED ALWAYS AS (%s) STORED', attrdef.attdefault) when attrdef.attgenerated \u003c\u003e '' then ' GENERATED AS NOT_IMPLEMENTED' else pg_catalog.format(' DEFAULT %s', attrdef.attdefault) end end, case when attrdef.attidentity\u003c\u003e'' then pg_catalog.format(' GENERATED %s AS IDENTITY', case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end) else '' end ) as col_create_sql FROM attrdef ORDER BY attrdef.attnum ), tabdef AS ( SELECT coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence, string_agg(coldef.col_create_sql, E',\\n ') as cols_create_sql FROM coldef GROUP BY coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence ) SELECT format( 'CREATE%s TABLE %I.%I%s%s%s;', case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end, tabdef.nspname, tabdef.relname, coalesce( (SELECT format(E'\\n PARTITION OF %I.%I %s\\n', pn.nspname, pc.relname, pg_get_expr(c.relpartbound, c.oid)) FROM pg_class c JOIN pg_inherits i ON c.oid = i.inhrelid JOIN pg_class pc ON pc.oid = i.inhparent JOIN pg_namespace pn ON pn.oid = pc.relnamespace WHERE c.oid = $1), format(E' (\\n %s\\n)', tabdef.cols_create_sql) ), case when tabdef.relopts \u003c\u003e '' then format(' WITH (%s)', tabdef.relopts) else '' end, coalesce(E'\\nPARTITION BY '||pg_get_partkeydef($1), '') ) as table_create_sql FROM tabdef $$; 通过 pgAdmin4 获取 抓包后，发现 pgAdmin4 实际查询的 sql 如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 SELECT rel.oid, rel.relname AS name, rel.reltablespace AS spcoid,rel.relacl AS relacl_str, (CASE WHEN length(spc.spcname::text) \u003e 0 OR rel.relkind = 'p' THEN spc.spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END) as spcname, (CASE rel.relreplident WHEN 'd' THEN 'default' WHEN 'n' THEN 'nothing' WHEN 'f' THEN 'full' WHEN 'i' THEN 'index' END) as replica_identity, (select nspname FROM pg_catalog.pg_namespace WHERE oid = 2200::oid ) as schema, pg_catalog.pg_get_userbyid(rel.relowner) AS relowner, rel.relkind, (CASE WHEN rel.relkind = 'p' THEN true ELSE false END) AS is_partitioned, rel.relhassubclass, rel.reltuples::bigint, des.description, con.conname, con.conkey, EXISTS(select 1 FROM pg_catalog.pg_trigger JOIN pg_catalog.pg_proc pt ON pt.oid=tgfoid AND pt.proname='logtrigger' JOIN pg_catalog.pg_proc pc ON pc.pronamespace=pt.pronamespace AND pc.proname='slonyversion' WHERE tgrelid=rel.oid) AS isrepl, (SELECT count(*) FROM pg_catalog.pg_trigger WHERE tgrelid=rel.oid AND tgisinternal = FALSE) AS triggercount, (SELECT ARRAY(SELECT CASE WHEN (nspname NOT LIKE 'pg\\_%') THEN pg_catalog.quote_ident(nspname)||'.'||pg_catalog.quote_ident(c.relname) ELSE pg_catalog.quote_ident(c.relname) END AS inherited_tables FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid ORDER BY inhseqno)) AS coll_inherits, (SELECT count(*) FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid) AS inherited_tables_cnt, (CASE WHEN rel.relpersistence = 'u' THEN true ELSE false END) AS relpersistence, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'fillfactor=([0-9]*)') AS fillfactor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'parallel_workers=([0-9]*)') AS parallel_workers, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'toast_tuple_target=([0-9]*)') AS toast_tuple_target, (substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS autovacuum_enabled, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS autovacuum_freeze_table_age, (substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS toast_autovacuum_enabled, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS toast_autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS toast_autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS toast_autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS toast_autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS toast_autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS toast_autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS toast_autovacuum_freeze_table_age, rel.reloptions AS reloptions, tst.reloptions AS toast_reloptions, rel.reloftype, CASE WHEN typ.typname IS NOT NULL THEN (select pg_catalog.quote_ident(nspname) FROM pg_catalog.pg_namespace WHERE oid = 2200::oid )||'.'||pg_catalog.quote_ident(typ.typname) ELSE typ.typname END AS typname, typ.typrelid AS typoid, rel.relrowsecurity as rlspolicy, rel.relforcerowsecurity as forcerlspolicy, (CASE WHEN rel.reltoastrelid = 0 THEN false ELSE true END) AS hastoasttable, (SELECT pg_catalog.array_agg(provider || '=' || label) FROM pg_catalog.pg_seclabels sl1 WHERE sl1.objoid=rel.oid AND sl1.objsubid=0) AS seclabels, (CASE WHEN rel.oid \u003c= 13756::oid THEN true ElSE false END) AS is_sys_table -- Added for partition table , (CASE WHEN rel.relkind = 'p' THEN pg_catalog.pg_get_partkeydef(16396::oid) ELSE '' END) AS partition_scheme FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=rel.oid AND des.objsubid=0 AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND NOT rel.relispartition AND rel.oid = 16396::oid ORDER BY rel.relname; https://github.com/postgres/pgadmin4\n","description":"","tags":null,"title":"How to get postgres create table sql","uri":"/posts/get_postgres_create_table_sql/"},{"categories":["Tech"],"content":"EventBus 的使用及注意点 使用 创建全局实例 1 static final EventBus INSTANCE = new EventBus(); 将含有 @Subscribe 的方法注册到全局实例上 1 INSTANCE.register(this); 发送 event 1 INSTANCE.post(event); 流程分析 采用上述方式时有以下几个注意点\n发送事件并且接受后处理事件是同步执行的，即同一个线程执行 如果在接受事件方法中有嵌套发送事件，那么该嵌套发送的事件不会立即执行，会等到第一个事件的接收方法完成后，再执行第二个事件。 EventBus 调用流程分析 创建 EventBus 实例 将订阅者注册到 EventBus 实例上 创建 EventBus 实例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public EventBus() { this(\"default\"); } /** * Creates a new EventBus with the given {@code identifier}. * * @param identifier a brief name for this bus, for logging purposes. Should be a valid Java * identifier. */ public EventBus(String identifier) { this( identifier, // 同步执行器 MoreExecutors.directExecutor(), Dispatcher.perThreadDispatchQueue(), LoggingHandler.INSTANCE); } 1 2 3 4 5 6 7 8 9 10 11 12 private static final class PerThreadQueuedDispatcher extends Dispatcher { // This dispatcher matches the original dispatch behavior of EventBus. /** Per-thread queue of events to dispatch. */ private final ThreadLocal\u003cQueue\u003cEvent\u003e\u003e queue = new ThreadLocal\u003cQueue\u003cEvent\u003e\u003e() { @Override protected Queue\u003cEvent\u003e initialValue() { return Queues.newArrayDeque(); } }; 注册 EventBus 订阅者 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 /** * Registers all subscriber methods on {@code object} to receive events. * * @param object object whose subscriber methods should be registered. */ public void register(Object object) { subscribers.register(object); } /** Registers all subscriber methods on the given listener object. */ void register(Object listener) { Multimap\u003cClass\u003c?\u003e, Subscriber\u003e listenerMethods = findAllSubscribers(listener); for (Entry\u003cClass\u003c?\u003e, Collection\u003cSubscriber\u003e\u003e entry : listenerMethods.asMap().entrySet()) { Class\u003c?\u003e eventType = entry.getKey(); Collection\u003cSubscriber\u003e eventMethodsInListener = entry.getValue(); CopyOnWriteArraySet\u003cSubscriber\u003e eventSubscribers = subscribers.get(eventType); if (eventSubscribers == null) { CopyOnWriteArraySet\u003cSubscriber\u003e newSet = new CopyOnWriteArraySet\u003c\u003e(); eventSubscribers = MoreObjects.firstNonNull(subscribers.putIfAbsent(eventType, newSet), newSet); } eventSubscribers.addAll(eventMethodsInListener); } } /** * Returns all subscribers for the given listener grouped by the type of event they subscribe to. */ private Multimap\u003cClass\u003c?\u003e, Subscriber\u003e findAllSubscribers(Object listener) { Multimap\u003cClass\u003c?\u003e, Subscriber\u003e methodsInListener = HashMultimap.create(); Class\u003c?\u003e clazz = listener.getClass(); for (Method method : getAnnotatedMethods(clazz)) { Class\u003c?\u003e[] parameterTypes = method.getParameterTypes(); Class\u003c?\u003e eventType = parameterTypes[0]; methodsInListener.put(eventType, Subscriber.create(bus, listener, method)); } return methodsInListener; } /** Creates a {@code Subscriber} for {@code method} on {@code listener}. */ static Subscriber create(EventBus bus, Object listener, Method method) { return isDeclaredThreadSafe(method) ? new Subscriber(bus, listener, method) // 调用方法采用 synchronized : new SynchronizedSubscriber(bus, listener, method); } 发送 event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 /** * Posts an event to all registered subscribers. This method will return successfully after the * event has been posted to all subscribers, and regardless of any exceptions thrown by * subscribers. * * \u003cp\u003eIf no subscribers have been subscribed for {@code event}'s class, and {@code event} is not * already a {@link DeadEvent}, it will be wrapped in a DeadEvent and reposted. * * @param event event to post. */ public void post(Object event) { Iterator\u003cSubscriber\u003e eventSubscribers = subscribers.getSubscribers(event); if (eventSubscribers.hasNext()) { dispatcher.dispatch(event, eventSubscribers); } else if (!(event instanceof DeadEvent)) { // the event had no subscribers and was not itself a DeadEvent post(new DeadEvent(this, event)); } } // 这里需要注意，如果当前线程在调用的 post 方法中，嵌套调用 post，第二个 post方法不会立刻执行，而是会加入 队列中，等到队列中第一个任务处理完，才会继续处理。 @Override void dispatch(Object event, Iterator\u003cSubscriber\u003e subscribers) { checkNotNull(event); checkNotNull(subscribers); Queue\u003cEvent\u003e queueForThread = queue.get(); queueForThread.offer(new Event(event, subscribers)); if (!dispatching.get()) { dispatching.set(true); try { Event nextEvent; while ((nextEvent = queueForThread.poll()) != null) { while (nextEvent.subscribers.hasNext()) { nextEvent.subscribers.next().dispatchEvent(nextEvent.event); } } } finally { dispatching.remove(); queue.remove(); } } } ","description":"","tags":null,"title":"guava EventBus 使用以及注意点","uri":"/posts/second/"},{"categories":["Tech"],"content":"背景 Apache ShardingSphere 基于用户的实际使用场景，为用户打造了多种实用功能，包括数据分片、读写分离等。在数据分片功能中，Apache ShardingSphere 提供了标准分片、复合分片等多种实用的分片策略，在各种分片策略中，用户又可以配置相关分片算法，从而解决数据分片的问题。在读写分离功能中，ShardingSphere 为用户提供了静态和动态的两种读写分离类型以及丰富的负载均衡算法以满足用户实际需求。 可以看到 ShardingSphere 的分片和读写分离功能已经非常丰富，不过用户的真实使用场景是千变万化的。以多租户场景为例，用户期望按照登录账号所属租户进行分片，但是租户信息却并不是存在于每条业务 SQL 中，这时从 SQL 中提取分片字段的算法将无法发挥作用。再以读写分离为例，大部分场景下用户都希望能够将查询操作路由到从库上执行，但是在某些实时性要求很高的场景下，用户希望将 SQL 强制路由到主库执行，这时读写分离就无法满足业务要求。 基于以上痛点 Apache ShardingSphere 为用户提供了 Hint 功能，用户可以结合实际业务场景，利用 SQL 外部的逻辑进行强制路由或者分片。目前 ShardingSphere 为用户提供了两种 Hint 方式，一种通过 Java API 手动编程，利用 HintManager 进行强制路由和分片，这种方式对采用 JDBC 编程的应用非常友好，只需要少量的代码编写，就能够轻松实现不依赖 SQL 的分片或者强制路由功能。另外一种方式对于不懂开发的 DBA 而言更加友好，ShardingSphere 基于分布式 SQL 提供的使用方式，利用 SQL HINT 和 DistSQL HINT， 为用户提供了无需编码就能实现的分片和强制路由功能。接下来，让我们一起了解下这两种使用方式。\n基于 HintManager 的手动编程 ShardingSphere 主要通过 HintManager 对象来实现强制路由和分片的功能。利用 HintManager，用户的分片将不用再依赖 SQL。它可以极大地扩展用户的使用场景，让用户可以更加灵活地进行数据分片或者强制路由。目前通过 HintManager，用户可以配合 ShardingSphere 内置的或者自定义的 Hint 算法实现分片功能，还可以通过设置指定数据源或者强制主库读写，实现强制路由功能。在学习 HintManager 的使用之前，让我们先来简单地了解一下它的实现原理，这有助于我们更好地使用它。\nHintManager 实现原理 其实通过查看 HintManager 代码，我们可以快速地了解它的原理。\n1 2 3 4 5 @NoArgsConstructor(access = AccessLevel.PRIVATE) public final class HintManager implements AutoCloseable { private static final ThreadLocal\u003cHintManager\u003e HINT_MANAGER_HOLDER = new ThreadLocal\u003c\u003e(); } 正如你所看到的，ShardingSphere 通过 ThreadLocal 来实现 HintManager 的功能，只要在同一个线程中，用户的分片设置都会得以保留。因此，只要用户在执行 SQL 之前调用 HintManager 相关功能，ShardingSphere 就能在当前线程中获取用户设置的分片或强制路由条件，从而进行分片或者路由操作。了解了 HintManager 的原理之后，让我们一起来学习一下它的使用。\nHintManager 的使用 使用 Hint 分片 Hint 分片算法需要用户实现 org.apache.shardingsphere.sharding.api.sharding.hint.HintShardingAlgorithm 接口。 Apache ShardingSphere 在进行路由时，将会从 HintManager 中获取分片值进行路由操作。\n参考配置如下： rules: - !SHARDING tables: t_order: actualDataNodes: demo_ds_${0..1}.t_order_${0..1} databaseStrategy: hint: algorithmClassName: xxx.xxx.xxx.HintXXXAlgorithm tableStrategy: hint: algorithmClassName: xxx.xxx.xxx.HintXXXAlgorithm defaultTableStrategy: none: defaultKeyGenerateStrategy: type: SNOWFLAKE column: order_id props: sql-show: true 获取 HintManager 实例 HintManager hintManager = HintManager.getInstance();\n添加分片键 使用 hintManager.addDatabaseShardingValue 来添加数据源分片键值。 使用 hintManager.addTableShardingValue 来添加表分片键值。 分库不分表情况下，强制路由至某一个分库时，可使用 hintManager.setDatabaseShardingValue 方式添加分片。 清除分片键值 分片键值保存在 ThreadLocal 中，所以需要在操作结束时调用 hintManager.close() 来清除 ThreadLocal 中的内容。 完整代码示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.addDatabaseShardingValue(\"t_order\", 1); hintManager.addTableShardingValue(\"t_order\", 2); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.setDatabaseShardingValue(3); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } 使用 Hint 强制主库路由 获取 HintManager 与基于 Hint 的数据分片相同。\n设置主库路由 使用 hintManager.setWriteRouteOnly 设置主库路由。 清除分片键值 与基于 Hint 的数据分片相同。\n完整代码示例 1 2 3 4 5 6 7 8 9 10 11 String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.setWriteRouteOnly(); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } 使用 Hint 路由至指定数据库 获取 HintManager 与基于 Hint 的数据分片相同。\n设置路由至指定数据库 使用 hintManager.setDataSourceName 设置数据库名称。 完整代码示例 1 2 3 4 5 6 7 8 9 10 11 String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.setDataSourceName(\"ds_0\"); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } 清除强制路由值 与基于 Hint 的数据分片相同。\n在了解了基于 HintManager 的手动编程方式之后，让我们一起来了解 ShardingSphere 基于分布式 SQL 提供的另一种 Hint 的解决方案。\n基于分布式 SQL 的 Hint Apache ShardingSphere 的分布式 SQL HINT 主要由两种功能组成，一种叫做 SQL HINT，即基于 SQL 注释的方式提供的功能，另外一种是通过 DistSQL 实现的作用于 HintManager 的功能。\nSQL HINT SQL HINT 就是通过在 SQL 语句上增加注释，从而实现强制路由的一种 Hint 方式。它降低了用户改造代码的成本，同时完全脱离了 Java API 的限制，不仅可以在 ShardingSphere-JDBC 中使用，也可以直接在 ShardingSphere-Proxy 上使用。 以下面 SQL 为例，即使用户配置了针对 t_order 的相关分片算法，该 SQL 也会直接在数据库 ds_0 上原封不动地执行，并返回执行结果。\n1 2 /* ShardingSphere hint: dataSourceName=ds_0 */ SELECT * FROM t_order; 通过注释的方式我们可以方便地将 SQL 直接送达指定数据库执行而无视其它分片逻辑。以多租户场景为例，用户不用再配置复杂的分库逻辑，也无需改造业务逻辑，只需要将指定库添加到注释信息中即可。在了解了 SQL HINT 的基本使用之后，让我们一起来了解一下 SQL HINT 的实现原理。\nSQL HINT 的实现原理 其实了解 Apache ShardingSphere 的读者朋友们一定对 SQL 解析引擎不会感到陌生。SQL HINT 实现的第一步就是提取 SQL 中的注释信息。利用 antlr4 的通道功能，可以将 SQL 中的注释信息单独送至特定的隐藏通道，ShardingSphere 也正是利用该功能，在生成解析结果的同时，将隐藏通道中的注释信息一并提取出来了。具体实现如下方代码所示。\n将 SQL 中的注释送入隐藏通道。 lexer grammar Comments; import Symbol; BLOCK_COMMENT: '/*' .*? '*/' -\u003e channel(HIDDEN); INLINE_COMMENT: (('-- ' | '#') ~[\\r\\n]* ('\\r'? '\\n' | EOF) | '--' ('\\r'? '\\n' | EOF)) -\u003e channel(HIDDEN); 访问语法树后增加对于注释信息的提取： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public \u003cT\u003e T visit(final ParseContext parseContext) { ParseTreeVisitor\u003cT\u003e visitor = SQLVisitorFactory.newInstance(databaseType, visitorType, SQLVisitorRule.valueOf(parseContext.getParseTree().getClass()), props); T result = parseContext.getParseTree().accept(visitor); appendSQLComments(parseContext, result); return result; } private \u003cT\u003e void appendSQLComments(final ParseContext parseContext, final T visitResult) { if (!parseContext.getHiddenTokens().isEmpty() \u0026\u0026 visitResult instanceof AbstractSQLStatement) { Collection\u003cCommentSegment\u003e commentSegments = parseContext.getHiddenTokens().stream().map(each -\u003e new CommentSegment(each.getText(), each.getStartIndex(), each.getStopIndex())) .collect(Collectors.toList()); ((AbstractSQLStatement) visitResult).getCommentSegments().addAll(commentSegments); } } 提取出用户 SQL 中的注释信息之后，我们就需要根据注释信息来进行相关强制路由了。既然是路由，那么自然就需要使用 Apache ShardingSphere 的路由引擎，我们在路由引擎上做了一些针对 HINT 的改造。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public RouteContext route(final LogicSQL logicSQL, final ShardingSphereMetaData metaData) { RouteContext result = new RouteContext(); Optional\u003cString\u003e dataSourceName = findDataSourceByHint(logicSQL.getSqlStatementContext(), metaData.getResource().getDataSources()); if (dataSourceName.isPresent()) { result.getRouteUnits().add(new RouteUnit(new RouteMapper(dataSourceName.get(), dataSourceName.get()), Collections.emptyList())); return result; } for (Entry\u003cShardingSphereRule, SQLRouter\u003e entry : routers.entrySet()) { if (result.getRouteUnits().isEmpty()) { result = entry.getValue().createRouteContext(logicSQL, metaData, entry.getKey(), props); } else { entry.getValue().decorateRouteContext(result, logicSQL, metaData, entry.getKey(), props); } } if (result.getRouteUnits().isEmpty() \u0026\u0026 1 == metaData.getResource().getDataSources().size()) { String singleDataSourceName = metaData.getResource().getDataSources().keySet().iterator().next(); result.getRouteUnits().add(new RouteUnit(new RouteMapper(singleDataSourceName, singleDataSourceName), Collections.emptyList())); } return result; } ShardingSphere 首先发现了符合定义的 SQL 注释，再经过基本的校验之后，就会直接返回用户指定的路由结果，从而实现强制路由功能。在了解了 SQL HINT 的基本原理之后，让我们一起学习如何使用 SQL HINT。\n如何使用 SQL HINT SQL HINT 的使用非常简单，无论是 ShardingSphere-JDBC 还是 ShardingSphere-Porxy，都可以使用。 第一步打开注释解析开关。将 sqlCommentParseEnabled 设置为 true。 第二步在 SQL 上增加注释即可。目前 SQL HINT 支持指定数据源路由和主库路由。\n指定数据源路由：目前只支持路由至一个数据源。 注释格式暂时只支持/* /，内容需要以ShardingSphere hint:开始，属性名为 dataSourceName。 / ShardingSphere hint: dataSourceName=ds_0 */ SELECT * FROM t_order;\n主库路由：注释格式暂时只支持/* /，内容需要以ShardingSphere hint:开始，属性名为 writeRouteOnly。 / ShardingSphere hint: writeRouteOnly=true */ SELECT * FROM t_order;\nDistSQL HINT Apache ShardingSphere 的 DistSQL 也提供了 Hint 相关功能，让用户可以通过 ShardingSphere-Proxy 来实现分片和强制路由功能。\nDistSQL HINT 的实现原理 同前文一致，在学习使用 DistSQL HINT 功能之前，让我们一起来了解一下 DistSQL Hint 的实现原理。DistSQL HINT 的实现原理非常简单，其实就是通过操作 HintManager 实现的 HINT 功能。以读写分离 Hint 为例，当用户通过 ShardingSphere-Proxy 执行以下 SQL 时，其实 ShardingSphere 内部对 SQL 做了如下方代码所示的操作。 – 强制主库读写\n1 set readwrite_splitting hint source = write 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @RequiredArgsConstructor public final class SetReadwriteSplittingHintExecutor extends AbstractHintUpdateExecutor\u003cSetReadwriteSplittingHintStatement\u003e { private final SetReadwriteSplittingHintStatement sqlStatement; @Override public ResponseHeader execute() { HintSourceType sourceType = HintSourceType.typeOf(sqlStatement.getSource()); switch (sourceType) { case AUTO: HintManagerHolder.get().setReadwriteSplittingAuto(); break; case WRITE: HintManagerHolder.get().setWriteRouteOnly(); break; default: break; } return new UpdateResponseHeader(new EmptyStatement()); } } @NoArgsConstructor(access = AccessLevel.PRIVATE) public final class HintManagerHolder { private static final ThreadLocal\u003cHintManager\u003e HINT_MANAGER_HOLDER = new ThreadLocal\u003c\u003e(); /** * Get an instance for {@code HintManager} from {@code ThreadLocal},if not exist,then create new one. * * @return hint manager */ public static HintManager get() { if (HINT_MANAGER_HOLDER.get() == null) { HINT_MANAGER_HOLDER.set(HintManager.getInstance()); } return HINT_MANAGER_HOLDER.get(); } /** * remove {@code HintManager} from {@code ThreadLocal}. */ public static void remove() { HINT_MANAGER_HOLDER.remove(); } } 用户执行 SQL 之后，DistSQL 解析引擎会首先识别出该 SQL 是读写分离 Hint 的 SQL，同时会提取出用户想要自动路由或者强制到主库的字段。之后它会采用 SetReadwriteSplittingHintExecutor 执行器去执行 SQL，从而将正确操作设置到 HintManager 中，进而实现强制路由主库的功能。\nDistSQL HINT 的使用 下表为大家展示了 DistSQL Hint 的相关语法。\n语句 说明 示例 set readwrite_splitting hint source = [auto / write] 针对当前连接，设置读写分离的路由策略（自动路由或强制到写库） set readwrite_splitting hint source = write set sharding hint database_value = yy 针对当前连接，设置 hint 仅对数据库分片有效，并添加分片值，yy：数据库分片值 set sharding hint database_value = 100 add sharding hint database_value xx = yy 针对当前连接，为表 xx 添加分片值 yy，xx：逻辑表名称，yy：数据库分片值 add sharding hint database_value t_order= 100 add sharding hint table_value xx = yy 针对当前连接，为表 xx 添加分片值 yy，xx：逻辑表名称，yy：表分片值 add sharding hint table_value t_order = 100 clear hint 针对当前连接，清除 hint 所有设置 clear hint clear [sharding hint / readwrite_splitting hint] 针对当前连接，清除 sharding 或 readwrite_splitting 的 hint 设置 clear readwrite_splitting hint show [sharding / readwrite_splitting] hint status 针对当前连接，查询 sharding 或 readwrite_splitting 的 hint 设置 show readwrite_splitting hint status 本文详细介绍了 Hint 使用的两种方式以及基本原理，相信通过本文，读者朋友们对 Hint 都有了一些基本了解了，大家可以根据自己的需求来选择使用合适的方式。如果在使用过程中遇到任何问题，或者有任何建议想法，都欢迎来社区反馈。\n","description":"","tags":null,"title":"ShardingSphere Hint 实用指南","uri":"/posts/my-first-post/"}]
