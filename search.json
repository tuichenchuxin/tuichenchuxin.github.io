[{"categories":null,"content":"前言 最近 deepseek 大火，所以我打算也追逐一下热点，学习下大模型的相关知识内容，拓宽一下自己的知识边界。\n","description":"","tags":null,"title":"大模型学习","uri":"/posts/ai-learn/"},{"categories":null,"content":"","description":"","tags":null,"title":"使用deepseek做一个帮老师写论文的助手工具","uri":"/posts/deepseek/"},{"categories":null,"content":"自从上次想支持 calcite benchmark 函数之后，通过 calcite 和 polardb-x 的一些调查，想直接支持是比较困难的，\n所以现在我转入了 calcite，考虑看能否直接提交一些 pr 来慢慢熟悉calcite 这个项目。\n查找可以提交的 jira 找了一段时间，也没找到啥方便支持的jira\nps: 发现了一个标签，可以找到新手方便参与的项目 jira project = CALCITE AND resolution = Unresolved AND labels in (newbie,easy-fix) ORDER BY priority DESC, updated DESC\n扫了一眼代码，发现 calcite 是可以支持 mongo 的，那么我探究看看，能够用 calcite 接入我本地的 mongo 项目，然后看看能否做一些提升。\n现在就是两个方向。\n使用 mongo adapter 来运行起来\n查找新手任务\n看了一圈新手任务，都是比较久远的了，而且看起来也不是那么容易实现。\n所以我还是先搞 mongo adapter 了。\n需求描述 启动客户端，通过 mysql 命令行连接\n后端通过 calcite 将 mysql 命令转化为 mongo\n通过 calcite 连接 mongo\n实现思路 参考 calcite 将 json 注册到 calcite 之后，通过 mysql 查数据的实现\nhttps://calcite.apache.org/docs/\n这篇文章告诉了我一些通过 calcite 查询 csv 的一些具体细节。以及规则应用和代价模型。\n我还需要进一步了解这个 sqlline 是怎么自动连上这个的，代码是如何生效的，然后再仿写一个 mongo 的。\n我先了解一下 sqlline 的大致原理和功能。\nsqlline本质上是一个动态JDBC命令行客户端，其核心架构分为三层：\ngraph TD A[命令行交互层] --\u003e B[JDBC元数据驱动层] B --\u003e C[SQL方言转换层] C --\u003e D[目标数据库驱动] 交互层：采用JLine库实现TAB补全、命令历史等交互功能\n驱动层：通过反射动态加载JDBC驱动（关键代码片段）：\n1 2 Class.forName(driver).getDeclaredConstructor() .newInstance().connect(url, props); 方言层：内置Hive/Phoenix等方言转换器，对Calcite使用ANSI SQL标准\n使用事例\n{ \"version\": \"1.0\", \"defaultSchema\": \"mongo\", \"schemas\": [{ \"name\": \"mongo\", \"type\": \"custom\", \"factory\": \"org.apache.calcite.adapter.mongodb.MongoSchemaFactory\", \"operand\": { \"host\": \"localhost\", \"database\": \"test\", \"collection\": \"users\" } }] } 1 2 3 ./sqlline -u \"jdbc:calcite:model=src/main/resources/mongo-model.json\" \\ -n admin -p admin \\ --driver=org.apache.calcite.jdbc.Driver 在 calcite /example/csv 下的 build.gradle.kts 中引入 mongodb 的依赖\ndependencies { api(project(\":core\")) api(project(\":file\")) api(project(\":linq4j\")) // mongodb api(project(\":mongodb\")) api(\"org.checkerframework:checker-qual\") 然后就可以连接上啦。\n那么接下来我应该做些什么呢？\n用 sqlline 源码调试 calcite mongodb 连接 下载 sqlline 通过代码 debug 连接 mongodb 和 测试 相关 sql。\n在 pom 中增加依赖\n1 2 3 4 5 6 7 8 9 10 \u003cdependency\u003e \u003cgroupId\u003eorg.apache.calcite\u003c/groupId\u003e \u003cartifactId\u003ecalcite-core\u003c/artifactId\u003e \u003cversion\u003e1.39.0-SNAPSHOT\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.calcite\u003c/groupId\u003e \u003cartifactId\u003ecalcite-mongodb\u003c/artifactId\u003e \u003cversion\u003e1.39.0-SNAPSHOT\u003c/version\u003e \u003c/dependency\u003e idea 上加上程序参数\nProgram arguments: -u \"jdbc:calcite:model=path/to/mongo-model.json\" -n ... 运行 SqlLine 类\n看起来展示有点儿问题。\n0: jdbc:calcite:model=/Users/chenchuxin/Docum\u003e !tables +--+ | | +--+ | | | | | | 找了网络信息，发现是 idea 的 debug 窗口 不支持 Jline，所以有这个问题。\nhttps://github.com/julianhyde/sqlline/issues/80 在这里找到了解决方案\n在program参数中添加这些后正常了\n--maxWidth=120 --maxHeight=2000 --color=true 0: jdbc:calcite:model=/Users/chenchuxin/Docum\u003e !tables +-----------+-------------+-------------------------------+--------------+---------+----------+------------+-----------+ | TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | +-----------+-------------+-------------------------------+--------------+---------+----------+------------+-----------+ | | metadata | COLUMNS | SYSTEM TABLE | | | | | | | metadata | TABLES | SYSTEM TABLE | | | | | | | mongo | absEntityDict | TABLE | | | | | | | mongo | actionBindingTag | TABLE | | | | | 这下就可以愉快的测试了\n试了下第一个 sql\n0: jdbc:calcite:model=/Users/chenchuxin/Docum\u003e select * from mechanism; 0: jdbc:calcite:model=/Users/chenchuxin/Docum\u003e Error: Error while executing SQL \"select * from mechanism\": From line 1, column 15 to line 1, column 23: Object 'MECHANISM' not found within 'mongo'; did you mean 'mechanism'? (state=,code=0) 奇怪，咋 select * 都不行，是支持的不完善么。\n试了下\nselect * from \"mechanism\"; 发现也是报错，但是表是找到了，debug 发现应该是元数据字段读取的有问题。\n经过一圈 debug 和查看源码的测试用例，发现mongodb 默认加了 _MAP 字段\n因此可以按照如下方式查询，哈哈，可以成功啦。\n0: jdbc:calcite:model=/Users/chenchuxin/Docum\u003e select cast(_MAP['name'] AS varchar(20)) AS name from \"mechanism\"; +----------------------------------------------------------------------------------------------------------------------+ | NAME | +----------------------------------------------------------------------------------------------------------------------+ | 专案机变更追踪机制 | | 分阶段出图应因机制 | | 旧料耗用机制 | 当然 deepseek 还给了其它方式，例如在配置中添加字段映射。\n开始测试 sql，看支持度，然后考虑解决不支持的 sql 哈哈，直接在 MongoAdapterTest 中发现了一些被标注 disable 的 test 可以修复一下了\nhttps://issues.apache.org/jira/browse/CALCITE-2115\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // @Disabled(\"broken; [CALCITE-2115] is logged to fix it\") @Test void testDistinctCount() { assertModel(MODEL) .query(\"select state, count(distinct city) as cdc from zips\\n\" + \"where state in ('CA', 'TX') group by state order by state\") .returns(\"STATE=CA; CDC=3\\n\" + \"STATE=TX; CDC=3\\n\") .queryContains( mongoChecker( \"{\\n\" + \" \\\"$match\\\": {\\n\" + \" \\\"$or\\\": [\\n\" + \" {\\n\" + \" \\\"state\\\": \\\"CA\\\"\\n\" + \" },\\n\" + \" {\\n\" + \" \\\"state\\\": \\\"TX\\\"\\n\" + \" }\\n\" + \" ]\\n\" + \" }\\n\" + \"}\", \"{$project: {CITY: '$city', STATE: '$state'}}\", \"{$group: {_id: {CITY: '$CITY', STATE: '$STATE'}}}\", \"{$project: {_id: 0, CITY: '$_id.CITY', STATE: '$_id.STATE'}}\", \"{$group: {_id: '$STATE', CDC: {$sum: {$cond: [ {$eq: ['CITY', null]}, 0, 1]}}}}\", \"{$project: {STATE: '$_id', CDC: '$CDC'}}\", \"{$sort: {STATE: 1}}\")); } 代码里找到了这个 待修复的测试用例\nhttps://issues.apache.org/jira/browse/CALCITE-2115\ndocker 搭建本地mongodb 环境 docker pull mongo docker run --name mongodb -e MONGO_INITDB_ROOT_USERNAME=mongoadmin -e MONGO_INITDB_ROOT_PASSWORD=secret -p 27017:27017 -d mongo:latest docker run -it --rm mongo mongosh --host host.docker.internal -u mongoadmin -p secret --authenticationDatabase admin some-db use test db.zips.insertMany([ { \"_id\" : \"01701\", \"city\" : \"FRAMINGHAM\", \"loc\" : [ -71.42548600000001, 42.300665 ], \"pop\" : 65046, \"state\" : \"MA\" }, { \"_id\" : \"02154\", \"city\" : \"NORTH WALTHAM\", \"loc\" : [ -71.236497, 42.382492 ], \"pop\" : 57871, \"state\" : \"MA\" }, { \"_id\" : \"02401\", \"city\" : \"BROCKTON\", \"loc\" : [ -71.03434799999999, 42.081571 ], \"pop\" : 59498, \"state\" : \"MA\" }, { \"_id\" : \"02840\", \"city\" : \"MIDDLETOWN\", \"loc\" : [ -71.30347999999999, 41.504502 ], \"pop\" : 47687, \"state\" : \"RI\" }, { \"_id\" : \"02860\", \"city\" : \"PAWTUCKET\", \"loc\" : [ -71.39071300000001, 41.872873 ], \"pop\" : 45442, \"state\" : \"RI\" }, { \"_id\" : \"02895\", \"city\" : \"NORTH SMITHFIELD\", \"loc\" : [ -71.513683, 41.99948 ], \"pop\" : 53733, \"state\" : \"RI\" }, { \"_id\" : \"03060\", \"city\" : \"NASHUA\", \"loc\" : [ -71.466684, 42.756395 ], \"pop\" : 41438, \"state\" : \"NH\" }, { \"_id\" : \"03103\", \"city\" : \"MANCHESTER\", \"loc\" : [ -71.449325, 42.965563 ], \"pop\" : 36613, \"state\" : \"NH\" }, { \"_id\" : \"03301\", \"city\" : \"CONCORD\", \"loc\" : [ -71.527734, 43.218525 ], \"pop\" : 34035, \"state\" : \"NH\" }, { \"_id\" : \"04240\", \"city\" : \"LEWISTON\", \"loc\" : [ -70.191619, 44.098538 ], \"pop\" : 40173, \"state\" : \"ME\" }, { \"_id\" : \"04401\", \"city\" : \"BANGOR\", \"loc\" : [ -68.791839, 44.824199 ], \"pop\" : 40434, \"state\" : \"ME\" }, { \"_id\" : \"05301\", \"city\" : \"BRATTLEBORO\", \"loc\" : [ -72.593322, 42.857353 ], \"pop\" : 17522, \"state\" : \"VT\" }, { \"_id\" : \"05401\", \"city\" : \"BURLINGTON\", \"loc\" : [ -73.219875, 44.484023 ], \"pop\" : 39127, \"state\" : \"VT\" }, { \"_id\" : \"05701\", \"city\" : \"RUTLAND\", \"loc\" : [ -72.97077299999999, 43.614131 ], \"pop\" : 22576, \"state\" : \"VT\" }, { \"_id\" : \"06010\", \"city\" : \"BRISTOL\", \"loc\" : [ -72.930193, 41.682293 ], \"pop\" : 60670, \"state\" : \"CT\" }, { \"_id\" : \"06450\", \"city\" : \"MERIDEN\", \"loc\" : [ -72.799734, 41.533396 ], \"pop\" : 59441, \"state\" : \"CT\" }, { \"_id\" : \"06902\", \"city\" : \"STAMFORD\", \"loc\" : [ -73.53742800000001, 41.052552 ], \"pop\" : 54605, \"state\" : \"CT\" }, { \"_id\" : \"07002\", \"city\" : \"BAYONNE\", \"loc\" : [ -74.119169, 40.666399 ], \"pop\" : 61444, \"state\" : \"NJ\" }, { \"_id\" : \"07087\", \"city\" : \"WEEHAWKEN\", \"loc\" : [ -74.030558, 40.768153 ], \"pop\" : 69646, \"state\" : \"NJ\" }, { \"_id\" : \"07111\", \"city\" : \"IRVINGTON\", \"loc\" : [ -74.23127100000001, 40.7261 ], \"pop\" : 60986, \"state\" : \"NJ\" }, { \"_id\" : \"10021\", \"city\" : \"NEW YORK\", \"loc\" : [ -73.958805, 40.768476 ], \"pop\" : 106564, \"state\" : \"NY\" }, { \"_id\" : \"11226\", \"city\" : \"BROOKLYN\", \"loc\" : [ -73.956985, 40.646694 ], \"pop\" : 111396, \"state\" : \"NY\" }, { \"_id\" : \"11373\", \"city\" : \"JACKSON HEIGHTS\", \"loc\" : [ -73.878551, 40.740388 ], \"pop\" : 88241, \"state\" : \"NY\" }, { \"_id\" : \"17042\", \"city\" : \"CLEONA\", \"loc\" : [ -76.425895, 40.335912 ], \"pop\" : 61993, \"state\" : \"PA\" }, { \"_id\" : \"18042\", \"city\" : \"FORKS TOWNSHIP\", \"loc\" : [ -75.23582, 40.6867 ], \"pop\" : 65784, \"state\" : \"PA\" }, { \"_id\" : \"19143\", \"city\" : \"PHILADELPHIA\", \"loc\" : [ -75.228819, 39.944815 ], \"pop\" : 80454, \"state\" : \"PA\" }, { \"_id\" : \"19711\", \"city\" : \"NEWARK\", \"loc\" : [ -75.737534, 39.701129 ], \"pop\" : 50573, \"state\" : \"DE\" }, { \"_id\" : \"19720\", \"city\" : \"MANOR\", \"loc\" : [ -75.589938, 39.67703 ], \"pop\" : 46906, \"state\" : \"DE\" }, { \"_id\" : \"19901\", \"city\" : \"DOVER\", \"loc\" : [ -75.535983, 39.156639 ], \"pop\" : 46005, \"state\" : \"DE\" }, { \"_id\" : \"20011\", \"city\" : \"WASHINGTON\", \"loc\" : [ -77.020251, 38.951786 ], \"pop\" : 62924, \"state\" : \"DC\" }, { \"_id\" : \"20301\", \"city\" : \"PENTAGON\", \"loc\" : [ -77.038196, 38.891019 ], \"pop\" : 21, \"state\" : \"DC\" }, { \"_id\" : \"21061\", \"city\" : \"GLEN BURNIE\", \"loc\" : [ -76.61886199999999, 39.158968 ], \"pop\" : 75692, \"state\" : \"MD\" }, { \"_id\" : \"21207\", \"city\" : \"GWYNN OAK\", \"loc\" : [ -76.734064, 39.329628 ], \"pop\" : 76002, \"state\" : \"MD\" }, { \"_id\" : \"21215\", \"city\" : \"BALTIMORE\", \"loc\" : [ -76.67939699999999, 39.344572 ], \"pop\" : 74402, \"state\" : \"MD\" }, { \"_id\" : \"22901\", \"city\" : \"CHARLOTTESVILLE\", \"loc\" : [ -78.490869, 38.054752 ], \"pop\" : 62708, \"state\" : \"VA\" }, { \"_id\" : \"23464\", \"city\" : \"VIRGINIA BEACH\", \"loc\" : [ -76.175909, 36.797772 ], \"pop\" : 67276, \"state\" : \"VA\" }, { \"_id\" : \"23602\", \"city\" : \"NEWPORT NEWS\", \"loc\" : [ -76.53212499999999, 37.131684 ], \"pop\" : 68525, \"state\" : \"VA\" }, { \"_id\" : \"25801\", \"city\" : \"BECKLEY\", \"loc\" : [ -81.206084, 37.793214 ], \"pop\" : 45196, \"state\" : \"WV\" }, { \"_id\" : \"26003\", \"city\" : \"ELM GROVE\", \"loc\" : [ -80.685126, 40.072736 ], \"pop\" : 49136, \"state\" : \"WV\" }, { \"_id\" : \"26505\", \"city\" : \"STAR CITY\", \"loc\" : [ -79.95422499999999, 39.633858 ], \"pop\" : 70185, \"state\" : \"WV\" }, { \"_id\" : \"27292\", \"city\" : \"LEXINGTON\", \"loc\" : [ -80.262049, 35.82306 ], \"pop\" : 69179, \"state\" : \"NC\" }, { \"_id\" : \"28677\", \"city\" : \"STATESVILLE\", \"loc\" : [ -80.894009, 35.799022 ], \"pop\" : 52895, \"state\" : \"NC\" }, { \"_id\" : \"29150\", \"city\" : \"OSWEGO\", \"loc\" : [ -80.32100800000001, 33.928199 ], \"pop\" : 46394, \"state\" : \"SC\" }, { \"_id\" : \"29501\", \"city\" : \"FLORENCE\", \"loc\" : [ -79.772786, 34.18375 ], \"pop\" : 66990, \"state\" : \"SC\" }, { \"_id\" : \"29801\", \"city\" : \"AIKEN\", \"loc\" : [ -81.71942900000001, 33.553024 ], \"pop\" : 51233, \"state\" : \"SC\" }, { \"_id\" : \"30032\", \"city\" : \"DECATUR\", \"loc\" : [ -84.263165, 33.740825 ], \"pop\" : 56056, \"state\" : \"GA\" }, { \"_id\" : \"30906\", \"city\" : \"PEACH ORCHARD\", \"loc\" : [ -82.038358, 33.402024 ], \"pop\" : 58646, \"state\" : \"GA\" }, { \"_id\" : \"32216\", \"city\" : \"JACKSONVILLE\", \"loc\" : [ -81.547387, 30.293907 ], \"pop\" : 58867, \"state\" : \"FL\" }, { \"_id\" : \"33012\", \"city\" : \"HIALEAH\", \"loc\" : [ -80.30589999999999, 25.865395 ], \"pop\" : 73194, \"state\" : \"FL\" }, { \"_id\" : \"33311\", \"city\" : \"FORT LAUDERDALE\", \"loc\" : [ -80.172786, 26.142104 ], \"pop\" : 65378, \"state\" : \"FL\" }, { \"_id\" : \"35215\", \"city\" : \"CENTER POINT\", \"loc\" : [ -86.693197, 33.635447 ], \"pop\" : 43862, \"state\" : \"AL\" }, { \"_id\" : \"35401\", \"city\" : \"TUSCALOOSA\", \"loc\" : [ -87.56266599999999, 33.196891 ], \"pop\" : 42124, \"state\" : \"AL\" }, { \"_id\" : \"35901\", \"city\" : \"SOUTHSIDE\", \"loc\" : [ -86.010279, 33.997248 ], \"pop\" : 44165, \"state\" : \"AL\" }, { \"_id\" : \"37042\", \"city\" : \"CLARKSVILLE\", \"loc\" : [ -87.418621, 36.585315 ], \"pop\" : 43296, \"state\" : \"TN\" }, { \"_id\" : \"37211\", \"city\" : \"NASHVILLE\", \"loc\" : [ -86.72403799999999, 36.072486 ], \"pop\" : 51478, \"state\" : \"TN\" }, { \"_id\" : \"38109\", \"city\" : \"MEMPHIS\", \"loc\" : [ -90.073238, 35.042538 ], \"pop\" : 60508, \"state\" : \"TN\" }, { \"_id\" : \"39180\", \"city\" : \"VICKSBURG\", \"loc\" : [ -90.85065, 32.325824 ], \"pop\" : 46968, \"state\" : \"MS\" }, { \"_id\" : \"39401\", \"city\" : \"HATTIESBURG\", \"loc\" : [ -89.306471, 31.314553 ], \"pop\" : 41866, \"state\" : \"MS\" }, { \"_id\" : \"39440\", \"city\" : \"LAUREL\", \"loc\" : [ -89.13115500000001, 31.705444 ], \"pop\" : 45040, \"state\" : \"MS\" }, { \"_id\" : \"40214\", \"city\" : \"LOUISVILLE\", \"loc\" : [ -85.77802699999999, 38.159318 ], \"pop\" : 42198, \"state\" : \"KY\" }, { \"_id\" : \"40216\", \"city\" : \"SHIVELY\", \"loc\" : [ -85.831771, 38.186138 ], \"pop\" : 41719, \"state\" : \"KY\" }, { \"_id\" : \"40601\", \"city\" : \"HATTON\", \"loc\" : [ -84.88061, 38.192831 ], \"pop\" : 46563, \"state\" : \"KY\" }, { \"_id\" : \"44035\", \"city\" : \"ELYRIA\", \"loc\" : [ -82.10508799999999, 41.372353 ], \"pop\" : 66674, \"state\" : \"OH\" }, { \"_id\" : \"44060\", \"city\" : \"MENTOR\", \"loc\" : [ -81.342133, 41.689468 ], \"pop\" : 60109, \"state\" : \"OH\" }, { \"_id\" : \"44107\", \"city\" : \"EDGEWATER\", \"loc\" : [ -81.79714300000001, 41.482654 ], \"pop\" : 59702, \"state\" : \"OH\" }, { \"_id\" : \"46360\", \"city\" : \"MICHIGAN CITY\", \"loc\" : [ -86.869899, 41.698031 ], \"pop\" : 55392, \"state\" : \"IN\" }, { \"_id\" : \"47130\", \"city\" : \"JEFFERSONVILLE\", \"loc\" : [ -85.735885, 38.307767 ], \"pop\" : 56543, \"state\" : \"IN\" }, { \"_id\" : \"47906\", \"city\" : \"WEST LAFAYETTE\", \"loc\" : [ -86.923661, 40.444025 ], \"pop\" : 54702, \"state\" : \"IN\" }, { \"_id\" : \"48180\", \"city\" : \"TAYLOR\", \"loc\" : [ -83.267269, 42.231738 ], \"pop\" : 70811, \"state\" : \"MI\" }, { \"_id\" : \"48185\", \"city\" : \"WESTLAND\", \"loc\" : [ -83.374908, 42.318882 ], \"pop\" : 84712, \"state\" : \"MI\" }, { \"_id\" : \"48227\", \"city\" : \"DETROIT\", \"loc\" : [ -83.193732, 42.388303 ], \"pop\" : 68390, \"state\" : \"MI\" }, { \"_id\" : \"50010\", \"city\" : \"AMES\", \"loc\" : [ -93.639398, 42.029859 ], \"pop\" : 52105, \"state\" : \"IA\" }, { \"_id\" : \"50317\", \"city\" : \"PLEASANT HILL\", \"loc\" : [ -93.549446, 41.612499 ], \"pop\" : 39883, \"state\" : \"IA\" }, { \"_id\" : \"52001\", \"city\" : \"DUBUQUE\", \"loc\" : [ -90.68191400000001, 42.514977 ], \"pop\" : 41934, \"state\" : \"IA\" }, { \"_id\" : \"53209\", \"city\" : \"MILWAUKEE\", \"loc\" : [ -87.947834, 43.118765 ], \"pop\" : 51008, \"state\" : \"WI\" }, { \"_id\" : \"54401\", \"city\" : \"WAUSAU\", \"loc\" : [ -89.633955, 44.963433 ], \"pop\" : 51083, \"state\" : \"WI\" }, { \"_id\" : \"54901\", \"city\" : \"OSHKOSH\", \"loc\" : [ -88.54363499999999, 44.021962 ], \"pop\" : 57187, \"state\" : \"WI\" }, { \"_id\" : \"55106\", \"city\" : \"SAINT PAUL\", \"loc\" : [ -93.048817, 44.968384 ], \"pop\" : 47905, \"state\" : \"MN\" }, { \"_id\" : \"55112\", \"city\" : \"NEW BRIGHTON\", \"loc\" : [ -93.199691, 45.074129 ], \"pop\" : 44128, \"state\" : \"MN\" }, { \"_id\" : \"55337\", \"city\" : \"BURNSVILLE\", \"loc\" : [ -93.275283, 44.76086 ], \"pop\" : 51421, \"state\" : \"MN\" }, { \"_id\" : \"57103\", \"city\" : \"SIOUX FALLS\", \"loc\" : [ -96.686415, 43.537386 ], \"pop\" : 32508, \"state\" : \"SD\" }, { \"_id\" : \"57401\", \"city\" : \"ABERDEEN\", \"loc\" : [ -98.485642, 45.466109 ], \"pop\" : 28786, \"state\" : \"SD\" }, { \"_id\" : \"57701\", \"city\" : \"ROCKERVILLE\", \"loc\" : [ -103.200259, 44.077041 ], \"pop\" : 45328, \"state\" : \"SD\" }, { \"_id\" : \"58103\", \"city\" : \"FARGO\", \"loc\" : [ -96.812252, 46.856406 ], \"pop\" : 38483, \"state\" : \"ND\" }, { \"_id\" : \"58501\", \"city\" : \"BISMARCK\", \"loc\" : [ -100.774755, 46.823448 ], \"pop\" : 36602, \"state\" : \"ND\" }, { \"_id\" : \"58701\", \"city\" : \"MINOT\", \"loc\" : [ -101.298476, 48.22914 ], \"pop\" : 42195, \"state\" : \"ND\" }, { \"_id\" : \"59102\", \"city\" : \"BILLINGS\", \"loc\" : [ -108.572662, 45.781265 ], \"pop\" : 40121, \"state\" : \"MT\" }, { \"_id\" : \"59601\", \"city\" : \"HELENA\", \"loc\" : [ -112.021283, 46.613066 ], \"pop\" : 40102, \"state\" : \"MT\" }, { \"_id\" : \"59801\", \"city\" : \"MISSOULA\", \"loc\" : [ -114.025207, 46.856274 ], \"pop\" : 33811, \"state\" : \"MT\" }, { \"_id\" : \"60623\", \"city\" : \"CHICAGO\", \"loc\" : [ -87.7157, 41.849015 ], \"pop\" : 112047, \"state\" : \"IL\" }, { \"_id\" : \"60634\", \"city\" : \"NORRIDGE\", \"loc\" : [ -87.796054, 41.945213 ], \"pop\" : 69160, \"state\" : \"IL\" }, { \"_id\" : \"60650\", \"city\" : \"CICERO\", \"loc\" : [ -87.76008, 41.84776 ], \"pop\" : 67670, \"state\" : \"IL\" }, { \"_id\" : \"63031\", \"city\" : \"FLORISSANT\", \"loc\" : [ -90.340097, 38.806865 ], \"pop\" : 52659, \"state\" : \"MO\" }, { \"_id\" : \"63116\", \"city\" : \"SAINT LOUIS\", \"loc\" : [ -90.26254299999999, 38.581356 ], \"pop\" : 49014, \"state\" : \"MO\" }, { \"_id\" : \"63136\", \"city\" : \"JENNINGS\", \"loc\" : [ -90.260189, 38.738878 ], \"pop\" : 54994, \"state\" : \"MO\" }, { \"_id\" : \"66502\", \"city\" : \"MANHATTAN\", \"loc\" : [ -96.585776, 39.193757 ], \"pop\" : 50178, \"state\" : \"KS\" }, { \"_id\" : \"67212\", \"city\" : \"WICHITA\", \"loc\" : [ -97.438344, 37.700683 ], \"pop\" : 41349, \"state\" : \"KS\" }, { \"_id\" : \"67401\", \"city\" : \"BAVARIA\", \"loc\" : [ -97.60878700000001, 38.823802 ], \"pop\" : 45208, \"state\" : \"KS\" }, { \"_id\" : \"68104\", \"city\" : \"OMAHA\", \"loc\" : [ -95.999888, 41.29186 ], \"pop\" : 35325, \"state\" : \"NE\" }, { \"_id\" : \"68502\", \"city\" : \"LINCOLN\", \"loc\" : [ -96.693763, 40.789282 ], \"pop\" : 27576, \"state\" : \"NE\" }, { \"_id\" : \"68847\", \"city\" : \"KEARNEY\", \"loc\" : [ -99.077883, 40.713608 ], \"pop\" : 28674, \"state\" : \"NE\" }, { \"_id\" : \"70072\", \"city\" : \"MARRERO\", \"loc\" : [ -90.110462, 29.859756 ], \"pop\" : 58905, \"state\" : \"LA\" }, { \"_id\" : \"70117\", \"city\" : \"NEW ORLEANS\", \"loc\" : [ -90.03124, 29.970298 ], \"pop\" : 56494, \"state\" : \"LA\" }, { \"_id\" : \"70560\", \"city\" : \"NEW IBERIA\", \"loc\" : [ -91.819959, 30.001027 ], \"pop\" : 56105, \"state\" : \"LA\" }, { \"_id\" : \"72032\", \"city\" : \"CONWAY\", \"loc\" : [ -92.423574, 35.084199 ], \"pop\" : 43236, \"state\" : \"AR\" }, { \"_id\" : \"72076\", \"city\" : \"GRAVEL RIDGE\", \"loc\" : [ -92.13043500000001, 34.881985 ], \"pop\" : 37428, \"state\" : \"AR\" }, { \"_id\" : \"72401\", \"city\" : \"JONESBORO\", \"loc\" : [ -90.69652600000001, 35.833016 ], \"pop\" : 53532, \"state\" : \"AR\" }, { \"_id\" : \"73034\", \"city\" : \"EDMOND\", \"loc\" : [ -97.47983499999999, 35.666483 ], \"pop\" : 43814, \"state\" : \"OK\" }, { \"_id\" : \"73505\", \"city\" : \"LAWTON\", \"loc\" : [ -98.455234, 34.617939 ], \"pop\" : 45542, \"state\" : \"OK\" }, { \"_id\" : \"74801\", \"city\" : \"SHAWNEE\", \"loc\" : [ -96.931321, 35.34907 ], \"pop\" : 40076, \"state\" : \"OK\" }, { \"_id\" : \"78207\", \"city\" : \"SAN ANTONIO\", \"loc\" : [ -98.52596699999999, 29.422855 ], \"pop\" : 58355, \"state\" : \"TX\" }, { \"_id\" : \"78521\", \"city\" : \"BROWNSVILLE\", \"loc\" : [ -97.461236, 25.922103 ], \"pop\" : 79463, \"state\" : \"TX\" }, { \"_id\" : \"78572\", \"city\" : \"ALTON\", \"loc\" : [ -98.342647, 26.24153 ], \"pop\" : 67604, \"state\" : \"TX\" }, { \"_id\" : \"80123\", \"city\" : \"BOW MAR\", \"loc\" : [ -105.07766, 39.596854 ], \"pop\" : 59418, \"state\" : \"CO\" }, { \"_id\" : \"80221\", \"city\" : \"FEDERAL HEIGHTS\", \"loc\" : [ -105.007985, 39.840562 ], \"pop\" : 54069, \"state\" : \"CO\" }, { \"_id\" : \"80631\", \"city\" : \"GARDEN CITY\", \"loc\" : [ -104.704756, 40.413968 ], \"pop\" : 53905, \"state\" : \"CO\" }, { \"_id\" : \"82001\", \"city\" : \"CHEYENNE\", \"loc\" : [ -104.796234, 41.143719 ], \"pop\" : 33107, \"state\" : \"WY\" }, { \"_id\" : \"82070\", \"city\" : \"LARAMIE\", \"loc\" : [ -105.581146, 41.312907 ], \"pop\" : 29327, \"state\" : \"WY\" }, { \"_id\" : \"82716\", \"city\" : \"GILLETTE\", \"loc\" : [ -105.497442, 44.282009 ], \"pop\" : 25968, \"state\" : \"WY\" }, { \"_id\" : \"83301\", \"city\" : \"TWIN FALLS\", \"loc\" : [ -114.469265, 42.556495 ], \"pop\" : 34539, \"state\" : \"ID\" }, { \"_id\" : \"83704\", \"city\" : \"BOISE\", \"loc\" : [ -116.295099, 43.633001 ], \"pop\" : 40912, \"state\" : \"ID\" }, { \"_id\" : \"83814\", \"city\" : \"COEUR D ALENE\", \"loc\" : [ -116.784976, 47.692841 ], \"pop\" : 33589, \"state\" : \"ID\" }, { \"_id\" : \"84118\", \"city\" : \"KEARNS\", \"loc\" : [ -111.98521, 40.652759 ], \"pop\" : 55999, \"state\" : \"UT\" }, { \"_id\" : \"84120\", \"city\" : \"WEST VALLEY CITY\", \"loc\" : [ -112.009783, 40.68708 ], \"pop\" : 52854, \"state\" : \"UT\" }, { \"_id\" : \"84604\", \"city\" : \"PROVO\", \"loc\" : [ -111.654906, 40.260681 ], \"pop\" : 43841, \"state\" : \"UT\" }, { \"_id\" : \"85023\", \"city\" : \"PHOENIX\", \"loc\" : [ -112.111838, 33.632383 ], \"pop\" : 54668, \"state\" : \"AZ\" }, { \"_id\" : \"85204\", \"city\" : \"MESA\", \"loc\" : [ -111.789554, 33.399168 ], \"pop\" : 55180, \"state\" : \"AZ\" }, { \"_id\" : \"85364\", \"city\" : \"YUMA\", \"loc\" : [ -114.642362, 32.701507 ], \"pop\" : 57131, \"state\" : \"AZ\" }, { \"_id\" : \"87501\", \"city\" : \"POJOAQUE VALLEY\", \"loc\" : [ -105.974818, 35.702472 ], \"pop\" : 51715, \"state\" : \"NM\" }, { \"_id\" : \"88001\", \"city\" : \"LAS CRUCES\", \"loc\" : [ -106.746034, 32.321641 ], \"pop\" : 57502, \"state\" : \"NM\" }, { \"_id\" : \"88201\", \"city\" : \"ROSWELL\", \"loc\" : [ -104.525857, 33.388504 ], \"pop\" : 53644, \"state\" : \"NM\" }, { \"_id\" : \"89031\", \"city\" : \"NORTH LAS VEGAS\", \"loc\" : [ -115.124832, 36.206228 ], \"pop\" : 48113, \"state\" : \"NV\" }, { \"_id\" : \"89115\", \"city\" : \"LAS VEGAS\", \"loc\" : [ -115.067062, 36.215818 ], \"pop\" : 51532, \"state\" : \"NV\" }, { \"_id\" : \"89502\", \"city\" : \"RENO\", \"loc\" : [ -119.776395, 39.497239 ], \"pop\" : 38332, \"state\" : \"NV\" }, { \"_id\" : \"90011\", \"city\" : \"LOS ANGELES\", \"loc\" : [ -118.258189, 34.007856 ], \"pop\" : 96074, \"state\" : \"CA\" }, { \"_id\" : \"90201\", \"city\" : \"BELL GARDENS\", \"loc\" : [ -118.17205, 33.969177 ], \"pop\" : 99568, \"state\" : \"CA\" }, { \"_id\" : \"90650\", \"city\" : \"NORWALK\", \"loc\" : [ -118.081767, 33.90564 ], \"pop\" : 94188, \"state\" : \"CA\" }, { \"_id\" : \"96734\", \"city\" : \"KAILUA\", \"loc\" : [ -157.744781, 21.406262 ], \"pop\" : 53403, \"state\" : \"HI\" }, { \"_id\" : \"96744\", \"city\" : \"KANEOHE\", \"loc\" : [ -157.811543, 21.422819 ], \"pop\" : 55236, \"state\" : \"HI\" }, { \"_id\" : \"96818\", \"city\" : \"HONOLULU\", \"loc\" : [ -157.926925, 21.353173 ], \"pop\" : 62915, \"state\" : \"HI\" }, { \"_id\" : \"97005\", \"city\" : \"BEAVERTON\", \"loc\" : [ -122.805395, 45.475035 ], \"pop\" : 46660, \"state\" : \"OR\" }, { \"_id\" : \"97206\", \"city\" : \"PORTLAND\", \"loc\" : [ -122.59727, 45.483995 ], \"pop\" : 43134, \"state\" : \"OR\" }, { \"_id\" : \"97301\", \"city\" : \"SALEM\", \"loc\" : [ -122.979692, 44.926039 ], \"pop\" : 48007, \"state\" : \"OR\" }, { \"_id\" : \"98031\", \"city\" : \"KENT\", \"loc\" : [ -122.193184, 47.388004 ], \"pop\" : 50515, \"state\" : \"WA\" }, { \"_id\" : \"98059\", \"city\" : \"RENTON\", \"loc\" : [ -122.151178, 47.467383 ], \"pop\" : 48197, \"state\" : \"WA\" }, { \"_id\" : \"98310\", \"city\" : \"BREMERTON\", \"loc\" : [ -122.629913, 47.601916 ], \"pop\" : 49057, \"state\" : \"WA\" }, { \"_id\" : \"99504\", \"city\" : \"ANCHORAGE\", \"loc\" : [ -149.74467, 61.203696 ], \"pop\" : 32383, \"state\" : \"AK\" }, { \"_id\" : \"99709\", \"city\" : \"FAIRBANKS\", \"loc\" : [ -147.846917, 64.85437 ], \"pop\" : 23238, \"state\" : \"AK\" }, { \"_id\" : \"99801\", \"city\" : \"JUNEAU\", \"loc\" : [ -134.529429, 58.362767 ], \"pop\" : 24947, \"state\" : \"AK\" } ] ) db.zips.aggregate([ { $match: { state: { $in: [\"CA\", \"TX\"] } }}, { $project: { STATE: \"$state\", CITY: \"$city\" } }, { $group: { _id: { STATE: \"$STATE\", CITY: \"$CITY\" } } }, { $group: { _id: \"$_id.STATE\", CDC: { $sum: 1 } } }, { $sort: { _id: 1 } } ]) 回到测试的这个case 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // @Disabled(\"broken; [CALCITE-2115] is logged to fix it\") @Test void testDistinctCount() { assertModel(MODEL) .query(\"select state, count(distinct city) as cdc from zips\\n\" + \"where state in ('CA', 'TX') group by state order by state\") .returns(\"STATE=CA; CDC=3\\n\" + \"STATE=TX; CDC=3\\n\") .queryContains( mongoChecker( \"{\\n\" + \" \\\"$match\\\": {\\n\" + \" \\\"$or\\\": [\\n\" + \" {\\n\" + \" \\\"state\\\": \\\"CA\\\"\\n\" + \" },\\n\" + \" {\\n\" + \" \\\"state\\\": \\\"TX\\\"\\n\" + \" }\\n\" + \" ]\\n\" + \" }\\n\" + \"}\", \"{$project: {STATE: '$state', CITY: '$city'}}\", \"{$group: {_id: {CITY: '$CITY', STATE: '$STATE'}}}\", \"{$project: {_id: 0, CITY: '$_id.CITY', STATE: '$_id.STATE'}}\", \"{$group: {_id: '$STATE', CDC: {$sum: {$cond: [ {$eq: ['CITY', null]}, 0, 1]}}}}\", \"{$project: {STATE: '$_id', CDC: '$CDC'}}\", \"{$sort: {STATE: 1}}\")); } 我发现这个实现中，group 和 count distinct 的实现并不是通过 mongo 语句实现的，而是通过代码和 api 实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @Override public Result implement(EnumerableRelImplementor implementor, Prefer pref) { // Generates a call to \"find\" or \"aggregate\", depending upon whether // an aggregate is present. // // ((MongoTable) schema.getTable(\"zips\")).find( // \"{state: 'CA'}\", // \"{city: 1, zipcode: 1}\") // // ((MongoTable) schema.getTable(\"zips\")).aggregate( // \"{$filter: {state: 'CA'}}\", // \"{$group: {_id: '$city', c: {$sum: 1}, p: {$sum: \"$pop\"}}\") final BlockBuilder list = new BlockBuilder(); final MongoRel.Implementor mongoImplementor = new MongoRel.Implementor(getCluster().getRexBuilder()); mongoImplementor.visitChild(0, getInput()); final RelDataType rowType = getRowType(); final PhysType physType = PhysTypeImpl.of( implementor.getTypeFactory(), rowType, pref.prefer(JavaRowFormat.ARRAY)); final Expression fields = list.append(\"fields\", constantArrayList( Pair.zip(MongoRules.mongoFieldNames(rowType), new AbstractList\u003cClass\u003e() { @Override public Class get(int index) { return physType.fieldClass(index); } @Override public int size() { return rowType.getFieldCount(); } }), Pair.class)); final Expression table = list.append(\"table\", mongoImplementor.table.getExpression( MongoTable.MongoQueryable.class)); List\u003cString\u003e opList = mongoImplementor.list.rightList(); final Expression ops = list.append(\"ops\", constantArrayList(opList, String.class)); // 这里是生成 agg 的逻辑 Expression enumerable = list.append(\"enumerable\", Expressions.call(table, MongoMethod.MONGO_QUERYABLE_AGGREGATE.method, fields, ops)); if (CalciteSystemProperty.DEBUG.value()) { System.out.println(\"Mongo: \" + opList); } Hook.QUERY_PLAN.run(opList); list.add( Expressions.return_(null, enumerable)); return implementor.result(physType, list.toBlock()); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 private Enumerable\u003cObject\u003e aggregate(final MongoDatabase mongoDb, final List\u003cMap.Entry\u003cString, Class\u003e\u003e fields, final List\u003cString\u003e operations) { final List\u003cBson\u003e list = new ArrayList\u003c\u003e(); for (String operation : operations) { list.add(BsonDocument.parse(operation)); } final Function1\u003cDocument, Object\u003e getter = MongoEnumerator.getter(fields); return new AbstractEnumerable\u003cObject\u003e() { @Override public Enumerator\u003cObject\u003e enumerator() { final Iterator\u003cDocument\u003e resultIterator; try { resultIterator = mongoDb.getCollection(collectionName) .aggregate(list).iterator(); } catch (Exception e) { throw new RuntimeException(\"While running MongoDB query \" + Util.toString(operations, \"[\", \",\\n\", \"]\"), e); } return new MongoEnumerator(resultIterator, getter); } }; } distinct 看起来是linq4j 中实现的\n1 2 3 4 5 6 7 8 9 10 11 public static \u003cTSource\u003e Enumerable\u003cTSource\u003e distinct( Enumerable\u003cTSource\u003e enumerable, EqualityComparer\u003cTSource\u003e comparer) { if (comparer == Functions.identityComparer()) { return distinct(enumerable); } final Set\u003cWrapped\u003cTSource\u003e\u003e set = new HashSet\u003c\u003e(); Function1\u003cTSource, Wrapped\u003cTSource\u003e\u003e wrapper = wrapperFor(comparer); Function1\u003cWrapped\u003cTSource\u003e, TSource\u003e unwrapper = unwrapper(); enumerable.select(wrapper).into(set); return Linq4j.asEnumerable(set).select(unwrapper); } 那么修复这个就简单了，需要去掉多余的语句即可。至于为什么原来有实现，后面改成代码了，就暂时不知道了。\n提交 pr https://github.com/apache/calcite/pull/4228\n","description":"","tags":null,"title":"在calcite提交pull request","uri":"/posts/calcite_pr/"},{"categories":null,"content":"今天想通过 arthas watch 一个方法，并过滤入参，死活写不出来。折腾了半天终于搞出来了\n例如我想监控这个方法\n1 2 3 public \u003cT\u003e List\u003cT\u003e find(Query query, Class\u003cT\u003e entityClass) { return this.\u003cT\u003efind(query, entityClass, this.determineCollectionName(entityClass)); } toString 输出的结构体如下 @Query[Query: { \"code\" : \"A1b4a_6437_task_0006\", \"tenantId\" : { \"$in\" : [\"athenaPaaSDesigner\", \"SYSTEM\"] }, \"version\" : \"2.0\" }, Fields: { }, Sort: { }] 1 2 3 4 5 6 7 8 9 10 11 12 13 public Document getQueryObject() { Document document = new Document(); for(CriteriaDefinition definition : this.criteria.values()) { document.putAll(definition.getCriteriaObject()); } if (!this.restrictedTypes.isEmpty()) { document.put(\"_$RESTRICTED_TYPES\", this.getRestrictedTypes()); } return document; } 最终的方式如下，getQueryObject() 返回的是一个 map，然后通过 get 方法过滤并判空\nwatch org.springframework.data.mongodb.core.MongoTemplate find \"{params, returnObj}\" \"params[0].getQueryObject().get('code') != null \u0026\u0026 params[0].getQueryObject().get('code').equals('A1b4a_6437_task_0006')\" -s -x 2 ","description":"","tags":null,"title":"Arthas watch ognl map 过滤","uri":"/posts/arthas_use/"},{"categories":null,"content":"背景 写了个功能，线上一跑，pod 就重启了。估计是 OOM 了。\n调查过程 于是本地准备模拟一下场景，分析下大对象。\n修改运行时参数\n-Xms200m -Xmx200m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/Users/chenchuxin/Documents/dgw/km-deployer-service/ 这样可以得到 hprof 文件，idea 就可以分析\n很容易发现大对象是 bson 文件，看起来是 1000 一批还是太多了，因为是 document 对象，所以可能存储很多内容。\n调下批次处理数量就可以了。\n","description":"","tags":null,"title":"idea 修改配置，模拟分析 OOM","uri":"/posts/oom_analysis/"},{"categories":null,"content":"需求 想支持下 https://dev.mysql.com/doc/refman/8.4/en/information-functions.html#function_benchmark\n1 2 3 4 5 6 7 mysql\u003e SELECT BENCHMARK(1000000,AES_ENCRYPT('hello','goodbye')); +---------------------------------------------------+ | BENCHMARK(1000000,AES_ENCRYPT('hello','goodbye')) | +---------------------------------------------------+ | 0 | +---------------------------------------------------+ 1 row in set (4.74 sec) 得先了解下嵌套函数的实现\n又折腾了一周 polardbx 的启动，终于能启动了。\n试一下 它是否能支持。 嗯，暂不支持，不过错误提示的挺好的。\nmysql\u003e SELECT BENCHMARK(1000000,AES_ENCRYPT('hello','goodbye')); ERROR 4998 (HY000): [1900df69ae400000][192.168.200.77:8527][sharding_db]ERR-CODE: [PXC-4998][ERR_NOT_SUPPORT] function BENCHMARK not support yet! 接下来做些啥呢？ 看看嵌套函数咋实现的。\n这个嵌套函数可以实现\nmysql\u003e select GREATEST(max(1),min(2)); +--------------------------+ | GREATEST(max(1), min(2)) | +--------------------------+ | 2 | +--------------------------+ 1 row in set (0.18 sec) 优化逻辑 经过解析和 validate. polardb-x 会通过 coverter 将 sql 转化成 RelNode\nrel#503:LogicalProject.NONE.[].any.[](input=LogicalAggregate#502,GREATEST(max(1), min(2))=GREATEST($0, $1)) RBO 优化 HepPlanner 添加很多变换规则。\nsetRoot() 时会创建 graph\n可以看到创建了edge（边）和对应的 vertex . 边记录了这些 vertex 的联系。\nexecuteProgram()\n循环各个vertex 并应用所有规则\n这个case 没有找到应用的规则，看起来是应用规则，并 transform vertex\n最终构建出\nrel#513:LogicalProject.NONE.[].any.[](input=LogicalAggregate#511,GREATEST(max(1), min(2))=GREATEST($0, $1)) CBO 优化 addCBORules 根据配置添加规则\nVolcanoPlanner setRoot()\n调用 registerImpl() 并且计算代价保存到 relsubset ，并且筛选规则 drive()\n应用规则 当前这个case 走的是 TopDownRuleDriver task.perform() 的过程中会不断产生 task。中间使用规则时，会进行 transform ，并且重新计算 cost。\n我的粗浅理解就是使用规则再算一次 cost,从而方便找到最小代价的执行计划。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Override public void drive() { TaskDescriptor description = new TaskDescriptor(); // Starting from the root's OptimizeGroup task. tasks.push( new OptimizeGroup( requireNonNull(planner.root, \"planner.root\"), planner.infCost)); // Ensure materialized view roots get explored. // Note that implementation rules or enforcement rules are not applied // unless the mv is matched. exploreMaterializationRoots(); try { // Iterates until the root is fully optimized. while (!tasks.isEmpty()) { Task task = tasks.pop(); description.log(task); task.perform(); } } catch (VolcanoTimeoutException ex) { LOGGER.warn(\"Volcano planning times out, cancels the subsequent optimization.\"); } } CBO 优化之后的结果\nrel#534:PhysicalProject.DRDS.[].any.[](input=HashAgg#533,GREATEST(max(1), min(2))=GREATEST($0, $1)) 执行逻辑 遍历 pysicalPlan 根据节点的不同，产生不同的执行器。\nLocalExecutionPlanner.plan() 方法会遍历节点生成 PipelineDepTree\n这里存着调用顺序已经相互依赖的关系\n通过 TaskExecutor.enqueueSplits() 开始实际调度执行\n调用之后，内外部如何传递结果暂时还没找到在哪儿搞的\n回到如何实现 benchmark 上来 select concat(1, max(10));\nrel#741:PhysicalProject.DRDS.[].any.[](input=HashAgg#739,concat(1, max(10))=CONCAT(?0, $0)) 先把 benchmark 当成 concat 来实现 搜索 concat 相关关键词，然后仿照着增加 benchmark。\n哈哈，走通了\nNo connection. Trying to reconnect... Connection id: 2 Current database: sharding_db +-----------------------------------------------------+ | BENCHMARK(1000000, AES_ENCRYPT('hello', 'goodbye')) | +-----------------------------------------------------+ | 0 | +-----------------------------------------------------+ 接下来就是要调整下逻辑适配一下\nrel#523:PhysicalProject.DRDS.[].any.[](input=LogicalValues#514,BENCHMARK(1000000, AES_ENCRYPT('hello', 'goodbye'))=BENCHMARK(?0, AES_ENCRYPT(?1, ?2))) 现在这个执行逻辑是先执行 AES_ENCRYPT 再将结果放到 BENCHMARK 方法中去执行。\n但是我需要在 BENCHMARK 中执行 AES_ENCRYPT。\n再找找已有的实现，嵌套后执行的。\n找了一圈也没有找到已有的实现方式，应该是一个回调实现，感觉跟执行顺序有关。\ndebug 发现如下逻辑, 它会遍历内部的参数，如果是方法的话，会调用计算结果并返回。\n所以考虑根据 benchmark 生成特定的执行类，而不是当前的这个，从而控制流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public class ScalarFunctionExpression extends AbstractExpression { private List\u003cIExpression\u003e args; private IScalarFunction function; private ExprContextProvider contextHolder; public ScalarFunctionExpression() { } public List\u003cIExpression\u003e getArgs() { return args; } @Override public Object eval(Row row, ExecutionContext ec) { if (function != null) { Object[] actualArgs = new Object[args.size()]; for (int i = 0; i \u003c args.size(); i++) { actualArgs[i] = args.get(i).eval(row, ec); } // function.setArgs(Arrays.asList(actualArgs)); Object result = function.compute(actualArgs, ec); DataType returnType = function.getReturnType(); return returnType.convertFrom(result); } else { GeneralUtil.nestedException(\"invoke function of null\"); } return null; } 调整逻辑, 成功实现 修改 compute 逻辑，将 callback 类型的不计算，直接传入\n修改 benchmark 逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class Benchmark extends AbstractCallBackScalarFunction { public Benchmark(List\u003cDataType\u003e operandTypes, DataType resultType) { super(operandTypes, resultType); } @Override public String[] getFunctionNames() { return new String[]{\"BENCHMARK\"}; } @Override public Object compute(Object[] args, ExecutionContext ec) { if (args[1] instanceof ScalarFunctionExpression \u0026\u0026 args[0] instanceof Number) { for (int i = 0; i \u003c ((Number) args[0]).intValue(); i++) { ((ScalarFunctionExpression) args[1]).eval(null, ec); } // TODO support args[1] is select } return 0; } } 考虑 select 的实现 先尝试下这个查询\n1 SELECT BENCHMARK(1000000, (SELECT 1)); 1 SELECT BENCHMARK(2, (SELECT id from sbtest_sharding_c limit 1)); 发现这两个查询都可以正常执行，但是很奇怪断点没有走到 benchmark 函数中。\nrel#1300:PhysicalProject.DRDS.[].any.[](input=LogicalCorrelate#1298,BENCHMARK(3, ( SELECT id FROM sbtest_sharding_c LIMIT 1 ))=BENCHMARK(?0, $1),variablesSet=[$cor0]) 看起来没走到是因为 benchmark 的参数类型跟函数实际的不匹配。\nbenchmark 应该也是属于是流程控制类函数。\npolarDB-X benchmark 的实现 我去，居然发现了 polardbx 的 benchmark 有一些实现\nBenchmarkVectorizedExpression 现在倒是走通了，但是好像是歪路啊。\n接下来应该是把我的修改给去掉一部分。然后实现debug 看它原来支持的 benchmark.\n我把改动都去除之后，发现这个 sql 是可以走通的，那么就要探究下流程，然后看看把 BENCHMARK(2, AES_ENCRYPT(‘hello’,‘goodbye’)) 走通。\n1 SELECT BENCHMARK(2, (SELECT id from sbtest_sharding_c limit 1)); rel#651:PhysicalProject.DRDS.[].any.[](input=LogicalCorrelate#649,BENCHMARK(3, ( SELECT id FROM sbtest_sharding_c LIMIT 1 ))=BENCHMARK(?0, $1),variablesSet=[$cor0]) 为什么 debug 发现虽然走到了 eval， 但是在内部都没有实际进行计算，很奇怪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public void eval(EvaluationContext ctx) { MutableChunk chunk = ctx.getPreAllocatedChunk(); int batchSize = chunk.batchSize(); boolean isSelectionInUse = chunk.isSelectionInUse(); int[] sel = chunk.selection(); // benchmark // / \\ // count function VectorizedExpression child = children[1]; for (int i = 0; i \u003c count; i++) { // try count times child.eval(ctx); } RandomAccessBlock outputVectorSlot = chunk.slotIn(outputIndex, outputDataType); long[] res = (outputVectorSlot.cast(LongBlock.class)).longArray(); if (isSelectionInUse) { for (int i = 0; i \u003c batchSize; i++) { int j = sel[i]; res[j] = 0L; } } else { for (int i = 0; i \u003c batchSize; i++) { res[i] = 0L; } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class InputRefVectorizedExpression extends AbstractVectorizedExpression { private final int inputIndex; public InputRefVectorizedExpression(DataType\u003c?\u003e dataType, int inputIndex, int outputIndex) { super(dataType, outputIndex, new VectorizedExpression[0]); this.inputIndex = inputIndex; } @Override public void eval(EvaluationContext ctx) { // 总是相等的 if (outputIndex != inputIndex) { MutableChunk chunk = ctx.getPreAllocatedChunk(); boolean isSelectionInUse = chunk.isSelectionInUse(); int[] selection = chunk.selection(); chunk.slotIn(outputIndex).copySelected(isSelectionInUse, selection, chunk.batchSize(), chunk.slotIn(inputIndex)); } } } 看起来是架子搭好了，但是 children 没有正确的传入，另外，benchmark(3,AES_ENCRYPT(‘HELLO’,‘WORLD’)) 也没有走到这个场景。\n目前还没具体方法，未知的太多，例如 vectoriedExpression 是否支持 select 等。\nhttps://github.com/polardb/polardbx-sql/issues/228 我提了个issue，期待大佬回答指导下。\n先看一下 concat 函数的实现逻辑\nmysql\u003e SELECT CONCAT(3, (SELECT id from sbtest_sharding_id limit 1)); +———————————————————-+ | CONCAT(3, ( SELECT id FROM sbtest_sharding_id LIMIT 1 )) | +———————————————————-+ | 3100001 | +———————————————————-+ 1 row in set (8.99 sec)\n看执行流程，似乎是 SELECT id from sbtest_sharding_id limit 1 被作为子查询，查询后再进行 concat 计算了。\nrel#1069:PhysicalProject.DRDS.[].any.[](input=LogicalCorrelate#1067,CONCAT(4, ( SELECT id FROM sbtest_sharding_id LIMIT 1 ))=CONCAT(?0, $1),variablesSet=[$cor0]) 看起来这个和 concat 没啥差别呀\n那么它为什么会走到子查询逻辑，而 benchmark 不行，需要探究下这点。\nbenchmark 和 concat 现在逻辑有何区别 1 2 3 SELECT CONCAT(3, (SELECT id from sbtest_sharding_id limit 1)); SELECT BENCHMARK(3, (SELECT id from sbtest_sharding_id limit 1)); 所以 benchmark 应该是要封装多次调用查询才行。\n但是benchmark 也是跟concat 一样，先调用了 SELECT id from sbtest_sharding_id limit 1 并获取结果。\n所以想要支持，核心是让他能多次调用。\n也就是 VectorizedProjectExec 类，需要看是否还是使用这个类来执行才行。需要调整执行顺序。\nVectorizedProjectExec 内部封装了 CorrelateExec 执行类。\n所以需要着重看一下，这些执行器是如何构建的。\n看起来执行器的构建是根据优化的结果来决定的。\n群里问了些大佬，感觉还是需要从执行器这边来支持。\n又看了下代码，如果想支持，就不要用 VectorizedProjectExec,例如下面这里，input 使用的 CorrelateExecutor, input.nextChunk() 就会将 benchmark 中的内容计算并返回。\n所以新的执行器应该是先进行 evaluate，然后在计算过程中，使用类似 CorrelateExec 来进行子查询，但是需要循环执行\n1 2 3 4 5 6 7 8 9 10 11 12 @Override Chunk doNextChunk() { Chunk inputChunk = input.nextChunk(); if (inputChunk == null) { return null; } for (int i = 0; i \u003c expressions.size(); i++) { if (mappedColumnIndex[i] == -1) { evaluateExpression(i, inputChunk); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @Override Chunk doNextChunk() { if (closed || isFinish) { forceClose(); return null; } if (currentChunk == null) { // read next chunk from input currentChunk = left.nextChunk(); if (currentChunk == null) { isFinish = left.produceIsFinished(); blocked = left.produceIsBlocked(); return null; } applyRowIndex = 0; } if (hasConstantValue) { for (; applyRowIndex \u003c currentChunk.getPositionCount(); applyRowIndex++) { // apply of applyRowIndex's row is finished, compute result for (int i = 0; i \u003c left.getDataTypes().size(); i++) { currentChunk.getBlock(i).writePositionTo(applyRowIndex, blockBuilders[i]); } if (constantValue == RexDynamicParam.DYNAMIC_SPECIAL_VALUE.EMPTY) { blockBuilders[getDataTypes().size() - 1].writeObject(null); } else { blockBuilders[getDataTypes().size() - 1].writeObject(outColumnType.convertFrom(constantValue)); } } currentChunk = null; return buildChunkAndReset(); } // 这里就是查询逻辑，也就是说修改后，是需要多次执行的 if (curSubqueryApply == null) { curSubqueryApply = createSubqueryApply(applyRowIndex); curSubqueryApply.prepare(); } curSubqueryApply.process(); if (curSubqueryApply.isFinished()) { curSubqueryApply.close(); if ((leftConditions == null || leftConditions.size() == 0) \u0026\u0026 isValueConstant) { constantValue = curSubqueryApply.getResultValue(); hasConstantValue = true; } // apply of applyRowIndex's row is finished, compute result for (int i = 0; i \u003c left.getDataTypes().size(); i++) { currentChunk.getBlock(i).writePositionTo(applyRowIndex, blockBuilders[i]); } if (curSubqueryApply.getResultValue() == RexDynamicParam.DYNAMIC_SPECIAL_VALUE.EMPTY) { blockBuilders[getDataTypes().size() - 1].writeObject(null); } else { blockBuilders[getDataTypes().size() - 1].writeObject( getDataTypes().get(getDataTypes().size() - 1) .convertFrom(curSubqueryApply.getResultValue())); } curSubqueryApply = null; if (++applyRowIndex == currentChunk.getPositionCount()) { applyRowIndex = 0; currentChunk = null; return buildChunkAndReset(); } } else { blocked = curSubqueryApply.isBlocked(); } return null; } 结论 基于以上分析，要支持，可以考虑动优化的结果，不使用如下的内容，不过应该生成成哪种形式，还需要考虑，如果直接在计划树上，封装多次引用关系，这样也不友好，可能循环 100次计划树就特别长了。\nrel#651:PhysicalProject.DRDS.[].any.[](input=LogicalCorrelate#649,BENCHMARK(3, ( SELECT id FROM sbtest_sharding_c LIMIT 1 ))=BENCHMARK(?0, $1),variablesSet=[$cor0]) 第二种方式，就是还是动执行器，写一个针对benchmark 的执行器。来支持，但是这种魔改的方式，从我现在比较片面的角度来改造，还不太成熟。\n基于这种考虑，我还是暂时放弃 benchmark 的支持，转而去看看 calcite 社区的一些任务，让我能更加熟悉这个技术框架。\n","description":"","tags":null,"title":"Calcite支持benchmark函数","uri":"/posts/calcite_benchmark/"},{"categories":null,"content":"问题描述 org.mockito.exceptions.base.MockitoException: Mockito cannot mock this class: class com.digiwin.athena.datamap.service.inner.DataMapPickService. If you're not sure why you're getting this error, please open an issue on GitHub. Java : 1.8 JVM vendor name : Oracle Corporation JVM vendor version : 25.11-b03 JVM name : Java HotSpot(TM) 64-Bit Server VM JVM version : 1.8.0_11-b12 JVM info : mixed mode OS name : Mac OS X OS version : 10.16 You are seeing this disclaimer because Mockito is configured to create inlined mocks. You can learn about inline mocks and their limitations under item #39 of the Mockito class javadoc. Underlying exception : java.lang.IllegalArgumentException: Could not create type Caused by: java.lang.IllegalArgumentException: Could not create type Caused by: java.lang.VerifyError 一直报错，但是本地却没有。\n调查过程 查询发现 Caused by: java.lang.VerifyError 可能是由于 jvm 验证到内存和class 可能不同，报出来的问题。\n于是下载对应版本，本地复现，确实能复现该问题。\n那么是哪个类不一致呢。\norg.mockito.exceptions.base.MockitoException: Mockito cannot mock this class: class org.springframework.data.mongodb.core.MongoTemplate. If you're not sure why you're getting this error, please open an issue on GitHub. Java : 1.8 JVM vendor name : Oracle Corporation JVM vendor version : 25.11-b03 JVM name : Java HotSpot(TM) 64-Bit Server VM JVM version : 1.8.0_11-b12 JVM info : mixed mode OS name : Mac OS X OS version : 10.16 You are seeing this disclaimer because Mockito is configured to create inlined mocks. You can learn about inline mocks and their limitations under item #39 of the Mockito class javadoc. Underlying exception : java.lang.IllegalArgumentException: Could not create type at org.mockito.junit.jupiter.MockitoExtension.beforeEach(MockitoExtension.java:153) at java.util.ArrayList.forEach(ArrayList.java:1234) at java.util.ArrayList.forEach(ArrayList.java:1234) Suppressed: java.lang.NullPointerException at org.mockito.junit.jupiter.MockitoExtension.afterEach(MockitoExtension.java:184) ... 2 more Caused by: java.lang.IllegalArgumentException: Could not create type at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:170) at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:399) at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:190) at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:410) at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:40) at org.mockito.internal.creation.bytebuddy.InlineDelegateByteBuddyMockMaker.createMockType(InlineDelegateByteBuddyMockMaker.java:396) at org.mockito.internal.creation.bytebuddy.InlineDelegateByteBuddyMockMaker.doCreateMock(InlineDelegateByteBuddyMockMaker.java:355) at org.mockito.internal.creation.bytebuddy.InlineDelegateByteBuddyMockMaker.createMock(InlineDelegateByteBuddyMockMaker.java:334) at org.mockito.internal.creation.bytebuddy.InlineByteBuddyMockMaker.createMock(InlineByteBuddyMockMaker.java:56) at org.mockito.internal.util.MockUtil.createMock(MockUtil.java:99) at org.mockito.internal.MockitoCore.mock(MockitoCore.java:88) at org.mockito.Mockito.mock(Mockito.java:2037) at org.mockito.internal.configuration.MockAnnotationProcessor.processAnnotationForMock(MockAnnotationProcessor.java:74) at org.mockito.internal.configuration.MockAnnotationProcessor.process(MockAnnotationProcessor.java:28) at org.mockito.internal.configuration.MockAnnotationProcessor.process(MockAnnotationProcessor.java:25) at org.mockito.internal.configuration.IndependentAnnotationEngine.createMockFor(IndependentAnnotationEngine.java:44) at org.mockito.internal.configuration.IndependentAnnotationEngine.process(IndependentAnnotationEngine.java:72) at org.mockito.internal.configuration.InjectingAnnotationEngine.processIndependentAnnotations(InjectingAnnotationEngine.java:73) at org.mockito.internal.configuration.InjectingAnnotationEngine.process(InjectingAnnotationEngine.java:47) at org.mockito.MockitoAnnotations.openMocks(MockitoAnnotations.java:81) at org.mockito.internal.framework.DefaultMockitoSession.\u003cinit\u003e(DefaultMockitoSession.java:43) at org.mockito.internal.session.DefaultMockitoSessionBuilder.startMocking(DefaultMockitoSessionBuilder.java:83) ... 3 more Caused by: java.lang.VerifyError at sun.instrument.InstrumentationImpl.retransformClasses0(Native Method) at sun.instrument.InstrumentationImpl.retransformClasses(InstrumentationImpl.java:144) at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.triggerRetransformation(InlineBytecodeGenerator.java:280) at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.mockClass(InlineBytecodeGenerator.java:217) at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.lambda$mockClass$0(TypeCachingBytecodeGenerator.java:47) at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$$Lambda$342/1424439581.call(Unknown Source) at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:168) ... 24 more debug 也没有太多的发现。\n如果去掉这个依赖，就可以规避这个问题。\n\u003cdependency\u003e \u003cgroupId\u003eorg.mockito\u003c/groupId\u003e \u003cartifactId\u003emockito-inline\u003c/artifactId\u003e \u003cversion\u003e${mockito.version}\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e 在 mockito github 上也找到了类似的 issue\nhttps://github.com/mockito/mockito/issues/2079 不过没有后续了。\n我理解上应该是 mockito 会修改 class 对应的字节文件，但是这样的修改没能通过 jvm 的检测，但是如何拿到修改后的字节码文件呢。\n暂时结论 先去掉 mockito-inline jar 包了，不用 mockStatic 方式。\n定位起来这个问题比较麻烦，盲点比较多，mockito 框架可以进入我的待贡献清单了。\n进一步定位 通过 ai 的帮忙，我试图找到 verifyError 的原因。 需要定位执行 mockito 前后的字节码差异。\n需要利用 java agent 技术来获取执行后的文件字节码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package org.example; import java.io.FileOutputStream; import java.io.IOException; import java.lang.instrument.ClassFileTransformer; import java.lang.instrument.Instrumentation; import java.security.ProtectionDomain; public class BytecodeSaverAgent { public static void premain(String agentArgs, Instrumentation inst) { inst.addTransformer(new ClassFileTransformer() { @Override public byte[] transform(ClassLoader loader, String className, Class\u003c?\u003e classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) { if (className.equals(\"org/springframework/data/mongodb/core/MongoTemplate\")) { try (FileOutputStream fos = new FileOutputStream(\"transformed_MongoTemplate.class\")) { fos.write(classfileBuffer); } catch (IOException e) { e.printStackTrace(); } } return classfileBuffer; } }); } 看起来是类加载时候会调用它。 于是我终于通过如下命令获取了对应的字节码\njavap -c -p -v /Users/chenchuxin/Documents/dgw/data_map/datamap_backend/develop/transformed_MongoTemplate.class\u003e MongoTemplate_bytecode_after.txt diff 后发现没有什么不同啊。\ndiff MongoTemplate_bytecode.txt MongoTemplate_bytecode_after.txt 1,2c1,2 \u003c Classfile /Users/chenchuxin/.m2/repository/org/springframework/data/spring-data-mongodb/2.0.6.RELEASE/spring-data-mongodb-2.0.6.RELEASE/org/springframework/data/mongodb/core/MongoTemplate.class \u003c Last modified 04-Apr-2018; size 105592 bytes --- \u003e Classfile /Users/chenchuxin/Documents/dgw/data_map/datamap_backend/develop/transformed_MongoTemplate.class \u003e Last modified 07-Nov-2024; size 105592 bytes 通过进一步 debug 发现这个 agnet 打出来的类并不是转换之后的字节码文件。\n算了，不太好搞了。\n告一段落 这个问题确实比较棘手。一个时 mocktio 框架还不太了解细节。\n二是定位字节码转换前后的差异比较麻烦。\n不过定位过程比较有趣。 mocktio 和 agent 技术确实值得进一步了解。\n","description":"","tags":null,"title":"本地测试可以运行，但是环境上 mockito java.lang.VerifyError","uri":"/posts/test_error/"},{"categories":null,"content":"现象 单独运行单元测试都是通过的，但是 mvn test 一起运行时就报错了。\n调查过程 开启 debug 模式，执行mvn test 时，发现 threadLocal 对象的值来自于其它 case.\n由于同一个模块执行 test 时是单线程串行的。所以应该是 treadLocal 泄漏。\n从而发现是测试用例没有清理 treadLocal.\n增加清理逻辑后正常。\n","description":"","tags":null,"title":"单元测试单独运行ok, mvn test 却失败","uri":"/posts/test_occasional_failure/"},{"categories":null,"content":"今天本地调试项目发现长日志始终打不出来，然后 debug 发现, flush 之后，原来打出来一半的日志就会消失\n1 2 3 4 5 6 protected void directEncodeEvent(final LogEvent event) { getLayout().encode(event, manager); if (this.immediateFlush || event.isEndOfBatch()) { manager.flush(); } } 然后我发现超过 2048 之后，会分批输出，奇怪的事为啥输出一半，后一半继续输出之后，就消失了呢。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 static void encodeText(final CharsetEncoder charsetEncoder, final CharBuffer charBuf, final ByteBuffer byteBuf, final StringBuilder text, final ByteBufferDestination destination) throws CharacterCodingException { charsetEncoder.reset(); if (text.length() \u003e charBuf.capacity()) { encodeChunkedText(charsetEncoder, charBuf, byteBuf, text, destination); return; } charBuf.clear(); text.getChars(0, text.length(), charBuf.array(), charBuf.arrayOffset()); charBuf.limit(text.length()); final CoderResult result = charsetEncoder.encode(charBuf, byteBuf, true); writeEncodedText(charsetEncoder, charBuf, byteBuf, destination, result); } 最后发现这里有个折叠标志，原来超过之后 idea 自动折叠了。。哈哈一个小乌龙\n","description":"","tags":null,"title":"Slf4j 控制台日志丢失问题调查","uri":"/posts/slf4j_console_log_missing/"},{"categories":null,"content":"背景 最近升级了公司框架，然后发现错误日志一直在刷。\n09-29 09:14:01.001 [o-32622-exec-64] WARN .p.p.t.d.i.CoyoteOutputStreamInterceptor -- before. Caused:null java.lang.ArrayIndexOutOfBoundsException: null at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.logResponseValue(DefaultDigiwinHttpBodyDataHolder.java:155) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext.logResponseValue(DefaultDigiwinHttpBodyContext.java:66) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.plugin.tomcat.digiwin.interceptor.CoyoteOutputStreamInterceptor.before(CoyoteOutputStreamInterceptor.java:38) ~[pinpoint-tomcat-plugin-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.interceptor.ExceptionHandleAroundInterceptor.before(ExceptionHandleAroundInterceptor.java:35) ~[?:2.5.1-p1] at org.apache.catalina.connector.CoyoteOutputStream.write(CoyoteOutputStream.java) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.springframework.util.FastByteArrayOutputStream.writeTo(FastByteArrayOutputStream.java:248) ~[spring-core-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.springframework.web.util.ContentCachingResponseWrapper.copyBodyToResponse(ContentCachingResponseWrapper.java:228) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.springframework.web.util.ContentCachingResponseWrapper.copyBodyToResponse(ContentCachingResponseWrapper.java:212) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at com.digiwin.gateway.filter.StandardHeaderFilter.doFilter(StandardHeaderFilter.java:66) ~[dwapiplatform-filter-5.2.0.1117.jar:5.2.0.1117] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:199) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:544) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:143) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:364) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:616) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:831) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1629) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372] 09-29 09:14:01.001 [o-32622-exec-64] WARN .p.p.t.d.i.CoyoteOutputStreamInterceptor -- before. Caused:null java.lang.ArrayIndexOutOfBoundsException: null at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.logResponseValue(DefaultDigiwinHttpBodyDataHolder.java:155) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext.logResponseValue(DefaultDigiwinHttpBodyContext.java:66) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.plugin.tomcat.digiwin.interceptor.CoyoteOutputStreamInterceptor.before(CoyoteOutputStreamInterceptor.java:38) ~[pinpoint-tomcat-plugin-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.interceptor.ExceptionHandleAroundInterceptor.before(ExceptionHandleAroundInterceptor.java:35) ~[?:2.5.1-p1] at org.apache.catalina.connector.CoyoteOutputStream.write(CoyoteOutputStream.java) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.springframework.util.FastByteArrayOutputStream.writeTo(FastByteArrayOutputStream.java:251) ~[spring-core-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.springframework.web.util.ContentCachingResponseWrapper.copyBodyToResponse(ContentCachingResponseWrapper.java:228) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.springframework.web.util.ContentCachingResponseWrapper.copyBodyToResponse(ContentCachingResponseWrapper.java:212) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at com.digiwin.gateway.filter.StandardHeaderFilter.doFilter(StandardHeaderFilter.java:66) ~[dwapiplatform-filter-5.2.0.1117.jar:5.2.0.1117] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.0.5.RELEASE.jar:5.0.5.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:199) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:544) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:143) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:364) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:616) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:831) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1629) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372] 09-29 09:12:55.055 [o-32622-exec-32] ERROR n.p.p.c.DefaultDigiwinHttpBodyDataHolder -- exact body error code exception: com.google.gson.JsonSyntaxException: com.google.gson.stream.MalformedJsonException: Use JsonReader.setLenient(true) to accept malformed JSON at line 1 column 241 path $ at com.google.gson.Gson.assertFullConsumption(Gson.java:1148) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1138) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1047) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:982) ~[gson-2.10.1.jar:?] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.close(DefaultDigiwinHttpBodyDataHolder.java:177) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext.close(DefaultDigiwinHttpBodyContext.java:48) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.plugin.request.ServletRequestListener.destroyed(ServletRequestListener.java:192) ~[?:2.5.1-p1] at com.navercorp.pinpoint.plugin.tomcat.javax.interceptor.StandardHostValveInvokeInterceptor.after(StandardHostValveInvokeInterceptor.java:152) ~[pinpoint-tomcat-plugin-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.interceptor.ExceptionHandleAroundInterceptor.after(ExceptionHandleAroundInterceptor.java:44) ~[?:2.5.1-p1] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:196) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:364) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:616) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:831) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1629) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372] Caused by: com.google.gson.stream.MalformedJsonException: Use JsonReader.setLenient(true) to accept malformed JSON at line 1 column 241 path $ at com.google.gson.stream.JsonReader.syntaxError(JsonReader.java:1659) ~[gson-2.10.1.jar:?] at com.google.gson.stream.JsonReader.checkLenient(JsonReader.java:1465) ~[gson-2.10.1.jar:?] at com.google.gson.stream.JsonReader.doPeek(JsonReader.java:551) ~[gson-2.10.1.jar:?] at com.google.gson.stream.JsonReader.peek(JsonReader.java:433) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.assertFullConsumption(Gson.java:1144) ~[gson-2.10.1.jar:?] ... 21 more 调查过程 因为 pinpoint jar 包只放在了容器里，所以本地复现麻烦些，所以准备用 arthas 定位下原因。\n反编译一下代码\njad com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 [arthas@78]$ jad com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder ClassLoader: +-ParallelClassLoader@2093176254{name='pinpoint.agent'} Location: /agent_pinpoint/lib/pinpoint-profiler-2.5.1-p1.jar /* * Decompiled with CFR. * * Could not load the following classes: * com.navercorp.pinpoint.bootstrap.config.ProfilerConfig * com.navercorp.pinpoint.bootstrap.context.Trace * com.navercorp.pinpoint.bootstrap.context.TraceId * com.navercorp.pinpoint.bootstrap.logging.PLogger * com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory * com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyDataHolder * com.navercorp.pinpoint.common.util.StringUtils * com.navercorp.pinpoint.profiler.context.Binder * com.navercorp.pinpoint.profiler.context.DigiwinHttpBody * com.navercorp.pinpoint.profiler.context.DigiwinRequestHttpBody * com.navercorp.pinpoint.profiler.context.DigiwinRequestHttpBody$RequestTypEnum * com.navercorp.pinpoint.profiler.context.DigiwinResponseHttpBody * com.navercorp.pinpoint.profiler.context.Reference * com.navercorp.pinpoint.profiler.context.digiwin.dto.DigiwinBusinessExceptionDto * com.navercorp.pinpoint.profiler.context.module.DigiwinHttpBodyDataSender * com.navercorp.pinpoint.profiler.sender.DataSender */ package com.navercorp.pinpoint.profiler.context; import com.google.gson.Gson; import com.google.inject.Inject; import com.navercorp.pinpoint.bootstrap.config.ProfilerConfig; import com.navercorp.pinpoint.bootstrap.context.Trace; import com.navercorp.pinpoint.bootstrap.context.TraceId; import com.navercorp.pinpoint.bootstrap.logging.PLogger; import com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory; import com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyDataHolder; import com.navercorp.pinpoint.common.util.StringUtils; import com.navercorp.pinpoint.profiler.context.Binder; import com.navercorp.pinpoint.profiler.context.DigiwinHttpBody; import com.navercorp.pinpoint.profiler.context.DigiwinRequestHttpBody; import com.navercorp.pinpoint.profiler.context.DigiwinResponseHttpBody; import com.navercorp.pinpoint.profiler.context.Reference; import com.navercorp.pinpoint.profiler.context.digiwin.dto.DigiwinBusinessExceptionDto; import com.navercorp.pinpoint.profiler.context.module.DigiwinHttpBodyDataSender; import com.navercorp.pinpoint.profiler.sender.DataSender; import java.util.HashMap; import java.util.Map; import javax.inject.Named; public class DefaultDigiwinHttpBodyDataHolder implements DigiwinHttpBodyDataHolder { private final PLogger logger = PLoggerFactory.getLogger(this.getClass()); private DataSender digiwinHttpBodyGrpcDataSender; private Binder\u003cString\u003e digiwinRequestPathThreadBinder; ThreadLocal\u003cLong\u003e digiwinResponseBodySizeThradLocal = new ThreadLocal(); private final ProfilerConfig profilerConfig; Binder\u003cTrace\u003e traceBinder; private final String applicationName; private Binder\u003cInteger\u003e diwinRequestTypeThreadBinder; private Binder\u003cLong\u003e diwinRequestBodySizeThreadBinder; private final Binder\u003cBoolean\u003e digiwinEaiExceptionThreadBinder; private final Map\u003cString, Binder\u003e binderMap = new HashMap\u003cString, Binder\u003e(); private final Binder\u003cbyte[]\u003e bodyValueBinder; private final int MAX_LEGTH; private final Gson GSON = new Gson(); public void logRequestPath(String path) { /* 86*/ Reference pathReference = this.digiwinRequestPathThreadBinder.get(); /* 87*/ pathReference.set((Object)path); } public boolean logDigiwinHttpRequestBody(TraceId traceId, String applicationName) { String binderPath = (String)this.digiwinRequestPathThreadBinder.get().get(); String realPath = StringUtils.isEmpty((String)binderPath) ? \"\" : binderPath; DigiwinRequestHttpBody digiwinHttpRequestBody = new DigiwinRequestHttpBody(traceId, traceId.getSpanId(), ((Long)this.diwinRequestBodySizeThreadBinder.get().get()).longValue(), applicationName, realPath, DigiwinRequestHttpBody.RequestTypEnum.valueToEnum((int)((Integer)this.diwinRequestTypeThreadBinder.get().get()))); /*101*/ return this.digiwinHttpBodyGrpcDataSender.send((Object)digiwinHttpRequestBody); } public boolean logDigiwinHttpResponseBody(TraceId traceId, long bodySize, String applicationName) { String binderPath = (String)this.digiwinRequestPathThreadBinder.get().get(); String realPath = StringUtils.isEmpty((String)binderPath) ? \"\" : binderPath; DigiwinHttpBody digiwinHttpResponseBody = new DigiwinHttpBody(traceId, traceId.getSpanId(), bodySize, applicationName, realPath); /*112*/ return this.digiwinHttpBodyGrpcDataSender.send((Object)digiwinHttpResponseBody); } public boolean logResponseSize(long size) { Long oldSize = this.digiwinResponseBodySizeThradLocal.get(); /*119*/ oldSize = null == oldSize ? Long.valueOf(0L + size) : Long.valueOf(oldSize + size); /*124*/ this.digiwinResponseBodySizeThradLocal.set(oldSize); /*125*/ return true; } public void logRequestType(Integer type) { /*130*/ this.diwinRequestTypeThreadBinder.get().set((Object)type); } public void logRequestBodySize(Long requestBodySize) { /*135*/ this.diwinRequestBodySizeThreadBinder.get().set((Object)requestBodySize); } public void logResponseValue(byte[] b, int off, int len) { Long currentSize; byte[] bytes = (byte[])this.bodyValueBinder.get().get(); /*141*/ if (bytes == null) { /*142*/ bytes = new byte[this.MAX_LEGTH]; /*144*/ this.bodyValueBinder.get().set((Object)bytes); } /*147*/ if ((currentSize = this.digiwinResponseBodySizeThradLocal.get()) == null) { /*150*/ currentSize = 0L; /*151*/ this.digiwinResponseBodySizeThradLocal.set(currentSize); } /*154*/ if (currentSize \u003c (long)this.MAX_LEGTH || currentSize + (long)len \u003c (long)this.MAX_LEGTH) { /*155*/ System.arraycopy(b, off, bytes, Math.toIntExact(currentSize), len); } } public void setFiled(String filedName, Object value) { Binder binder = this.binderMap.get(filedName); /*200*/ if (null != binder) { /*201*/ binder.get().set(value); } } public \u003cT\u003e T getFiled(String filedName, Class\u003cT\u003e type) { Binder binder = this.binderMap.get(filedName); /*209*/ if (null != binder) { /*210*/ return (T)binder.get().get(); } /*213*/ return null; } @Inject public DefaultDigiwinHttpBodyDataHolder(@DigiwinHttpBodyDataSender DataSender digiwinHttpBodyGrpcDataSender, @Named(value=\"digiwinRequestPathThreadBinder\") Binder\u003cString\u003e digiwinRequestPathThreadBinder, Binder\u003cTrace\u003e traceBinder, ProfilerConfig profilerConfig, @Named(value=\"diwinRequestTypeThreadBinder\") Binder\u003cInteger\u003e diwinRequestTypeThreadBinder, @Named(value=\"diwinRequestBodySizeThreadBinder\") Binder\u003cLong\u003e diwinRequestBodySizeThreadBinder, @Named(value=\"digiwinEaiExceptionThreadBinder\") Binder\u003cBoolean\u003e digiwinEaiExceptionThreadBinder, @Named(value=\"diwinResponseBodyValueThreadBinder\") Binder\u003cbyte[]\u003e bodyValueBinder) { /* 60*/ this.digiwinHttpBodyGrpcDataSender = digiwinHttpBodyGrpcDataSender; /* 61*/ this.digiwinRequestPathThreadBinder = digiwinRequestPathThreadBinder; /* 62*/ this.diwinRequestTypeThreadBinder = diwinRequestTypeThreadBinder; /* 63*/ this.diwinRequestBodySizeThreadBinder = diwinRequestBodySizeThreadBinder; /* 64*/ this.traceBinder = traceBinder; /* 65*/ this.profilerConfig = profilerConfig; /* 66*/ this.digiwinEaiExceptionThreadBinder = digiwinEaiExceptionThreadBinder; /* 67*/ this.applicationName = profilerConfig.readString(\"pinpoint.applicationName\", \"unknown_applicationName\"); /* 68*/ this.bodyValueBinder = bodyValueBinder; /* 71*/ this.MAX_LEGTH = profilerConfig.readInt(\"digiwin.profiler.error.response.max.length\", 5120); /* 74*/ this.binderMap.put(\"digiwinRequestPathThreadBinder\", digiwinRequestPathThreadBinder); /* 75*/ this.binderMap.put(\"diwinRequestTypeThreadBinder\", diwinRequestTypeThreadBinder); /* 77*/ this.binderMap.put(\"diwinRequestBodySizeThreadBinder\", diwinRequestBodySizeThreadBinder); /* 78*/ this.binderMap.put(\"digiwinEaiExceptionThreadBinder\", digiwinEaiExceptionThreadBinder); /* 79*/ this.binderMap.put(\"diwinResponseBodyValueThreadBinder\", bodyValueBinder); } public void close(int statusCode) { String binderPath = (String)this.digiwinRequestPathThreadBinder.get().get(); String realPath = StringUtils.isEmpty((String)binderPath) ? \"\" : binderPath; /*164*/ Long bodySize = this.digiwinResponseBodySizeThradLocal.get(); Trace trace = (Trace)this.traceBinder.get().get(); /*167*/ if (null == trace) { /*168*/ return; } /*172*/ byte[] bytes = (byte[])this.bodyValueBinder.get().get(); /*173*/ String errApp = null; /*174*/ if (bytes != null) { try { String body = new String(bytes, 0, Math.toIntExact(bodySize)); /*177*/ DigiwinBusinessExceptionDto digiwinBusinessExceptionDto = this.GSON.fromJson(body, DigiwinBusinessExceptionDto.class); /*179*/ if (statusCode != 200 \u0026\u0026 statusCode != 404 \u0026\u0026 digiwinBusinessExceptionDto.vailed()) { /*180*/ errApp = digiwinBusinessExceptionDto.extractErrorAppCode(); } } catch (Throwable e) { /*183*/ this.logger.error(\"exact body error code exception:\", e); } } /*187*/ TraceId traceId = trace.getTraceId(); /*188*/ this.logDigiwinHttpRequestBody(traceId, this.applicationName); DigiwinResponseHttpBody digiwinHttpResponseBody = new DigiwinResponseHttpBody(traceId, traceId.getSpanId(), null == bodySize ? 0L : bodySize, this.applicationName, realPath, statusCode, System.currentTimeMillis() - trace.getStartTime(), errApp); /*191*/ this.digiwinHttpBodyGrpcDataSender.send((Object)digiwinHttpResponseBody); /*192*/ this.digiwinResponseBodySizeThradLocal.remove(); /*193*/ this.digiwinRequestPathThreadBinder.get().clear(); /*194*/ this.digiwinEaiExceptionThreadBinder.get().clear(); } } 看起来就进行了数组拷贝。 那么推测原因可能是 byte[] 不完整\n继续顺着堆栈来看\njad com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 [arthas@78]$ jad com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext ClassLoader: +-ParallelClassLoader@2093176254{name='pinpoint.agent'} Location: /agent_pinpoint/lib/pinpoint-profiler-2.5.1-p1.jar /* * Decompiled with CFR. * * Could not load the following classes: * com.navercorp.pinpoint.bootstrap.logging.PLogger * com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory * com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyContext * com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyDataHolder */ package com.navercorp.pinpoint.profiler.context; import com.google.inject.Inject; import com.navercorp.pinpoint.bootstrap.logging.PLogger; import com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory; import com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyContext; import com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyDataHolder; public class DefaultDigiwinHttpBodyContext implements DigiwinHttpBodyContext { private final PLogger logger = PLoggerFactory.getLogger(this.getClass()); DigiwinHttpBodyDataHolder digiwinHttpBodyDataHolder; public void logRequestType(Integer type) { /*32*/ this.digiwinHttpBodyDataHolder.logRequestType(type); } public void logRequestBodySize(Long requestBodySize) { /*37*/ this.digiwinHttpBodyDataHolder.logRequestBodySize(requestBodySize); } public void logResponseValue(byte[] b, int off, int len) { /*66*/ this.digiwinHttpBodyDataHolder.logResponseValue(b, off, len); } public void setFiled(String filedName, Object value) { /*56*/ this.digiwinHttpBodyDataHolder.setFiled(filedName, value); } public \u003cT\u003e T getFiled(String filedName, Class\u003cT\u003e type) { /*61*/ return (T)this.digiwinHttpBodyDataHolder.getFiled(filedName, type); } public DigiwinHttpBodyDataHolder getDigiwinHttpBodyDataHolder() { /*22*/ return this.digiwinHttpBodyDataHolder; } public void logResponseBodySize(long size) { /*42*/ this.digiwinHttpBodyDataHolder.logResponseSize(size); } @Inject public DefaultDigiwinHttpBodyContext(DigiwinHttpBodyDataHolder digiwinHttpBodyDataHolder) { /*17*/ this.digiwinHttpBodyDataHolder = digiwinHttpBodyDataHolder; } public void init() { } public void close(int statusCode) { try { /*48*/ this.digiwinHttpBodyDataHolder.close(statusCode); } catch (Throwable e) { /*50*/ this.logger.error(\"DefaultDigiwinHttpBodyContext close error\", e); } } } 继续朝上面查找\njad com.navercorp.pinpoint.plugin.tomcat.digiwin.interceptor.CoyoteOutputStreamInterceptor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 [arthas@78]$ jad com.navercorp.pinpoint.plugin.tomcat.digiwin.interceptor.CoyoteOutputStreamInterceptor ClassLoader: +-sun.misc.Launcher$AppClassLoader@18b4aac2 +-sun.misc.Launcher$ExtClassLoader@498a612d Location: /agent_pinpoint/plugin/pinpoint-tomcat-plugin-2.5.1-p1.jar /* * Decompiled with CFR. * * Could not load the following classes: * com.navercorp.pinpoint.bootstrap.context.TraceContext * com.navercorp.pinpoint.bootstrap.interceptor.AroundInterceptor * com.navercorp.pinpoint.bootstrap.logging.PLogger * com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory * com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyContext */ package com.navercorp.pinpoint.plugin.tomcat.digiwin.interceptor; import com.navercorp.pinpoint.bootstrap.context.TraceContext; import com.navercorp.pinpoint.bootstrap.interceptor.AroundInterceptor; import com.navercorp.pinpoint.bootstrap.logging.PLogger; import com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory; import com.navercorp.pinpoint.bootstrap.plugin.http.digiwin.DigiwinHttpBodyContext; public class CoyoteOutputStreamInterceptor implements AroundInterceptor { private final PLogger logger = PLoggerFactory.getLogger(this.getClass()); private final TraceContext traceContext; public CoyoteOutputStreamInterceptor(TraceContext traceContext) { /*14*/ this.traceContext = traceContext; } public void before(Object target, Object[] args) { /*20*/ int len = 0; try { byte[] b; /*22*/ DigiwinHttpBodyContext digiwinHttpBodyContext = this.traceContext.getDigiwinHttpBodyContext(); /*23*/ if (args.length == 1) { // empty if block } /*25*/ if (args.length == 1 \u0026\u0026 args[0].getClass().isArray()) { /*26*/ b = (byte[])args[0]; /*27*/ len = b.length; /*28*/ digiwinHttpBodyContext.logResponseValue(b, 0, len); } /*31*/ if (args.length == 3) { /*33*/ b = (byte[])args[0]; /*34*/ int off = (Integer)args[1]; /*35*/ len = (Integer)args[2]; /*38*/ digiwinHttpBodyContext.logResponseValue(b, off, len); } /*41*/ if (null != digiwinHttpBodyContext) { /*42*/ digiwinHttpBodyContext.logResponseBodySize((long)len); } } catch (Throwable t) { /*46*/ this.logger.warn(\"before. Caused:{}\", (Object)t.getMessage(), (Object)t); } } public void after(Object target, Object[] args, Object result, Throwable throwable) { } } 这里应该可以推测是 args[1] args[2] 的参数和 0 不匹配\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 /* * Decompiled with CFR. * * Could not load the following classes: * com.navercorp.pinpoint.bootstrap.interceptor.AroundInterceptor * com.navercorp.pinpoint.bootstrap.interceptor.Interceptor * com.navercorp.pinpoint.bootstrap.interceptor.registry.InterceptorRegistry */ package org.apache.catalina.connector; import com.navercorp.pinpoint.bootstrap.interceptor.AroundInterceptor; import com.navercorp.pinpoint.bootstrap.interceptor.Interceptor; import com.navercorp.pinpoint.bootstrap.interceptor.registry.InterceptorRegistry; import java.io.IOException; import java.nio.ByteBuffer; import javax.servlet.ServletOutputStream; import javax.servlet.WriteListener; import org.apache.catalina.connector.OutputBuffer; import org.apache.tomcat.util.res.StringManager; public class CoyoteOutputStream extends ServletOutputStream { protected static final StringManager sm = StringManager.getManager(CoyoteOutputStream.class); protected OutputBuffer ob; protected CoyoteOutputStream(OutputBuffer ob) { /* 47*/ this.ob = ob; } protected Object clone() throws CloneNotSupportedException { throw new CloneNotSupportedException(); } void clear() { /* 70*/ this.ob = null; } /* * WARNING - void declaration */ @Override public void write(int n) throws IOException { Interceptor _$PINPOINT$_interceptor = InterceptorRegistry.getInterceptor((int)2344); Object _$PINPOINT$_result = null; Throwable _$PINPOINT$_throwable2 = null; Object[] _$PINPOINT$_args = new Object[]{new Integer(n)}; ((AroundInterceptor)_$PINPOINT$_interceptor).before((Object)this, _$PINPOINT$_args); try { void i; /* 79*/ boolean nonBlocking = this.checkNonBlockingWrite(); /* 80*/ this.ob.writeByte((int)i); /* 81*/ if (nonBlocking) { /* 82*/ this.checkRegisterForWrite(); } /* 84*/ _$PINPOINT$_result = null; _$PINPOINT$_throwable2 = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); return; } catch (Throwable _$PINPOINT$_throwable2) { _$PINPOINT$_result = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); throw _$PINPOINT$_throwable2; } } /* * WARNING - void declaration */ @Override public void write(byte[] byArray) throws IOException { Interceptor _$PINPOINT$_interceptor = InterceptorRegistry.getInterceptor((int)2345); Object _$PINPOINT$_result = null; Throwable _$PINPOINT$_throwable2 = null; Object[] _$PINPOINT$_args = new Object[]{byArray}; ((AroundInterceptor)_$PINPOINT$_interceptor).before((Object)this, _$PINPOINT$_args); try { void b; /* 89*/ this.write((byte[])b, 0, ((void)b).length); /* 90*/ _$PINPOINT$_result = null; _$PINPOINT$_throwable2 = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); return; } catch (Throwable _$PINPOINT$_throwable2) { _$PINPOINT$_result = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); throw _$PINPOINT$_throwable2; } } /* * WARNING - void declaration */ @Override public void write(byte[] byArray, int n, int n2) throws IOException { Interceptor _$PINPOINT$_interceptor = InterceptorRegistry.getInterceptor((int)2346); Object _$PINPOINT$_result = null; Throwable _$PINPOINT$_throwable2 = null; Object[] _$PINPOINT$_args = new Object[]{byArray, new Integer(n), new Integer(n2)}; ((AroundInterceptor)_$PINPOINT$_interceptor).before((Object)this, _$PINPOINT$_args); try { void len; void off; void b; /* 95*/ boolean nonBlocking = this.checkNonBlockingWrite(); /* 96*/ this.ob.write((byte[])b, (int)off, (int)len); /* 97*/ if (nonBlocking) { /* 98*/ this.checkRegisterForWrite(); } /*100*/ _$PINPOINT$_result = null; _$PINPOINT$_throwable2 = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); return; } catch (Throwable _$PINPOINT$_throwable2) { _$PINPOINT$_result = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); throw _$PINPOINT$_throwable2; } } /* * WARNING - void declaration */ public void write(ByteBuffer byteBuffer) throws IOException { Interceptor _$PINPOINT$_interceptor = InterceptorRegistry.getInterceptor((int)2347); Object _$PINPOINT$_result = null; Throwable _$PINPOINT$_throwable2 = null; Object[] _$PINPOINT$_args = new Object[]{byteBuffer}; ((AroundInterceptor)_$PINPOINT$_interceptor).before((Object)this, _$PINPOINT$_args); try { void from; /*104*/ boolean nonBlocking = this.checkNonBlockingWrite(); /*105*/ this.ob.write((ByteBuffer)from); /*106*/ if (nonBlocking) { /*107*/ this.checkRegisterForWrite(); } /*109*/ _$PINPOINT$_result = null; _$PINPOINT$_throwable2 = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); return; } catch (Throwable _$PINPOINT$_throwable2) { _$PINPOINT$_result = null; ((AroundInterceptor)_$PINPOINT$_interceptor).after((Object)this, _$PINPOINT$_args, _$PINPOINT$_result, _$PINPOINT$_throwable2); throw _$PINPOINT$_throwable2; } } @Override public void flush() throws IOException { /*117*/ boolean nonBlocking = this.checkNonBlockingWrite(); /*118*/ this.ob.flush(); /*119*/ if (nonBlocking) { /*120*/ this.checkRegisterForWrite(); } } private boolean checkNonBlockingWrite() { boolean nonBlocking; /*134*/ boolean bl = nonBlocking = !this.ob.isBlocking(); /*135*/ if (nonBlocking \u0026\u0026 !this.ob.isReady()) { throw new IllegalStateException(sm.getString(\"coyoteOutputStream.nbNotready\")); } /*138*/ return nonBlocking; } private void checkRegisterForWrite() { /*151*/ this.ob.checkRegisterForWrite(); } @Override public void close() throws IOException { /*157*/ this.ob.close(); } @Override public boolean isReady() { /*162*/ return this.ob.isReady(); } @Override public void setWriteListener(WriteListener listener) { /*168*/ this.ob.setWriteListener(listener); } } 一直找上去都没有发现问题，那么写入本身为什么会报错呢？\n查看报错的代码入参\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [arthas@78]$ watch com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder logResponseValue \"{params}\" -e -x 2 Press Q or Ctrl+C to abort. Affect(class count: 1 , method count: 1) cost in 234 ms, listenerId: 4 method=com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.logResponseValue location=AtExceptionExit ts=2024-09-29 10:11:17.615; [cost=0.155178ms] result=@ArrayList[ @Object[][ @byte[][isEmpty=false;size=8192], @Integer[0], @Integer[6131], ], ] method=com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.logResponseValue location=AtExceptionExit ts=2024-09-29 10:11:18.614; [cost=0.056122ms] result=@ArrayList[ @Object[][ @byte[][isEmpty=false;size=8192], @Integer[0], @Integer[6159], ], ] method=com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.logResponseValue location=AtExceptionExit ts=2024-09-29 10:21:54.738; [cost=0.066177ms] result=@ArrayList[ @Object[][ @byte[][isEmpty=false;size=8192], @Integer[0], @Integer[6170], ], @DefaultDigiwinHttpBodyDataHolder[ logger=@Log4j2PLoggerAdapter[com.navercorp.pinpoint.profiler.logging.Log4j2PLoggerAdapter@1ba88f5b], digiwinHttpBodyGrpcDataSender=@DigiwinHttpBodyGrpcDataSender[DigiwinHttpBodyGrpcDataSender{name='SpanGrpcDataSender', host='60.204.141.105', port=9998} com.navercorp.pinpoint.profiler.sender.DigiwinHttpBodyGrpcDataSender@6b8fe2b0], digiwinRequestPathThreadBinder=@ThreadLocalBinder[com.navercorp.pinpoint.profiler.context.ThreadLocalBinder@5b0a671f], digiwinResponseBodySizeThradLocal=@ThreadLocal[java.lang.ThreadLocal@557df459], profilerConfig=@DefaultProfilerConfig[DefaultProfilerConfig{profileEnable='true', activeProfile=release, logDirMaxBackupSize=5, staticResourceCleanup=false, jdbcSqlCacheSize=1024, traceSqlBindValue=true, maxSqlBindValueSize=1024, httpStatusCodeErrors=HttpStatusCodeErrors{errors=[5xx]}, injectionModuleFactoryClazzName='null', applicationNamespace=''}], traceBinder=@ThreadLocalBinder[com.navercorp.pinpoint.profiler.context.ThreadLocalBinder@a707bfa], applicationName=@String[Ali_PaaS_datamap], diwinRequestTypeThreadBinder=@ThreadLocalBinder[com.navercorp.pinpoint.profiler.context.ThreadLocalBinder@1e7e9266], diwinRequestBodySizeThreadBinder=@ThreadLocalBinder[com.navercorp.pinpoint.profiler.context.ThreadLocalBinder@16c43c29], digiwinEaiExceptionThreadBinder=@ThreadLocalBinder[com.navercorp.pinpoint.profiler.context.ThreadLocalBinder@2ca1cd94], binderMap=@HashMap[isEmpty=false;size=5], bodyValueBinder=@ThreadLocalBinder[com.navercorp.pinpoint.profiler.context.ThreadLocalBinder@2507cc51], MAX_LEGTH=@Integer[5125], GSON=@Gson[{serializeNulls:false,factories:[Factory[typeHierarchy=com.google.gson.JsonElement,adapter=com.google.gson.internal.bind.TypeAdapters$28@4f008735], com.google.gson.internal.bind.ObjectTypeAdapter$1@7892cf88, com.google.gson.internal.Excluder@5a7d6063, Factory[type=java.lang.String,adapter=com.google.gson.internal.bind.TypeAdapters$15@41034a39], Factory[type=java.lang.Integer+int,adapter=com.google.gson.internal.bind.TypeAdapters$7@af15403], Factory[type=java.lang.Boolean+boolean,adapter=com.google.gson.internal.bind.TypeAdapters$3@64f1245a], Factory[type=java.lang.Byte+byte,adapter=com.google.gson.internal.bind.TypeAdapters$5@553e2edf], Factory[type=java.lang.Short+short,adapter=com.google.gson.internal.bind.TypeAdapters$6@e37e4c3], Factory[type=java.lang.Long+long,adapter=com.google.gson.internal.bind.TypeAdapters$11@e66e84a], Factory[type=java.lang.Double+double,adapter=com.google.gson.Gson$1@6358de28], Factory[type=java.lang.Float+float,adapter=com.google.gson.Gson$2@47344c4a], com.google.gson.internal.bind.NumberTypeAdapter$1@7614a993, Factory[type=java.util.concurrent.atomic.AtomicInteger,adapter=com.google.gson.TypeAdapter$1@4f893ba3], Factory[type=java.util.concurrent.atomic.AtomicBoolean,adapter=com.google.gson.TypeAdapter$1@6a8515e4], Factory[type=java.util.concurrent.atomic.AtomicLong,adapter=com.google.gson.TypeAdapter$1@f3652e2], Factory[type=java.util.concurrent.atomic.AtomicLongArray,adapter=com.google.gson.TypeAdapter$1@5cbd2aa6], Factory[type=java.util.concurrent.atomic.AtomicIntegerArray,adapter=com.google.gson.TypeAdapter$1@5a6f56b1], Factory[type=java.lang.Character+char,adapter=com.google.gson.internal.bind.TypeAdapters$14@1807522d], Factory[type=java.lang.StringBuilder,adapter=com.google.gson.internal.bind.TypeAdapters$19@259ba28d], Factory[type=java.lang.StringBuffer,adapter=com.google.gson.internal.bind.TypeAdapters$20@10c64482], Factory[type=java.math.BigDecimal,adapter=com.google.gson.internal.bind.TypeAdapters$16@30bc42f5], Factory[type=java.math.BigInteger,adapter=com.google.gson.internal.bind.TypeAdapters$17@1288f8f5], Factory[type=com.google.gson.internal.LazilyParsedNumber,adapter=com.google.gson.internal.bind.TypeAdapters$18@6a7fbfe3], Factory[type=java.net.URL,adapter=com.google.gson.internal.bind.TypeAdapters$21@48bf047a], Factory[type=java.net.URI,adapter=com.google.gson.internal.bind.TypeAdapters$22@1a464c72], Factory[type=java.util.UUID,adapter=com.google.gson.internal.bind.TypeAdapters$24@4f5c509f], Factory[type=java.util.Currency,adapter=com.google.gson.TypeAdapter$1@584b7e30], Factory[type=java.util.Locale,adapter=com.google.gson.internal.bind.TypeAdapters$27@68672d97], Factory[typeHierarchy=java.net.InetAddress,adapter=com.google.gson.internal.bind.TypeAdapters$23@406ad935], Factory[type=java.util.BitSet,adapter=com.google.gson.TypeAdapter$1@6d1fa10f], com.google.gson.internal.bind.DateTypeAdapter$1@6e26f862, Factory[type=java.util.Calendar+java.util.GregorianCalendar,adapter=com.google.gson.internal.bind.TypeAdapters$26@3f7c6ba4], com.google.gson.internal.sql.SqlTimeTypeAdapter$1@29c9be0f, com.google.gson.internal.sql.SqlDateTypeAdapter$1@39ce75e0, com.google.gson.internal.sql.SqlTimestampTypeAdapter$1@60727f53, com.google.gson.internal.bind.ArrayTypeAdapter$1@3c8f19b2, Factory[type=java.lang.Class,adapter=com.google.gson.TypeAdapter$1@58d2f1ea], com.google.gson.internal.bind.CollectionTypeAdapterFactory@12f9fe65, com.google.gson.internal.bind.MapTypeAdapterFactory@6182838e, com.google.gson.internal.bind.JsonAdapterAnnotationTypeAdapterFactory@25044485, com.google.gson.internal.bind.TypeAdapters$29@6a16ac6e, com.google.gson.internal.bind.ReflectiveTypeAdapterFactory@37416d9f],instanceCreators:{}}], ], ] [arthas@78]$ 所以报错的原因应该是 currentSize + len 超过了 b 数组的长度了。 也就是说 b 和 currentSize 应该不匹配\nSystem.arraycopy(b, off, bytes, Math.toIntExact(currentSize), len); 那么也就是要看 digiwinResponseBodySizeThradLocal 是在哪里设置值，以及他的值是如何流转的。\n那么得下载这个包 pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1 再看。\n从另一个方向调查下 09-29 09:12:55.055 [o-32622-exec-32] ERROR n.p.p.c.DefaultDigiwinHttpBodyDataHolder -- exact body error code exception: com.google.gson.JsonSyntaxException: com.google.gson.stream.MalformedJsonException: Use JsonReader.setLenient(true) to accept malformed JSON at line 1 column 241 path $ at com.google.gson.Gson.assertFullConsumption(Gson.java:1148) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1138) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1047) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:982) ~[gson-2.10.1.jar:?] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.close(DefaultDigiwinHttpBodyDataHolder.java:177) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext.close(DefaultDigiwinHttpBodyContext.java:48) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.plugin.request.ServletRequestListener.destroyed(ServletRequestListener.java:192) ~[?:2.5.1-p1] at com.navercorp.pinpoint.plugin.tomcat.javax.interceptor.StandardHostValveInvokeInterceptor.after(StandardHostValveInvokeInterceptor.java:152) ~[pinpoint-tomcat-plugin-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.interceptor.ExceptionHandleAroundInterceptor.after(ExceptionHandleAroundInterceptor.java:44) ~[?:2.5.1-p1] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:196) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:364) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:616) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:831) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1629) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372] Caused by: com.google.gson.stream.MalformedJsonException: Use JsonReader.setLenient(true) to accept malformed JSON at line 1 column 241 path $ at com.google.gson.stream.JsonReader.syntaxError(JsonReader.java:1659) ~[gson-2.10.1.jar:?] at com.google.gson.stream.JsonReader.checkLenient(JsonReader.java:1465) ~[gson-2.10.1.jar:?] at com.google.gson.stream.JsonReader.doPeek(JsonReader.java:551) ~[gson-2.10.1.jar:?] at com.google.gson.stream.JsonReader.peek(JsonReader.java:433) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.assertFullConsumption(Gson.java:1144) ~[gson-2.10.1.jar:?] ... 21 more 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public void close(int statusCode) { String binderPath = (String)this.digiwinRequestPathThreadBinder.get().get(); String realPath = StringUtils.isEmpty((String)binderPath) ? \"\" : binderPath; /*164*/ Long bodySize = this.digiwinResponseBodySizeThradLocal.get(); Trace trace = (Trace)this.traceBinder.get().get(); /*167*/ if (null == trace) { /*168*/ return; } /*172*/ byte[] bytes = (byte[])this.bodyValueBinder.get().get(); /*173*/ String errApp = null; /*174*/ if (bytes != null) { try { String body = new String(bytes, 0, Math.toIntExact(bodySize)); /*177*/ DigiwinBusinessExceptionDto digiwinBusinessExceptionDto = this.GSON.fromJson(body, DigiwinBusinessExceptionDto.class); /*179*/ if (statusCode != 200 \u0026\u0026 statusCode != 404 \u0026\u0026 digiwinBusinessExceptionDto.vailed()) { /*180*/ errApp = digiwinBusinessExceptionDto.extractErrorAppCode(); } } catch (Throwable e) { /*183*/ this.logger.error(\"exact body error code exception:\", e); } } /*187*/ TraceId traceId = trace.getTraceId(); /*188*/ this.logDigiwinHttpRequestBody(traceId, this.applicationName); DigiwinResponseHttpBody digiwinHttpResponseBody = new DigiwinResponseHttpBody(traceId, traceId.getSpanId(), null == bodySize ? 0L : bodySize, this.applicationName, realPath, statusCode, System.currentTimeMillis() - trace.getStartTime(), errApp); /*191*/ this.digiwinHttpBodyGrpcDataSender.send((Object)digiwinHttpResponseBody); /*192*/ this.digiwinResponseBodySizeThradLocal.remove(); /*193*/ this.digiwinRequestPathThreadBinder.get().clear(); /*194*/ this.digiwinEaiExceptionThreadBinder.get().clear(); } } 09-29 09:10:59.059 [o-32622-exec-31] ERROR n.p.p.c.DefaultDigiwinHttpBodyDataHolder -- exact body error code exception: com.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected BEGIN_OBJECT but was STRING at line 1 column 1 path $ at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:397) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1227) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1137) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:1047) ~[gson-2.10.1.jar:?] at com.google.gson.Gson.fromJson(Gson.java:982) ~[gson-2.10.1.jar:?] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyDataHolder.close(DefaultDigiwinHttpBodyDataHolder.java:177) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.profiler.context.DefaultDigiwinHttpBodyContext.close(DefaultDigiwinHttpBodyContext.java:48) ~[pinpoint-profiler-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.plugin.request.ServletRequestListener.destroyed(ServletRequestListener.java:192) ~[?:2.5.1-p1] at com.navercorp.pinpoint.plugin.tomcat.javax.interceptor.StandardHostValveInvokeInterceptor.after(StandardHostValveInvokeInterceptor.java:152) ~[pinpoint-tomcat-plugin-2.5.1-p1.jar:2.5.1-p1] at com.navercorp.pinpoint.bootstrap.interceptor.ExceptionHandleAroundInterceptor.after(ExceptionHandleAroundInterceptor.java:44) ~[?:2.5.1-p1] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:196) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:364) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:616) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:831) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1629) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-8.5.64.jar:8.5.64] at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372] Caused by: java.lang.IllegalStateException: Expected BEGIN_OBJECT but was STRING at line 1 column 1 path $ at com.google.gson.stream.JsonReader.beginObject(JsonReader.java:393) ~[gson-2.10.1.jar:?] at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:386) ~[gson-2.10.1.jar:?] ... 22 more [arthas@84]$ watch com.google.gson.Gson fromJson \"{params}\" -e -x 2 method=com.google.gson.Gson.fromJson location=AtExceptionExit ts=2024-09-29 11:19:30.190; [cost=0.122757ms] result=@ArrayList[ @Object[][ @String[Business\":false,\"performerType\":\"user\",\"performerValue\":\"wangpan0920@digiwin.com\",\"performerName\":\"PR\",\"performerVariable\":null,\"companyId\":null,\"config\":null,\"lang\":null,\"_mergeRule\":null}],\"condition\":null},\"resCode\":null,\"to\":[\"a26dcd0882439f99b6e74052198199ad\"],\"category\":\"PROCESS\",\"config\":{\"groupField\":\"\",\"supportPart\":false,\"supportSplit\":false}},\"profile\":{\"tenantName\":\"智驱中台工作台\",\"tenantSid\":593420788953664,\"tenantId\":\"IntelligentDriveCenterWorkbench\",\"userSid\":1984610499,\"userName\":\"集成账号\",\"userId\":\"integration\"},\"uuid\":\"\",\"status\":200}], @TypeToken[com.navercorp.pinpoint.profiler.context.digiwin.dto.DigiwinBusinessExceptionDto], ], ] Press Q or Ctrl+C to abort. 发现 json 都不太完整\n看起来是 bytes 不完整\n到这里基本上没啥头绪了。\n只能下载 pinpoint 包到本地复现下了 最后 相关团队的同事帮忙修改了下问题\n","description":"","tags":null,"title":"Pinpoint_error","uri":"/posts/pinpoint_error/"},{"categories":null,"content":"背景 多语言的设置在测试环境可以正常展示，但是在生产环境却无法展示。\n确认代码版本 分支 hotfix/132424 5.2.0:2.2.22.1082\n测试： 5.2.0:2.2.22.1082 5.2.0:2.2.22.1082\n2.2.22.1050\n生产： 5.2.0:2.2.22.1082 5.2.0:2.2.22.1082 5.2.0:2.2.22.1082\n2.2.22.1050 2.2.22.1050\n生产和测试区镜像一致\n确认数据是否一致 测试区\n{ \"_id\": \"66f170b450c88226a84db263\", \"mechanismCode\": \"ud_m_e9cb9bde2d4e423093b900a6912040d2\", \"mechanismVersion\": \"2.0\", \"mechanismTheoryCode\": \"ud_p_6f474b61aeaf4609b29f61bcfdd1216d\", \"status\": 1, \"createDate\": \"2024/09/23 21:43:19\", \"from\": \"\", \"createdByDevPlatform\": true, \"deletedByDevPlatform\": false, \"tenantId\": \"SYSTEM\", \"application\": \"purchase99\", \"athena_namespace\": \"purchase99\", \"priority\": 0, \"score\": 0, \"code\": \"ud_c_792142a1511544e7a0fa6c2e8a1a14b0\", \"version\": \"1.0\", \"name\": \"测试能力1\", \"description\": \"测试能力描述\", \"datamap\": \"2.0\", \"lang\": { \"name\": { \"zh_TW\": \"測試能力1\", \"en_US\": \"test1\", \"zh_CN\": \"测试能力1\" }, \"description\": { \"zh_TW\": \"測試能力描述\", \"en_US\": \"test1desc\", \"zh_CN\": \"测试能力描述\" } } } 生产区\n{ \"_id\": \"66f170d128fd7d621d1e931a\", \"mechanismCode\": \"ud_m_e9cb9bde2d4e423093b900a6912040d2\", \"mechanismVersion\": \"2.0\", \"mechanismTheoryCode\": \"ud_p_6f474b61aeaf4609b29f61bcfdd1216d\", \"status\": 1, \"createDate\": \"2024/09/23 21:43:19\", \"from\": \"\", \"createdByDevPlatform\": true, \"deletedByDevPlatform\": false, \"tenantId\": \"SYSTEM\", \"application\": \"purchase99\", \"athena_namespace\": \"purchase99\", \"priority\": 0, \"score\": 0, \"code\": \"ud_c_792142a1511544e7a0fa6c2e8a1a14b0\", \"version\": \"1.0\", \"name\": \"测试能力1\", \"description\": \"测试能力描述\", \"datamap\": \"2.0\", \"lang\": { \"name\": { \"zh_TW\": \"測試能力1\", \"en_US\": \"test1\", \"zh_CN\": \"测试能力1\" }, \"description\": { \"zh_TW\": \"測試能力描述\", \"en_US\": \"test1desc\", \"zh_CN\": \"测试能力描述\" } } } 生产区和测试区数据一致\n通过 Arthas 来进一步定位问题 [arthas@82]$ watch com.digiwin.athena.knowledgegraph.service.impl.MechanismService postQueryMechanismByCode \"{params,returnObj}\" -s -x 1 Press Q or Ctrl+C to abort. Affect(class count: 2 , method count: 8) cost in 714 ms, listenerId: 5 method=com.digiwin.athena.knowledgegraph.service.impl.MechanismService.postQueryMechanismByCode location=AtExit ts=2024-09-24 10:24:18.588; [cost=55.195385ms] result=@ArrayList[ @Object[][isEmpty=false;size=3], @Mechanism[Mechanism(code=ud_m_e9cb9bde2d4e423093b900a6912040d2, name=按时测试, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=数, key=number_zxy_1, value=null, dataType=number, componentType=null, format=null, express=null, fields=null, description=数, defaultValue=null, displayType=NUMBER, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=1, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=人员, key=personnel_zxy, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=人员, defaultValue=null, displayType=MULTIPLE_USER, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=2, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=频率_按时使用, key=pl_time_1, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=频率_按时使用, defaultValue=null, displayType=FREQUENCY, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=2, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=ud_p_6f474b61aeaf4609b29f61bcfdd1216d, description=null, details=[PrincipleDetail(code=ud_pd_445433bf630844d9be9cd53c037b6c55, sequence=null, content=按时, description=null, relatedParamKeys=[number_zxy_1, personnel_zxy, pl_time_1], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=ud_c_650c0c128f3546ab8729f850d7b14415, sequence=null, content=null, description=时机参数-plan-前-number-小时--含变量, relatedParamKeys=[ud_c_650c0c128f3546ab8729f850d7b14415], otherParamKeys=null, title=时机参数-plan-前-number-小时--含变量, from=user, subtitle=null), PrincipleDetail(code=ud_c_319b94cbdd784eb2a1e0a2c58c103ca8, sequence=null, content=null, description=每周-plan--后-自定义1--小时--无跳转无变量, relatedParamKeys=[ud_c_319b94cbdd784eb2a1e0a2c58c103ca8], otherParamKeys=null, title=每周-plan--后-自定义1--小时--无跳转无变量, from=user, subtitle=null), PrincipleDetail(code=ud_c_fc1576a20af74c868c301f667ce08505, sequence=null, content=null, description=每分钟--create--后--number-小时--跳转*3, relatedParamKeys=[ud_c_fc1576a20af74c868c301f667ce08505], otherParamKeys=null, title=每分钟--create--后--number-小时--跳转*3, from=user, subtitle=null), PrincipleDetail(code=ud_c_4a7039737f0f4c2ea2ed1b86a86a738e, sequence=null, content=null, description=完成率, relatedParamKeys=[ud_c_4a7039737f0f4c2ea2ed1b86a86a738e], otherParamKeys=null, title=完成率, from=user, subtitle=null), PrincipleDetail(code=ud_c_4bf4c47cc134428eb038ee19bbdd5323, sequence=null, content=null, description=每小时--周期-跳转全局咨询, relatedParamKeys=[ud_c_4bf4c47cc134428eb038ee19bbdd5323], otherParamKeys=null, title=每小时--周期-跳转全局咨询, from=user, subtitle=null), PrincipleDetail(code=ud_c_6cc7efe7835149258b37c16d3be0a654, sequence=null, content=null, description=能力描述, relatedParamKeys=[ud_c_6cc7efe7835149258b37c16d3be0a654], otherParamKeys=null, title=新建能力, from=user, subtitle=null), PrincipleDetail(code=ud_c_792142a1511544e7a0fa6c2e8a1a14b0, sequence=null, content=null, description=测试能力描述, relatedParamKeys=[ud_c_792142a1511544e7a0fa6c2e8a1a14b0], otherParamKeys=null, title=测试能力1, from=user, subtitle=null)], title=按时, sequence=null, from=null, capacities=null, status=0)], extensions=null, version=1.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], ] method=com.digiwin.athena.knowledgegraph.service.impl.MechanismService.postQueryMechanismByCode location=AtExit ts=2024-09-24 10:24:18.590; [cost=56.918778ms] result=@ArrayList[ @Object[][isEmpty=false;size=2], @Mechanism[Mechanism(code=ud_m_e9cb9bde2d4e423093b900a6912040d2, name=按时测试, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=数, key=number_zxy_1, value=null, dataType=number, componentType=null, format=null, express=null, fields=null, description=数, defaultValue=null, displayType=NUMBER, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=1, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=人员, key=personnel_zxy, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=人员, defaultValue=null, displayType=MULTIPLE_USER, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=2, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=频率_按时使用, key=pl_time_1, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=频率_按时使用, defaultValue=null, displayType=FREQUENCY, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=2, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=ud_p_6f474b61aeaf4609b29f61bcfdd1216d, description=null, details=[PrincipleDetail(code=ud_pd_445433bf630844d9be9cd53c037b6c55, sequence=null, content=按时, description=null, relatedParamKeys=[number_zxy_1, personnel_zxy, pl_time_1], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=ud_c_650c0c128f3546ab8729f850d7b14415, sequence=null, content=null, description=时机参数-plan-前-number-小时--含变量, relatedParamKeys=[ud_c_650c0c128f3546ab8729f850d7b14415], otherParamKeys=null, title=时机参数-plan-前-number-小时--含变量, from=user, subtitle=null), PrincipleDetail(code=ud_c_319b94cbdd784eb2a1e0a2c58c103ca8, sequence=null, content=null, description=每周-plan--后-自定义1--小时--无跳转无变量, relatedParamKeys=[ud_c_319b94cbdd784eb2a1e0a2c58c103ca8], otherParamKeys=null, title=每周-plan--后-自定义1--小时--无跳转无变量, from=user, subtitle=null), PrincipleDetail(code=ud_c_fc1576a20af74c868c301f667ce08505, sequence=null, content=null, description=每分钟--create--后--number-小时--跳转*3, relatedParamKeys=[ud_c_fc1576a20af74c868c301f667ce08505], otherParamKeys=null, title=每分钟--create--后--number-小时--跳转*3, from=user, subtitle=null), PrincipleDetail(code=ud_c_4a7039737f0f4c2ea2ed1b86a86a738e, sequence=null, content=null, description=完成率, relatedParamKeys=[ud_c_4a7039737f0f4c2ea2ed1b86a86a738e], otherParamKeys=null, title=完成率, from=user, subtitle=null), PrincipleDetail(code=ud_c_4bf4c47cc134428eb038ee19bbdd5323, sequence=null, content=null, description=每小时--周期-跳转全局咨询, relatedParamKeys=[ud_c_4bf4c47cc134428eb038ee19bbdd5323], otherParamKeys=null, title=每小时--周期-跳转全局咨询, from=user, subtitle=null), PrincipleDetail(code=ud_c_6cc7efe7835149258b37c16d3be0a654, sequence=null, content=null, description=能力描述, relatedParamKeys=[ud_c_6cc7efe7835149258b37c16d3be0a654], otherParamKeys=null, title=新建能力, from=user, subtitle=null), PrincipleDetail(code=ud_c_792142a1511544e7a0fa6c2e8a1a14b0, sequence=null, content=null, description=测试能力描述, relatedParamKeys=[ud_c_792142a1511544e7a0fa6c2e8a1a14b0], otherParamKeys=null, title=测试能力1, from=user, subtitle=null)], title=按时, sequence=null, from=null, capacities=null, status=0)], extensions=null, version=1.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], ] method=com.digiwin.athena.knowledgegraph.service.impl.MechanismService$$EnhancerBySpringCGLIB$$265447ce.postQueryMechanismByCode location=AtExit ts=2024-09-24 10:24:18.591; [cost=58.240999ms] result=@ArrayList[ @Object[][isEmpty=false;size=2], @Mechanism[Mechanism(code=ud_m_e9cb9bde2d4e423093b900a6912040d2, name=按时测试, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=数, key=number_zxy_1, value=null, dataType=number, componentType=null, format=null, express=null, fields=null, description=数, defaultValue=null, displayType=NUMBER, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=1, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=人员, key=personnel_zxy, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=人员, defaultValue=null, displayType=MULTIPLE_USER, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=2, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=频率_按时使用, key=pl_time_1, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=频率_按时使用, defaultValue=null, displayType=FREQUENCY, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=2, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=ud_p_6f474b61aeaf4609b29f61bcfdd1216d, description=null, details=[PrincipleDetail(code=ud_pd_445433bf630844d9be9cd53c037b6c55, sequence=null, content=按时, description=null, relatedParamKeys=[number_zxy_1, personnel_zxy, pl_time_1], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=ud_c_650c0c128f3546ab8729f850d7b14415, sequence=null, content=null, description=时机参数-plan-前-number-小时--含变量, relatedParamKeys=[ud_c_650c0c128f3546ab8729f850d7b14415], otherParamKeys=null, title=时机参数-plan-前-number-小时--含变量, from=user, subtitle=null), PrincipleDetail(code=ud_c_319b94cbdd784eb2a1e0a2c58c103ca8, sequence=null, content=null, description=每周-plan--后-自定义1--小时--无跳转无变量, relatedParamKeys=[ud_c_319b94cbdd784eb2a1e0a2c58c103ca8], otherParamKeys=null, title=每周-plan--后-自定义1--小时--无跳转无变量, from=user, subtitle=null), PrincipleDetail(code=ud_c_fc1576a20af74c868c301f667ce08505, sequence=null, content=null, description=每分钟--create--后--number-小时--跳转*3, relatedParamKeys=[ud_c_fc1576a20af74c868c301f667ce08505], otherParamKeys=null, title=每分钟--create--后--number-小时--跳转*3, from=user, subtitle=null), PrincipleDetail(code=ud_c_4a7039737f0f4c2ea2ed1b86a86a738e, sequence=null, content=null, description=完成率, relatedParamKeys=[ud_c_4a7039737f0f4c2ea2ed1b86a86a738e], otherParamKeys=null, title=完成率, from=user, subtitle=null), PrincipleDetail(code=ud_c_4bf4c47cc134428eb038ee19bbdd5323, sequence=null, content=null, description=每小时--周期-跳转全局咨询, relatedParamKeys=[ud_c_4bf4c47cc134428eb038ee19bbdd5323], otherParamKeys=null, title=每小时--周期-跳转全局咨询, from=user, subtitle=null), PrincipleDetail(code=ud_c_6cc7efe7835149258b37c16d3be0a654, sequence=null, content=null, description=能力描述, relatedParamKeys=[ud_c_6cc7efe7835149258b37c16d3be0a654], otherParamKeys=null, title=新建能力, from=user, subtitle=null), PrincipleDetail(code=ud_c_792142a1511544e7a0fa6c2e8a1a14b0, sequence=null, content=null, description=测试能力描述, relatedParamKeys=[ud_c_792142a1511544e7a0fa6c2e8a1a14b0], otherParamKeys=null, title=测试能力1, from=user, subtitle=null)], title=按时, sequence=null, from=null, capacities=null, status=0)], extensions=null, version=1.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], ] [arthas@80]$ watch com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService mergeMechanismDesignInfo \"{params,returnObj}\" -s -x 2 Press Q or Ctrl+C to abort. Affect(class count: 2 , method count: 2) cost in 641 ms, listenerId: 3 method=com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService.mergeMechanismDesignInfo location=AtExit method=com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService.mergeMechanismDesignInfo location=AtExit ts=2024-09-24 10:37:00.007; [cost=7.126417ms] result=@ArrayList[ @Object[][ @Mechanism[Mechanism(code=111_TEST, name=111_TEST, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=品號資訊偵測, key=FCI_item_cloud_detection, value=null, dataType=string, componentType=null, format=rule;yyyyMMdd HH:mm;switch, express=null, fields=null, description=品號資訊偵測, defaultValue=F10;20100101 00:00;true, displayType=PERIODIC, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=FCI_item_cloud_detection, category=period, from=default, isOperation=false, eocLevel=null, executable=1, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=false, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=ud_p_616ba68134ae42ff9f1f9517e8ac2555, description=null, details=[PrincipleDetail(code=ud_pd_587c83822a194a948c2ae7c4add7471d, sequence=null, content=111_TEST, description=null, relatedParamKeys=[FCI_item_cloud_detection], otherParamKeys=null, title=null, from=null, subtitle=null)], title=111_TEST, sequence=null, from=null, capacities=null, status=0)], extensions=null, version=2.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], @Integer[1], ], null, ] ts=2024-09-24 10:37:00.007; [cost=3.524955ms] result=@ArrayList[ @Object[][ @Mechanism[Mechanism(code=SelfForecastingOfSupplyAndDemandMechanism, name=自主预测供需机制, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=客戶資訊維護, key=maintenanceOfCustomerInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfCustomerInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=責任人設定, key=forecastingResponderAndExecutor, value=\u0026routerLink=/base-data-entry\u0026code=forecastingResponderAndExecutor\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=品號資訊維護, key=maintenanceOfProductInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfProductInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=客戶品號關係維護, key=baseDataEntry_product_informtion_maintenance, value=\u0026routerLink=/base-data-entry\u0026code=baseDataEntry_product_informtion_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=sfosad_p1, description=null, details=[PrincipleDetail(code=sfosad_p1d1, sequence=numethod=com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService$$EnhancerBySpringCGLIB$$e4137eb9.mergeMechanismDesignInfo location=AtExit *产业景气趋势分析 *依据历史销售数据--\u003e统计每月热销品,分析热销产品项目 *库存数量周转天数分析 *依据目标终端市场车种销售数据--\u003e以车损维修期预估统计每月销量,分析热销车种/车型对应产品项目 1-2.拟定预测叁考变量 1-3.拟定产品预测滚动更新周期 , description=1.规划产品预测项目 每[半年]针对[下半年]检讨预测产品品项,招开销售分析会议 会议前需准备下列项目： 1.2. 业务收集产业景气预测平台及透过客户访谈了解对未来景气预估 1.3. 汇总统计过去3年每月产品销货数量及评估热销产品项目 1.4. 针对以上产品项目汇总统计过去3年库存周转天数 1.5. 目标市场下期在过去市场新车销售量数据 会议产出： ts=2024-09-24 10:37:00.010; [cost=9.873338ms] result=@ArrayList[ 1.6. 预测主要目标客户 @Object[][ 1.7. 预测主要目标客户之预测目标产品项目 1.8. 评估各目标客户之预测产品项目预测数量 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=规划产品预测项目, sequence=null, from=null, capacities=null, status=null), Principle(code=sfosad_p2, description=null, details=[PrincipleDetail(code=sfosad_p2d1, sequence=null, content=2-1.设置客户及负责预测责任对象 2-2.设置预测流程及预测模型 @Mechanism[Mechanism(code=111_TEST, name=111_TEST, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=品號資訊偵測, key=FCI_item_cloud_detection, value=null, dataType=string, componentType=null, format=rule;yyyyMMdd HH:mm;switch, express=null, fields=null, description=品號資訊偵測, defaultValue=F10;20100101 00:00;true, displayType=PERIODIC, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=FCI_item_cloud_detection, category=period, from=default, isOperation=false, eocLevel=null, executable=1, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=false, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=ud_p_616ba68134ae42ff9f1f9517e8ac2555, description=null, details=[PrincipleDetail(code=ud_pd_587c83822a194a948c2ae7c4add7471d, sequence=null, content=111_TEST, description=null, relatedParamKeys=[FCI_item_cloud_detection], otherParamKeys=null, title=null, from=null, subtitle=null)], title=111_TEST, sequence=null, from=null, capacities=null, status=0)], extensions=null, version=2.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], @Integer[1],产品预测运作基础 ], 将销售分析会议决议项目设置预测系统, relatedParamKeys=[maintenanceOfCustomerInfo, forecastingResponderAndExecutor, maintenanceOfProductInfo, baseDataEntry_product_informtion_maintenance], otherParamKeys=null, title=null, from=null, subtitle=null)], title=设置产品预测运作基础, sequence=null, from=null, capacities=null, status=null), Principle(code=sfosad_p3, description=null, details=[PrincipleDetail(code=sfosad_p3d1, sequence=null, content= 3-0.依据预测周期事前主动通知负责业务 null, ] 3-1.汇入/同步ERP各项变量来源数据 3-2.负责业务自主计算产品预测数量,并根据叁考变量调整后录入提交主管审批 3-3.主管审批通过后汇出/同步ERP生成销售预测订单 , description=3.执行产品预测量 依据预测周期事前主动通知负责业务 3.1. 定期透过系统主动通知业务发起预测 3.2. 若未完成预测建置,系统通知业务主管预警 3.3. 系统提醒各项ERP数据同步/汇入资料完备列表 汇入/同步ERP各项变量来源数据 3.4. 将ERP同步/汇入资料导入系统 3.5. 各项变量数据完备及统整 3.6. 计算各项产品自主预测并逐项填入自主预测任务卡 3.7. 提交主管审批 3.8. 预测经主管审批通过后,将各客户产品预测项目汇出/同步 ERP销售预测订单提供生管备货 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=执行产品预测量, sequence=null, from=null, capacities=null, status=null), Principle(code=sfosad_p4, description=null, details=[PrincipleDetail(code=sfosad_p4d1, sequence=null, content= 4-0.以最大生产周期为限,预分析预测偏差,做预测量调教 4-1.汇入/同步实际订单 4-2.产品预测偏差分析: *供给溢出:调降未来预测量 *供给不足:调升未来预测量 4-3.主管审批通过后汇出/同步ERP生成销售预测订单 , description=4.检讨预测达成偏差暨动态调校预测量 以最大生产周期为限,预分析预测偏差,做预测量调教 4.1. 设置产品预测偏差计算期间 汇入/同步实际订单 4.2. 汇入/同步 ERP当期订单 产品预测偏差分析 4.3. 供给溢出:调降未来预测量 4.4. 供给不足:调升未来预测量 主管审批通过后汇出/同步ERP生成销售预测订单 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=检讨预测达成偏差暨动态调校预测量, sequence=null, from=null, capacities=null, status=null)], extensions=[], version=2.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], @Integer[1], ], null, ] method=com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService$$EnhancerBySpringCGLIB$$e4137eb9.mergeMechanismDesignInfo location=AtExit ts=2024-09-24 10:37:00.011; [cost=7.558471ms] result=@ArrayList[ @Object[][ @Mechanism[Mechanism(code=SelfForecastingOfSupplyAndDemandMechanism, name=自主预测供需机制, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=客戶資訊維護, key=maintenanceOfCustomerInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfCustomerInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=責任人設定, key=forecastingResponderAndExecutor, value=\u0026routerLink=/base-data-entry\u0026code=forecastingResponderAndExecutor\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=品號資訊維護, key=maintenanceOfProductInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfProductInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=客戶品號關係維護, key=baseDataEntry_product_informtion_maintenance, value=\u0026routerLink=/base-data-entry\u0026code=baseDataEntry_product_informtion_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=sfosad_p1, description=null, details=[PrincipleDetail(code=sfosad_p1d1, sequence=null, content= 1-1.拟定预测产品项目 *产业景气趋势分析 *依据历史销售数据--\u003e统计每月热销品,分析热销产品项目 *库存数量周转天数分析 *依据目标终端市场车种销售数据--\u003e以车损维修期预估统计每月销量,分析热销车种/车型对应产品项目 1-2.拟定预测叁考变量 1-3.拟定产品预测滚动更新周期 , description=1.规划产品预测项目 每[半年]针对[下半年]检讨预测产品品项,招开销售分析会议 会议前需准备下列项目： 1.2. 业务收集产业景气预测平台及透过客户访谈了解对未来景气预估 1.3. 汇总统计过去3年每月产品销货数量及评估热销产品项目 1.4. 针对以上产品项目汇总统计过去3年库存周转天数 1.5. 目标市场下期在过去市场新车销售量数据 会议产出： 1.6. 预测主要目标客户 1.7. 预测主要目标客户之预测目标产品项目 1.8. 评估各目标客户之预测产品项目预测数量 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=规划产品预测项目, sequence=null, from=null, capacities=null, status=null), Principle(code=sfosad_p2, description=null, details=[PrincipleDetail(code=sfosad_p2d1, sequence=null, content=2-1.设置客户及负责预测责任对象 2-2.设置预测流程及预测模型 2-3.设置产品预测叁考变量 , description=2.设置产品预测运作基础 将销售分析会议决议项目设置预测系统, relatedParamKeys=[maintenanceOfCustomerInfo, forecastingResponderAndExecutor, maintenanceOfProductInfo, baseDataEntry_product_informtion_maintenance], otherParamKeys=null, title=null, from=null, subtitle=null)], title=设置产品预测运作基础, sequence=null, from=null, capacities=null, status=null), Principle(code=sfosad_p3, description=null, details=[PrincipleDetail(code=sfosad_p3d1, sequence=null, content= 3-0.依据预测周期事前主动通知负责业务 3-1.汇入/同步ERP各项变量来源数据 3-2.负责业务自主计算产品预测数量,并根据叁考变量调整后录入提交主管审批 3-3.主管审批通过后汇出/同步ERP生成销售预测订单 , description=3.执行产品预测量 依据预测周期事前主动通知负责业务 3.1. 定期透过系统主动通知业务发起预测 3.2. 若未完成预测建置,系统通知业务主管预警 3.3. 系统提醒各项ERP数据同步/汇入资料完备列表 汇入/同步ERP各项变量来源数据 3.4. 将ERP同步/汇入资料导入系统 3.5. 各项变量数据完备及统整 3.6. 计算各项产品自主预测并逐项填入自主预测任务卡 3.7. 提交主管审批 3.8. 预测经主管审批通过后,将各客户产品预测项目汇出/同步 ERP销售预测订单提供生管备货 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=执行产品预测量, sequence=null, from=null, capacities=null, status=null), Principle(code=sfosad_p4, description=null, details=[PrincipleDetail(code=sfosad_p4d1, sequence=null, content= 4-0.以最大生产周期为限,预分析预测偏差,做预测量调教 4-1.汇入/同步实际订单 4-2.产品预测偏差分析: *供给溢出:调降未来预测量 *供给不足:调升未来预测量 4-3.主管审批通过后汇出/同步ERP生成销售预测订单 , description=4.检讨预测达成偏差暨动态调校预测量 以最大生产周期为限,预分析预测偏差,做预测量调教 4.1. 设置产品预测偏差计算期间 汇入/同步实际订单 4.2. 汇入/同步 ERP当期订单 产品预测偏差分析 4.3. 供给溢出:调降未来预测量 4.4. 供给不足:调升未来预测量 主管审批通过后汇出/同步ERP生成销售预测订单 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=检讨预测达成偏差暨动态调校预测量, sequence=null, from=null, capacities=null, status=null)], extensions=[], version=2.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=null)], @Integer[1], ], null, ] method=com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService.mergeMechanismDesignInfo location=AtExit ts=2024-09-24 10:37:00.045; [cost=4.446351ms] result=@ArrayList[ @Object[][ @Mechanism[Mechanism(code=ForecastingSupplyAndDemandControlPlanning, name=预测供需调控机制, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=客戶資訊維護, key=maintenanceOfCustomerInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfCustomerInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=品號資訊維護, key=maintenanceOfProductInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfProductInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=客戶品號關係維護, key=baseDataEntry_product_informtion_maintenance, value=\u0026routerLink=/base-data-entry\u0026code=baseDataEntry_product_informtion_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=責任人設定, key=forecastingResponderAndExecutor, value=\u0026routerLink=/base-data-entry\u0026code=forecastingResponderAndExecutor\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=預測流程設定, key=DataEntry_PredictiveProcessSetup, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_PredictiveProcessSetup\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=21, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=預測需求量, key=calculationRules, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=預測需求量, defaultValue=[inventory_qty], displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=庫存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交貨量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在製, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, label=在途量, lang={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, value=on_way_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=null, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=預測BOM資訊維護, key=maintenanceOfBomInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfBomInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=取穩態的資料類型, key=steadyStateDataType, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=取穩態的資料類型, defaultValue=null, displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=庫存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交貨量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在製, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=下階庫存, en_US=next qty, zh_CN=下阶库存}}, label=下階庫存, lang={label={zh_TW=下階庫存, en_US=next qty, zh_CN=下阶库存}}, value=next_qty}, {language={label={zh_TW=下階採購, en_US=next purchase, zh_CN=下阶采购}}, label=下階採購, lang={label={zh_TW=下階採購, en_US=next purchase, zh_CN=下阶采购}}, value=next_purchase}, {language={label={zh_TW=下階在製, en_US=next in qty, zh_CN=下阶在制}}, label=下階在製, lang={label={zh_TW=下階在製, en_US=next in qty, zh_CN=下阶在制}}, value=next_in_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=null, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed={valueMapping=[{op=EQUAL, rightValueType=CONSTANT, right=Y, value=[inventory_qty]}, {op=EQUAL, rightValueType=CONSTANT, right=N, value=[]}], serviceName=hasGroundEnd, type=SYSTEM}, isTop=false), FieldDescription(name=庫存資訊維護, key=DataEntry_MaintenanceOfInventoryInfo, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_MaintenanceOfInventoryInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=訂單資訊維護, key=DataEntry_order_information_maintenance, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_order_information_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=備料資訊維護, key=maintenanceOfSparePartsInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfSparePartsInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=彙總真預測, key=aggregateTruePrediction, value=null, dataType=object, componentType=null, format=null, express=null, fields=null, description=設定真預測的彙總規則，依客戶彙總：同一客戶有新的預測數據時,按最新預測調整該客戶所有的真預測數據；依客戶和品號彙總：同一客戶有新的預測數據時，按最新預測調整該客戶同品號的真預測數據, defaultValue=customer, displayType=RADIO, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, label=依客戶彙總, lang={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, value=customer}, {language={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, label=依客戶和品號彙總, lang={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, value=customer_item}], sequence=13, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=庫存資訊維護, key=DataEntry_MaintenanceOfInventoryInfo2, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_MaintenanceOfInventoryInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=訂單資訊維護, key=DataEntry_order_information_maintenance2, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_order_information_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=備料資訊維護, key=maintenanceOfSparePartsInfo2, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfSparePartsInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=DetectingSalesForecastData, key=DetectingSalesForecastData, value=null, dataType=object, componentType=null, format=null, express=null, fields=null, description=DetectingSalesForecastData, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=9999, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=null, executeName=null, colspan=0, extendData=null, category=link, from=default, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=fsadcp_p1, description=null, details=[PrincipleDetail(code=fsadcp_p1d1, sequence=null, content=明确需纳入预测管理的范畴，先行厘清各产品类型，对应之备货流程与产销模式，方能使得预测管理的效益最大化。 , description=一、建构预测管理模型的规则： 1-1 針對固定頻率提供預測的客戶進行維護 , relatedParamKeys=[maintenanceOfCustomerInfo], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d2, sequence=null, content=, description=1-2 針對1-1客戶預測的備貨品號進行維護 , relatedParamKeys=[maintenanceOfProductInfo], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d3, sequence=null, content=, description=1-3 針對預測廠內品號維護對應客戶品號 , relatedParamKeys=[baseDataEntry_product_informtion_maintenance], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d4, sequence=null, content=, description=2. 当预测偏差发生时，为确保响应不遗漏，需事先明确负责人员；未来发生预测偏差时，会自动派送任务卡给对应负责人： 2-1 负责处理预测偏差的人员 2-2 负责监督处理状况的人员 , relatedParamKeys=[forecastingResponderAndExecutor], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d5, sequence=null, content=, description=3. 订定预测偏差分析的计划周期区间范围： 预测偏差分析周期需小于等于滚动式预测(Rolling Forecast)的范围，且至少超过合理的前置时间(Lead Time)。例如：客户每个月会更新未来12个月的预测，且客户产品于厂内产销协调后的备货周期是3个月，则预测偏差分析周期须设定介于3-12个月之间。 预测偏差分析周期可以根据实际备货需求动态调整。 , relatedParamKeys=[DataEntry_PredictiveProcessSetup], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d6, sequence=null, content=, description=4. 是否与客户有协议，历史未交货的预测需求，客户要全数出货。 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d7, sequence=null, content=, description=5. 预测备货如需考虑订单需求，须择一设定预测与订单的冲销方式，避免相同需求重复备货： 5-1 当期有预测需求，则以预测为主，不考虑订单数量。 5-2 当期有订单需求，则以订单为主，不考虑预测数量。 5-3 比对当期预测与订单数量，两者取其大。 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d8, sequence=null, content=, description=6. 预测偏差分析周期内的需求，与厂内备货的供给有落差时，订定需预警的偏差幅度。 未来可以依据偏差率稳定下降后，动态调降偏差分析标准。 6-1 供给溢出－评估合理备货范围 6-2 供给不足－如果不允许任何交货风险则设定0% , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d9, sequence=null, content=, description=7-1 订定备货供给的范畴 7-1-1 MTS(Make to Stock 存货式生产) , relatedParamKeys=[calculationRules], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d10, sequence=null, content=, description=7-1-2 ATO(Assembly to Order 接单后装配) 预测BOM与生产BOM不同，生产BOM是生产过程中的产品架构，预测BOM则是以预测备货的角度建置，不存在多阶架构。 举例来说，半成品到成品的生产周期很短，因此备货至半成品即可满足出货需求，则预测BOM只需要建置上阶成品，对应下阶半成品，不需要建置尾阶原材料。 此外，如果上阶主件成品对应下阶有20个元件，其中17个备货风险较低，则预测BOM只需要建置3个关键元件，进行预测备货管理的风险监控。 , relatedParamKeys=[maintenanceOfBomInfo], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d11, sequence=null, content=, description=7-2 订定备货供给的数据来源： , relatedParamKeys=[steadyStateDataType, DataEntry_MaintenanceOfInventoryInfo, DataEntry_order_information_maintenance, maintenanceOfSparePartsInfo], otherParamKeys=[steadyStateInfos], title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d12, sequence=null, content=, description=8. 与客户制定预测资料更新方式，是否直接覆盖，或是根据品号更新。 , relatedParamKeys=[aggregateTruePrediction], otherParamKeys=null, title=null, from=null, subtitle=null)], title=项⽬建构预测管理模型的规则, sequence=null, from=null, capacities=null, status=null), Principle(code=fsadcp_p2, description=null, details=[PrincipleDetail(code=fsadcp_p2d1, sequence=null, content=透过任务过滤的前置参数设定，于后续每次拿到客户预测需求，系统即可计算并汇整，列出符合合理备货条件的清单，以及供需不平衡需要处理的品项，让业务省去资料整理的繁琐过程，直接让数据结果驱动相应的任务，使业务可以更加专注于重要的产销协调。, description=二、预测偏差分析的响应策略 1. 生管/物控根据客户预测展开厂内备货计划 敏态：, relatedParamKeys=[DataEntry_MaintenanceOfInventoryInfo2, DataEntry_order_information_maintenance2, maintenanceOfSparePartsInfo2], otherParamKeys=[steadyStateInfos2], title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p2d2, sequence=null, content=, description=稳态：ERP作业 2. 各业务收到负责客户之预测资料后，于FCI发起项目，将客户预测资料导入系统，产生预测偏差分析结果。 , relatedParamKeys=[DetectingSalesForecastData], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p2d3, sequence=null, content=, description=3. 将预测供需异常的预警清单，推送给负责业务进行处理，负责业务需于2天内完成所有预警品项的跟进。 3-1 供给不足 3-1-1 召开产销会议，讨论如何协调生产计划。 3-1-2 检视前后预测版本的差异数量，判断客户需求突增，导致交货不及，则实时与客户进行协商。 3-2 供给溢出 3-2-1 确认已开制令单之生产状况，请生管删除未开工之制令单。 3-2-2 检视前后预测版本的差异数量，如果出现客户砍单情形，则及时与客户协商是否确定取消需求或是延迟交货，尽量降低厂内损失。 3-2-3 召开呆滞会议，讨论备货合理性，是否处理多余存货。 4. 将预测供需整体状况推送给主管，并监控业务处理进度，如果负责业务超过2天未处理完成，会出现逾时的异常警示。 5. 产销会议后的追踪 5-1 业务于任务卡点选重新计算，验证生管/物控是否依据产销协调的共识，调整备货计划。 5-2 系统根据产销会议的内容，自动侦测并主动预警。 5-2-1 产销会议针对供给不足的品项，决议增加生产，于指定开工日后重新计算，如果偏差量未修正，则预警负责业务跟进，避免出现交货风险。 5-2-2 产销会议针对供给溢出的品项，决议取消生产，任务卡提交后2天检查，如果供给量未下修，则预警负责业务跟进，避免库存呆滞的风险。 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=预测偏差分析的响应策略, sequence=null, from=null, capacities=null, status=null)], extensions=[{packageCode=FCI, name=预测流程设定, language={name={zh_TW=預測流程設定, zh_CN=预测流程设定}}, lang={name={zh_TW=預測流程設定, zh_CN=预测流程设定}}, type=@@export_pic_DataEntry_PredictiveProcessSetup, key=DataEntry_PredictiveProcessSetup}], version=2.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=[ParamApi(from=esp, queryApi=bm.ppsc.parameter.setting.get, updateApi=bm.ppsc.parameter.setting.update, paramDescription=[FieldDescription(name=预测需求量, key=calculationRules, value=null, dataType=null, componentType=null, format=null, express=null, fields=null, description=预测需求量, defaultValue=[forecast_demand_qty, inventory_qty], displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{disabled=true, language={label={zh_TW=預測需求量, en_US=预测需求量, zh_CN=预测需求量}}, label=预测需求量, lang={label={zh_TW=預測需求量, en_US=预测需求量, zh_CN=预测需求量}}, value=forecast_demand_qty}, {language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=库存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交货量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在制, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, label=在途量, lang={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, value=on_the_way_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=null, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null), FieldDescription(name=取稳态的数据类型, key=steadyStateDataType, value=null, dataType=null, componentType=null, format=null, express=null, fields=null, description=取稳态的数据类型, defaultValue=[], displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=库存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交货量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在制, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=下階庫存, en_US=lower_level_inventory_qty, zh_CN=下阶库存}}, label=下阶库存, lang={label={zh_TW=下階庫存, en_US=lower_level_inventory_qty, zh_CN=下阶库存}}, value=lower_level_inventory_qty}, {language={label={zh_TW=下階採購, en_US=lower_level_purchase_qty, zh_CN=下阶采购}}, label=下阶采购, lang={label={zh_TW=下階採購, en_US=lower_level_purchase_qty, zh_CN=下阶采购}}, value=lower_level_purchase_qty}, {language={label={zh_TW=下階在製, en_US=lower_level_in_qty, zh_CN=下阶在制}}, label=下阶在制, lang={label={zh_TW=下階在製, en_US=lower_level_in_qty, zh_CN=下阶在制}}, value=lower_level_in_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=null, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null), FieldDescription(name=汇总真预测, key=aggregateTruePrediction, value=null, dataType=null, componentType=null, format=null, express=null, fields=null, description=设置真预测的汇总规则，依客户汇总：同一客户有新的预测数据时,按最新预测调整该客户所有的真预测数据；依客户和品号汇总：同一客户有新的预测数据时，按最新预测调整该客户同品号的真预测数据, defaultValue=customer, displayType=RADIO, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, label=依客户汇总, lang={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, value=customer}, {language={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, label=依客户和品号汇总, lang={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, value=customer_item}], sequence=13, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=null, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null)], queryRequestParmas=null, supportOperationUnit=false)])], @Integer[1], ], null, ] method=com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService$$EnhancerBySpringCGLIB$$e4137eb9.mergeMechanismDesignInfo location=AtExit ts=2024-09-24 10:37:00.049; [cost=10.075226ms] result=@ArrayList[ @Object[][ @Mechanism[Mechanism(code=ForecastingSupplyAndDemandControlPlanning, name=预测供需调控机制, description=null, author=null, createTime=null, executeDescription=null, tags=null, supportProducts=null, requireDataFrame=null, paramDescription=[FieldDescription(name=客戶資訊維護, key=maintenanceOfCustomerInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfCustomerInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=品號資訊維護, key=maintenanceOfProductInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfProductInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=客戶品號關係維護, key=baseDataEntry_product_informtion_maintenance, value=\u0026routerLink=/base-data-entry\u0026code=baseDataEntry_product_informtion_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=責任人設定, key=forecastingResponderAndExecutor, value=\u0026routerLink=/base-data-entry\u0026code=forecastingResponderAndExecutor\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=預測流程設定, key=DataEntry_PredictiveProcessSetup, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_PredictiveProcessSetup\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=21, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=預測需求量, key=calculationRules, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=預測需求量, defaultValue=[inventory_qty], displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=庫存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交貨量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在製, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, label=在途量, lang={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, value=on_way_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=null, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=預測BOM資訊維護, key=maintenanceOfBomInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfBomInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=取穩態的資料類型, key=steadyStateDataType, value=null, dataType=string, componentType=null, format=null, express=null, fields=null, description=取穩態的資料類型, defaultValue=null, displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=庫存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交貨量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在製, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=下階庫存, en_US=next qty, zh_CN=下阶库存}}, label=下階庫存, lang={label={zh_TW=下階庫存, en_US=next qty, zh_CN=下阶库存}}, value=next_qty}, {language={label={zh_TW=下階採購, en_US=next purchase, zh_CN=下阶采购}}, label=下階採購, lang={label={zh_TW=下階採購, en_US=next purchase, zh_CN=下阶采购}}, value=next_purchase}, {language={label={zh_TW=下階在製, en_US=next in qty, zh_CN=下阶在制}}, label=下階在製, lang={label={zh_TW=下階在製, en_US=next in qty, zh_CN=下阶在制}}, value=next_in_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=null, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed={valueMapping=[{op=EQUAL, rightValueType=CONSTANT, right=Y, value=[inventory_qty]}, {op=EQUAL, rightValueType=CONSTANT, right=N, value=[]}], serviceName=hasGroundEnd, type=SYSTEM}, isTop=false), FieldDescription(name=庫存資訊維護, key=DataEntry_MaintenanceOfInventoryInfo, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_MaintenanceOfInventoryInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=訂單資訊維護, key=DataEntry_order_information_maintenance, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_order_information_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=備料資訊維護, key=maintenanceOfSparePartsInfo, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfSparePartsInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=DynamicCondition(items=null), fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=彙總真預測, key=aggregateTruePrediction, value=null, dataType=object, componentType=null, format=null, express=null, fields=null, description=設定真預測的彙總規則，依客戶彙總：同一客戶有新的預測數據時,按最新預測調整該客戶所有的真預測數據；依客戶和品號彙總：同一客戶有新的預測數據時，按最新預測調整該客戶同品號的真預測數據, defaultValue=customer, displayType=RADIO, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, label=依客戶彙總, lang={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, value=customer}, {language={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, label=依客戶和品號彙總, lang={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, value=customer_item}], sequence=13, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=庫存資訊維護, key=DataEntry_MaintenanceOfInventoryInfo2, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_MaintenanceOfInventoryInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=訂單資訊維護, key=DataEntry_order_information_maintenance2, value=\u0026routerLink=/base-data-entry\u0026code=DataEntry_order_information_maintenance\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=備料資訊維護, key=maintenanceOfSparePartsInfo2, value=\u0026routerLink=/base-data-entry\u0026code=maintenanceOfSparePartsInfo\u0026category=SIGN-DOCUMENT, dataType=object, componentType=null, format=null, express=null, fields=null, description=跳轉到athena基礎資料錄入進行相關基礎資料錄入, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=20, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=false, executeName=null, colspan=0, extendData=null, category=link, from=athena, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=false, openWindowTip=null, valueComputed=null, isTop=false), FieldDescription(name=DetectingSalesForecastData, key=DetectingSalesForecastData, value=null, dataType=object, componentType=null, format=null, express=null, fields=null, description=DetectingSalesForecastData, defaultValue=null, displayType=LINK, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=null, sequence=9999, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=true, isSimulation=null, executeName=null, colspan=0, extendData=null, category=link, from=default, isOperation=null, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null)], sourceOperationUnit=null, skipSimulate=true, defaultPageTemplate=null, defaultPresent=null, composite=true, subMechanisms=null, hasSet=false, enhance=null, simulateResultWriteBack=null, skipOutputFile=false, principles=[Principle(code=fsadcp_p1, description=null, details=[PrincipleDetail(code=fsadcp_p1d1, sequence=null, content=明确需纳入预测管理的范畴，先行厘清各产品类型，对应之备货流程与产销模式，方能使得预测管理的效益最大化。 , description=一、建构预测管理模型的规则： 1-1 針對固定頻率提供預測的客戶進行維護 , relatedParamKeys=[maintenanceOfCustomerInfo], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d2, sequence=null, content=, description=1-2 針對1-1客戶預測的備貨品號進行維護 , relatedParamKeys=[maintenanceOfProductInfo], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d3, sequence=null, content=, description=1-3 針對預測廠內品號維護對應客戶品號 , relatedParamKeys=[baseDataEntry_product_informtion_maintenance], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d4, sequence=null, content=, description=2. 当预测偏差发生时，为确保响应不遗漏，需事先明确负责人员；未来发生预测偏差时，会自动派送任务卡给对应负责人： 2-1 负责处理预测偏差的人员 2-2 负责监督处理状况的人员 , relatedParamKeys=[forecastingResponderAndExecutor], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d5, sequence=null, content=, description=3. 订定预测偏差分析的计划周期区间范围： 预测偏差分析周期需小于等于滚动式预测(Rolling Forecast)的范围，且至少超过合理的前置时间(Lead Time)。例如：客户每个月会更新未来12个月的预测，且客户产品于厂内产销协调后的备货周期是3个月，则预测偏差分析周期须设定介于3-12个月之间。 预测偏差分析周期可以根据实际备货需求动态调整。 , relatedParamKeys=[DataEntry_PredictiveProcessSetup], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d6, sequence=null, content=, description=4. 是否与客户有协议，历史未交货的预测需求，客户要全数出货。 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d7, sequence=null, content=, description=5. 预测备货如需考虑订单需求，须择一设定预测与订单的冲销方式，避免相同需求重复备货： 5-1 当期有预测需求，则以预测为主，不考虑订单数量。 5-2 当期有订单需求，则以订单为主，不考虑预测数量。 5-3 比对当期预测与订单数量，两者取其大。 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d8, sequence=null, content=, description=6. 预测偏差分析周期内的需求，与厂内备货的供给有落差时，订定需预警的偏差幅度。 未来可以依据偏差率稳定下降后，动态调降偏差分析标准。 6-1 供给溢出－评估合理备货范围 6-2 供给不足－如果不允许任何交货风险则设定0% , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d9, sequence=null, content=, description=7-1 订定备货供给的范畴 7-1-1 MTS(Make to Stock 存货式生产) , relatedParamKeys=[calculationRules], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d10, sequence=null, content=, description=7-1-2 ATO(Assembly to Order 接单后装配) 预测BOM与生产BOM不同，生产BOM是生产过程中的产品架构，预测BOM则是以预测备货的角度建置，不存在多阶架构。 举例来说，半成品到成品的生产周期很短，因此备货至半成品即可满足出货需求，则预测BOM只需要建置上阶成品，对应下阶半成品，不需要建置尾阶原材料。 此外，如果上阶主件成品对应下阶有20个元件，其中17个备货风险较低，则预测BOM只需要建置3个关键元件，进行预测备货管理的风险监控。 , relatedParamKeys=[maintenanceOfBomInfo], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d11, sequence=null, content=, description=7-2 订定备货供给的数据来源： , relatedParamKeys=[steadyStateDataType, DataEntry_MaintenanceOfInventoryInfo, DataEntry_order_information_maintenance, maintenanceOfSparePartsInfo], otherParamKeys=[steadyStateInfos], title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p1d12, sequence=null, content=, description=8. 与客户制定预测资料更新方式，是否直接覆盖，或是根据品号更新。 , relatedParamKeys=[aggregateTruePrediction], otherParamKeys=null, title=null, from=null, subtitle=null)], title=项⽬建构预测管理模型的规则, sequence=null, from=null, capacities=null, status=null), Principle(code=fsadcp_p2, description=null, details=[PrincipleDetail(code=fsadcp_p2d1, sequence=null, content=透过任务过滤的前置参数设定，于后续每次拿到客户预测需求，系统即可计算并汇整，列出符合合理备货条件的清单，以及供需不平衡需要处理的品项，让业务省去资料整理的繁琐过程，直接让数据结果驱动相应的任务，使业务可以更加专注于重要的产销协调。, description=二、预测偏差分析的响应策略 1. 生管/物控根据客户预测展开厂内备货计划 敏态：, relatedParamKeys=[DataEntry_MaintenanceOfInventoryInfo2, DataEntry_order_information_maintenance2, maintenanceOfSparePartsInfo2], otherParamKeys=[steadyStateInfos2], title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p2d2, sequence=null, content=, description=稳态：ERP作业 2. 各业务收到负责客户之预测资料后，于FCI发起项目，将客户预测资料导入系统，产生预测偏差分析结果。 , relatedParamKeys=[DetectingSalesForecastData], otherParamKeys=null, title=null, from=null, subtitle=null), PrincipleDetail(code=fsadcp_p2d3, sequence=null, content=, description=3. 将预测供需异常的预警清单，推送给负责业务进行处理，负责业务需于2天内完成所有预警品项的跟进。 3-1 供给不足 3-1-1 召开产销会议，讨论如何协调生产计划。 3-1-2 检视前后预测版本的差异数量，判断客户需求突增，导致交货不及，则实时与客户进行协商。 3-2 供给溢出 3-2-1 确认已开制令单之生产状况，请生管删除未开工之制令单。 3-2-2 检视前后预测版本的差异数量，如果出现客户砍单情形，则及时与客户协商是否确定取消需求或是延迟交货，尽量降低厂内损失。 3-2-3 召开呆滞会议，讨论备货合理性，是否处理多余存货。 4. 将预测供需整体状况推送给主管，并监控业务处理进度，如果负责业务超过2天未处理完成，会出现逾时的异常警示。 5. 产销会议后的追踪 5-1 业务于任务卡点选重新计算，验证生管/物控是否依据产销协调的共识，调整备货计划。 5-2 系统根据产销会议的内容，自动侦测并主动预警。 5-2-1 产销会议针对供给不足的品项，决议增加生产，于指定开工日后重新计算，如果偏差量未修正，则预警负责业务跟进，避免出现交货风险。 5-2-2 产销会议针对供给溢出的品项，决议取消生产，任务卡提交后2天检查，如果供给量未下修，则预警负责业务跟进，避免库存呆滞的风险。 , relatedParamKeys=null, otherParamKeys=null, title=null, from=null, subtitle=null)], title=预测偏差分析的响应策略, sequence=null, from=null, capacities=null, status=null)], extensions=[{packageCode=FCI, name=预测流程设定, language={name={zh_TW=預測流程設定, zh_CN=预测流程设定}}, lang={name={zh_TW=預測流程設定, zh_CN=预测流程设定}}, type=@@export_pic_DataEntry_PredictiveProcessSetup, key=DataEntry_PredictiveProcessSetup}], version=2.0, paradigmCode=null, dependMechanismCodes=null, existsSystemPrinciples=null, mergeVersion=null, capacities=null, eocLevel=null, paramApi=[ParamApi(from=esp, queryApi=bm.ppsc.parameter.setting.get, updateApi=bm.ppsc.parameter.setting.update, paramDescription=[FieldDescription(name=预测需求量, key=calculationRules, value=null, dataType=null, componentType=null, format=null, express=null, fields=null, description=预测需求量, defaultValue=[forecast_demand_qty, inventory_qty], displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{disabled=true, language={label={zh_TW=預測需求量, en_US=预测需求量, zh_CN=预测需求量}}, label=预测需求量, lang={label={zh_TW=預測需求量, en_US=预测需求量, zh_CN=预测需求量}}, value=forecast_demand_qty}, {language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=库存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交货量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在制, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, label=在途量, lang={label={zh_TW=在途量, en_US=on way qty, zh_CN=在途量}}, value=on_the_way_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=null, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null), FieldDescription(name=取稳态的数据类型, key=steadyStateDataType, value=null, dataType=null, componentType=null, format=null, express=null, fields=null, description=取稳态的数据类型, defaultValue=[], displayType=CHECKBOX, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, label=库存, lang={label={zh_TW=庫存, en_US=inventory qty, zh_CN=库存}}, value=inventory_qty}, {language={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, label=交货量, lang={label={zh_TW=交貨量, en_US=delivery qty, zh_CN=交货量}}, value=delivery_qty}, {language={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, label=在制, lang={label={zh_TW=在製, en_US=in qty, zh_CN=在制}}, value=in_qty}, {language={label={zh_TW=下階庫存, en_US=lower_level_inventory_qty, zh_CN=下阶库存}}, label=下阶库存, lang={label={zh_TW=下階庫存, en_US=lower_level_inventory_qty, zh_CN=下阶库存}}, value=lower_level_inventory_qty}, {language={label={zh_TW=下階採購, en_US=lower_level_purchase_qty, zh_CN=下阶采购}}, label=下阶采购, lang={label={zh_TW=下階採購, en_US=lower_level_purchase_qty, zh_CN=下阶采购}}, value=lower_level_purchase_qty}, {language={label={zh_TW=下階在製, en_US=lower_level_in_qty, zh_CN=下阶在制}}, label=下阶在制, lang={label={zh_TW=下階在製, en_US=lower_level_in_qty, zh_CN=下阶在制}}, value=lower_level_in_qty}], sequence=7, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=null, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null), FieldDescription(name=汇总真预测, key=aggregateTruePrediction, value=null, dataType=null, componentType=null, format=null, express=null, fields=null, description=设置真预测的汇总规则，依客户汇总：同一客户有新的预测数据时,按最新预测调整该客户所有的真预测数据；依客户和品号汇总：同一客户有新的预测数据时，按最新预测调整该客户同品号的真预测数据, defaultValue=customer, displayType=RADIO, displayTypeDefinition=null, displayFormat=null, valueDisplay=null, options=[{language={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, label=依客户汇总, lang={label={zh_TW=依客戶彙總, en_US=依客户汇总, zh_CN=依客户汇总}}, value=customer}, {language={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, label=依客户和品号汇总, lang={label={zh_TW=依客戶和品號彙總, en_US=依客户和品号汇总, zh_CN=依客户和品号汇总}}, value=customer_item}], sequence=13, required=true, readOnly=false, validateScript=null, paramVisbleScope=null, paramVisble=null, paramDisableScope=null, paramDisable=null, isExecute=null, isSimulation=false, executeName=null, colspan=0, extendData=null, category=variable, from=default, isOperation=false, eocLevel=null, executable=null, periodEnable=null, fieldMappings=null, dynamicCondition=null, group=null, mechanismCode=null, visibleCondition=null, fieldOpenWindowMap=null, fieldOpenWindowDefinitions=null, forceOpen=null, max=null, min=null, step=null, target=null, linkKey=null, linkOptions=null, parameters=null, precision=null, restrictions=null, filters=null, isSingleOperation=null, openWindowTip=null, valueComputed=null, isTop=null)], queryRequestParmas=null, supportOperationUnit=false)])], @Integer[1], ], null, ] 发现生产区代码没上上去 [arthas@80]$ sm com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService handleLanguage Affect(row-cnt:0) cost in 20 ms. [arthas@82]$ sm com.digiwin.athena.knowledgegraph.service.impl.MechanismDesignService handleLanguage Affect(row-cnt:0) cost in 23 ms. 镜像版本一致，但是代码不同。\n应该是 ops 的 bug 造成。\n","description":"","tags":null,"title":"代码未生效问题调查","uri":"/posts/not_effect_multilan/"},{"categories":null,"content":"背景 还是老问题，线上接口多次请求数据不一致。\n过程 通过本地程序并发测试，本地无法复现。\n并发测试线上接口，可以复现。\n查看日志，发现日志打印不全。不要使用 log.error(“xxxx:{}”, e.getMessage())\n修改日志, 打出异常堆栈。\n1 log.error(\"xxxx\", e) 发现在并发中，调用会报空指针异常。\n1 HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); 当前线程是没有设置的，但是代码里会调用继承的, 这里本地并发测试无法复现，但是线上会出现。\n1 2 3 4 5 6 7 8 @Nullable public static RequestAttributes getRequestAttributes() { RequestAttributes attributes = requestAttributesHolder.get(); if (attributes == null) { attributes = inheritableRequestAttributesHolder.get(); } return attributes; } 这个内部的原因暂时还未知，需要了解下继承 threadLoacal 和内部线程流转。\n","description":"","tags":null,"title":"数据不一致问题调查","uri":"/posts/data_not_same/"},{"categories":null,"content":"背景 最近发现一个 bug, 有时返回数据，有时返回null。\n过程 初步调查发现，应该是报错，但是异常被吞了，导致返回 null。 本地调用无法复现，推测可能是并发场景下的报错。\n于是通过 postman 来并发调用接口，果然复现了异常。\nRequest.notifyAttributeAssigned 时由于 context 对象为 null 因此异常。\njava.lang.NullPointerException at org.apache.catalina.connector.Request.notifyAttributeAssigned(Request.java:1555) at org.apache.catalina.connector.Request.setAttribute(Request.java:1541) at org.apache.catalina.connector.RequestFacade.setAttribute(RequestFacade.java:540) at javax.servlet.ServletRequestWrapper.setAttribute(ServletRequestWrapper.java:293) at javax.servlet.ServletRequestWrapper.setAttribute(ServletRequestWrapper.java:293) at com.digiwin.athena.knowledgegraph.configuration.KgCacheAspect.around(KgCacheAspect.java:48) at sun.reflect.GeneratedMethodAccessor183.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644) at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633) at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:174) at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:689) at com.digiwin.athena.knowledgegraph.service.impl.TaskService$$EnhancerBySpringCGLIB$$73d3828b.postActivityDefinition(\u003cgenerated\u003e) at com.digiwin.athena.knowledgegraph.service.impl.TaskService.lambda$null$98(TaskService.java:3799) at java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1640) at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:750) 那说明有线程 clear context\n加了一些 debug 日志条件发现如下现象\n46 线程访问完毕后，清除这个对象，但是 45号线程内部的线程池线程 18 正在访问。\n为什么 45号内部的线程会访问不属于 45号的对象 2be669ff 而是用 15e2628d\nhttp-nio-8085-exec-46 mappingData is clear contexts:org.apache.catalina.mapper.MappingData@15e2628d nullhttp-nio-8085-exec-45 notifyorg.apache.catalina.mapper.MappingData@2be669ff:StandardEngine[Tomcat].StandardHost[localhost].TomcatEmbeddedContext[] http-nio-8085-exec-45 pool-18-thread-1 notifyorg.apache.catalina.mapper.MappingData@15e2628d:null 1 2 3 4 5 6 7 8 @Nullable public static RequestAttributes getRequestAttributes() { RequestAttributes attributes = requestAttributesHolder.get(); if (attributes == null) { attributes = inheritableRequestAttributesHolder.get(); } return attributes; } 一般线程会取到从外部继承的，但是为什么这个跟外部继承的不一致呢？\n推测是由于内部使用了线程池。线程池的中线程继承的外部 threadLocal 对象。当外部存在并发时，内部调用 inheritableRequestAttributesHolder.get() 可能拿到的并非是实际外部的线程中的 threadLocal。\n例如 nio 线程 1 分配了 thread-1 nio 线程 2 分配了 thread-2 nio 线程3 分配了 thread-1 这是 thread-1 拿到的可能是 3 的 threadLocal 对象，那么就会出现错乱。\n解决方案 既然推测是线程继承导致的，那么就不用使用继承了，直接用自己的 threadLocal 这样一个线程就一个了，应该就可以了。\nRequestContextHolder.setRequestAttributes(requestAttributes);\n","description":"","tags":null,"title":"HttpServletRequest.setAttribute 不支持并发问题调查","uri":"/posts/httpservletrequest/"},{"categories":null,"content":"","description":"","tags":null,"title":"Async-profiler","uri":"/posts/asyncprofiler/"},{"categories":null,"content":"","description":"","tags":null,"title":"Pinpoint 使用","uri":"/posts/pinpoint/"},{"categories":null,"content":"最近调研了接口文档生成工具，发现了 smart-doc，但是不支持公司内部的微服务，于是在 issue 上相同问题的内容下提问了下\nhttps://github.com/TongchengOpenSource/smart-doc/issues/683\n没想到社区很快回复了，并列出了可行思路。\nhttps://github.com/TongchengOpenSource/smart-doc/discussions/795\n我看了下思路，又看了下代码，发现实现起来比想象的要简单很多。\n于是捣鼓了半天，提交了代码\nhttps://github.com/TongchengOpenSource/smart-doc/pull/797 https://github.com/TongchengOpenSource/smart-doc-maven-plugin/pull/70 https://github.com/smart-doc-group/smart-doc-group.github.io/pull/87\n很快经过 review 修改后合并了。\n哈哈，开心。\n以后只要增加 @javadoc 注释，就可以支持普通 service 接口了。\n","description":"","tags":null,"title":"smart-doc 支持普通 service 接口或者公司内部 rpc 微服务接口","uri":"/posts/smart_doc/"},{"categories":null,"content":"背景 由于后端接口众多，接口文档的维护不好，因此需要调研相关接口管理工具，来规范化接口文档，提高开发和沟通效率。\n工具 Swagger 介绍 swagger 通过代码注解将相关注释内容通过 swagger-ui 体现到接口文档上，文档会随着项目启动而实时变化。 国内目前多数使用的为 springfox-swagger2（最近版本停留在 2020-07 月 https://github.com/springfox/springfox） 最新的版本已经改为 springdoc（https://github.com/springdoc/springdoc-openapi） 使用 由于国内主要使用的 springfox-swagger2， 所以这里就主要介绍下 swagger2 注解的使用。\nhttps://juejin.cn/post/6993166706398986277\nSmart-doc 介绍 Smart-doc 是基于java 标准注释来生成对应的文档接口 运行时代码无 smart-doc 相关内容，需要利用 mvn 插件生成对应接口文档 文档不会在项目启动时实时更新，但是可以在编译期更新 Github: https://github.com/TongchengOpenSource/smart-doc 使用 https://smart-doc-group.github.io/#/zh-cn/start/guide\n集成 https://smart-doc-group.github.io/#/zh-cn/start/quickstart\n优劣势分析 Swagger: 优点：使用广泛，代码中已经集成，方便前端调试 缺点：需要侵入代码，通过运行时反射机制实现，代码库不会更新\nSmart-doc: 优点：无代码侵入，运行时不受影响（可以理解为分析和生成文档的工具），只写注释即可，开源库更新及时 缺点：调试能力稍弱于 swagger\n问题记录 无论 swagger 还是 smart-doc 都无法创建我们项目内部封装的 rpc 接口。\n幸运的是，通过 github 联系上了 smart-doc 的相关 Maintainer shalousun，在他的提议下，我按照他的建议提了个 PR 来支持普通的 service 接口。\n通过 @javadoc 注解来标注该 service 接口需要显示出来。\nhttps://github.com/TongchengOpenSource/smart-doc/pull/797\n希望这个 pr 最终能够被合并进去。\n","description":"","tags":null,"title":"接口管理工具调研","uri":"/posts/interface_management_tool/"},{"categories":null,"content":"初探 MongoDB 感觉上存的主要是对象，也可以理解为文档信息\n","description":"","tags":null,"title":"MongoDB","uri":"/posts/mongodb/"},{"categories":null,"content":"开篇 最近看到了个 rust 开源库 fluvio\n数据流处理平台，初步看起来像 MQ\nhttps://github.com/infinyon/fluvio\n首先代码下下来看看情况\n按照这个文档，可以快速创建 topic，然后发消息，并且通过 smdk 来快速创建 smartmodule 并生效\nhttps://www.fluvio.io/docs/#installing-the-cli\n接下来，看下有没有 good first issue.\n看了半天，感觉暂缓吧～\n先自己写两个项目。\n","description":"","tags":null,"title":"Fluvio","uri":"/posts/fluvio/"},{"categories":null,"content":"提及次数排名\nMysql 19\nSpringBoot 17\nMyBatis 17\nRedis 17\nSpring 16\nSrping Cloud 15\nMongoDB 10\nSpringMVC 9\nLinux系统 9\nKafka 9\nDubbo 6\nDocker 6\nVUE 4\nES 4\nkubernetes 4\nRabbitMQ 4\nzookeeper 4\nTCP/IP 4\nNode 3\noracle 3\nHttp 3\nNetty 3\nAngular 2\nreact 2\nNginx 2\nMemcached 2\nFlink 2\nSpring 相关 Spring 16\nSrping Cloud 15\nSpringBoot 17\nSpringMVC 9\nSpring Data\n微服务 Dubbo 6\nIstio\njdbc 框架 MyBatis 17\nhibernate\nJPA\n操作系统 Linux系统 9\n前端技术 VUE 4\nAngular 2\nreact 2\nNode 3\nNginx 2\nJQuery\nExtJS\nECharts\nJavaScript\nHTML\nCSS\n数据库 Mysql 19\nMongoDB 10\noracle 3\nSQLServer\nTiDB\n缓存/ 查询加速 Redis 17\nMemcached 2\nES 4\n云原生 kubernetes 4\nDocker 6\n消息 Kafka 9\nRabbitMQ 4\nRocketMQ\nActiveMQ\n协调通信 zookeeper 4\nTCP/IP 4\nHttp 3\nNetty 3\nUDP SIP\nRCS\nRTP MQTT\nNB-IoT\nCoAP\nGRPC\n大数据 Flink 2\nHadoop\nHbase\nClickHouse\nDoris\nSpark\n","description":"","tags":null,"title":"分析最近投递的 30 家招聘技术要求中提及的相关技术的次数","uri":"/posts/need_tech/"},{"categories":null,"content":"ORACLE 同义词的定义及使用 1 CREATE SYNONYM TEST FOR DM.TM_WGG_ATM_GTW_MON; 同义词就是别名，与视图类似，都是不占实际存储空间，相当于是访问数据库对象的另外一种方式。 公用同义词和私用同义词 同义词的作用 多用户协同开发，不再需要使用 user.tableA 而是直接使用同义词即可。 简化 sql 语句 如何实现不同用户使用相同 SQL 查询 用户 encrypt\n1 2 3 create synonym TEST_SYNONYM FOR AB01; grant select on TEST_SYNONYM to encrypt_1; 用户 encrypt_1\n1 2 ## 创建一个指向 encrypt.TEST_SYNONYM 的同义词 create synonym TEST_SYNONYM for encrypt.TEST_SYNONYM; 这样两个用户都可以使用\n1 select * from TEST_SYNONYM; ShardingSphere 中同义词加载的初步想法 查询用户 schema 下能够访问到的所有同义词，包含私有和公有的同义词 找到同义词指向的对象 找到对象指向的对象 通过如下 SQL 可以找到最终指向的对象。但是如果指向的是其它用户下的 SYNONYM ，似乎查不出来\n1 2 3 4 5 6 SELECT * FROM USER_DEPENDENCIES where REFERENCED_TYPE in ('TABLE', 'VIEW') START WITH NAME IN ('ENCRYPT_1_B_TEST_1') CONNECT BY (PRIOR REFERENCED_NAME) = NAME AND (SELECT SYS_CONTEXT('USERENV', 'CURRENT_SCHEMA') FROM DUAL) = (PRIOR REFERENCED_OWNER) ORDER BY NAME, REFERENCED_NAME; ","description":"","tags":null,"title":"Oracle 同义词","uri":"/posts/oracle_synonym/"},{"categories":null,"content":"RocketMQ 初探 4个组件 producer，consumer，broker，nameserver\n查看 RocketMQ 文档 https://rocketmq.apache.org/zh/docs/\n功能特性 普通消息 定时/延时消息 ","description":"","tags":null,"title":"RocketMQ","uri":"/posts/rocketmq/"},{"categories":null,"content":"以配置 XA 事务为例，查看整体流程 transaction rule 的初始化 1 2 3 4 5 6 7 8 9 public TransactionRule(final TransactionRuleConfiguration ruleConfig, final Map\u003cString, ShardingSphereDatabase\u003e databases) { configuration = ruleConfig; defaultType = TransactionType.valueOf(ruleConfig.getDefaultType().toUpperCase()); providerType = ruleConfig.getProviderType(); props = ruleConfig.getProps(); // 创建 transactionManager resource = new AtomicReference\u003c\u003e(createTransactionManagerEngine(databases)); attributes = new RuleAttributes(); } XA transactionManager 内部管理着 XA DataSource 也一并初始化(mysql 提供的 com.mysql.cj.jdbc.MysqlXADataSource)\n1 2 3 4 5 public final class XAShardingSphereTransactionManager implements ShardingSphereTransactionManager { private final Map\u003cString, XATransactionDataSource\u003e cachedDataSources = new CaseInsensitiveMap\u003c\u003e(); private XATransactionManagerProvider xaTransactionManagerProvider; 以 insert 为例，查看全卷事务如何管理 其实就是使用初始化时使用的 transactionManager 来 begine 和 commit.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 private \u003cT\u003e T doExecuteWithImplicitCommitTransaction(final ImplicitTransactionCallback\u003cT\u003e callback) throws SQLException { T result; // 获取上述初始化的 xatransactionManagerEngine BackendTransactionManager transactionManager = new BackendTransactionManager(databaseConnectionManager); try { transactionManager.begin(); result = callback.execute(); transactionManager.commit(); // CHECKSTYLE:OFF } catch (final Exception ex) { // CHECKSTYLE:ON transactionManager.rollback(); String databaseName = databaseConnectionManager.getConnectionSession().getDatabaseName(); throw SQLExceptionTransformEngine.toSQLException(ex, ProxyContext.getInstance().getContextManager().getMetaDataContexts().getMetaData().getDatabase(databaseName).getProtocolType()); } return result; } 总结 从 shardingSphere 角度来看，其实对于分布式事务而言，主要提供了一些封装和框架流程的调用。内部的 transactionManager 实际由其它开源组件提供。 例如 narayana 和 atomics。\nnarayana 和 atomics 其实提供了 transactionManager 角色和功能， shardingSphere 做了一些封装。 各个数据库，承担的是 resourceManager 角色和功能。 TM 发起 prepare， RM 准备提交，达到万事俱备只欠东风的状态。 TM 发现大家都完成了之后，发起提交命令，如果有人失败了，那么就整体回滚。\n","description":"","tags":null,"title":"ShardingSphere 中的分布式事务","uri":"/posts/distributed_transactions_in_shardingsphere/"},{"categories":null,"content":"背景 今天写一个并发查询的功能，但是报了如下的错误\njava.sql.SQLException: Streaming result set com.mysql.cj.protocol.a.result.ResultsetRowsStreaming@3e72fd68 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129) at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) at com.mysql.cj.jdbc.StatementImpl.executeSimpleNonQuery(StatementImpl.java:1241) at com.mysql.cj.jdbc.StatementImpl.setupStreamingTimeout(StatementImpl.java:632) at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:947) at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52) at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java) 探索 奇怪的是，虽然是并发，但是我都使用了独立的 hikari 连接，并且每次查询完都主动 close 了连接和 statments，当然这里要注意下，随时使用了全新的 hikari 连接，但是因为它有连接池，所以其它线程依然可以重复使用空闲的数据库连接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Override public void close() throws SQLException { Collection\u003cSQLException\u003e result = new LinkedList\u003c\u003e(); for (Statement each : statements) { try { each.cancel(); each.close(); } catch (final SQLException ex) { if (isClosed(each)) { continue; } result.add(ex); } } for (Connection each : connections) { try { each.close(); } catch (final SQLException ex) { result.add(ex); } } if (result.isEmpty()) { return; } SQLException ex = new SQLException(); result.forEach(ex::setNextException); throw ex; } 为什么会出现这个问题呢？难道是没有实际关掉 ResultSet 导致的么, 于是我又增加了如下代码\n1 2 3 4 5 6 7 8 9 10 for (ResultSet each : resultSets) { try { each.close(); } catch (final SQLException ex) { if (isClosed(each)) { continue; } result.add(ex); } } 报错信息变了\njava.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"Object.toString()\" because \"type\" is null 这原来是真正的问题，查询过程中出现了异常。\n当我修改了如下问题后，即使没有关闭 resultSet 仅关闭 statment 也不会再有异常了。\n所以可以推测，仅关闭 statment 并不会关闭 resultSet.\n那么为什么无异常的情况下，不主动关闭 resultSet 也无报错呢。\n推测是由于异常了，所以没有调用 while（resultSet.next()） 因此没有关。\n那我们可以修改 while 为 if 来验证\n可以看到异常又重现了\njava.sql.SQLException: Streaming result set com.mysql.cj.protocol.a.result.ResultsetRowsStreaming@3d0961f9 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. 结论 仅关闭 statement 不能关闭 resultset resultSet next() 方法需要循环调用到结束，可以自动关闭流式结果集 最好主动关闭 resultSet. ","description":"","tags":null,"title":"Streaming result set com.mysql.cj.protocol.a.result.ResultsetRowsStreaming@3e72fd68 is still active.","uri":"/posts/hikari_result_set/"},{"categories":null,"content":"注解源码解析 @Configuration 如何扫描注解，并且构建 bean @ComponentScan 定义 filter, 源码查看如何扫描注释，使用过滤规则 @Bean 源码查看如何 parser bean 注解信息，如何 实例化，并调用自定义的 init 和 destroy。 从 IOC 容器中获取 Bean 的过程 @Import 注解与 bean 注解类似，不过使用起来更灵活些 @PropertySource 如何解析自定义文件，并加入到 spring 环境中 ","description":"","tags":null,"title":"Spring 核心知识","uri":"/posts/spring/"},{"categories":null,"content":"Innodb 索引结构 B+tree 相比红黑树可以用更少的磁盘 IO B+tree 相比 Hash 结构和 B-tree，支持范围查询\n索引分为聚簇索引和二级索引。 聚簇索引就是包含所有的内容，表数据存于叶子节点上。\n通过二级索引一般需要回表查询，但是如果是二级索引是覆盖索引，也就是包含了所有需要的字段，那么就可以不回表了。\n联合索引并不是多个索引，而是一个索引结构，按书写顺序来的。\nMVCC 原理 MVCC 依赖 undolog + readView + 数据库隐藏字段\nreadView 是开始读时候的快照，维护了开始时刻活跃事务 id\n可见性就是依赖这个活跃事务 idList，如果当前事务（readView 中的相关事务 id） id 比最小的还小，那么可见，比最大的还大，那么不可见，如果在之间，那么就看在不在列表里，如果在，那么就不可见，不在就可见。\n如果不可见，那么就会去找 undolog 中的版本链，找到合适的事务 id。比较方法应该还是如上所述。\nExplain/ DESC 优化查询 extra 中 filesort 并不是文件排序，加了 order by 之后一般都有。\n锁 行锁和表锁\n行锁 -》 锁种类：排他锁 （X 锁），共享锁（S 锁）\n排他锁中又有：Gap 锁，Record lock, next-key lock(就是 record lock 和 gap lock 的组合)\n表锁 -》 意向锁\n","description":"","tags":null,"title":"Mysql基础知识学习","uri":"/posts/mysql/"},{"categories":null,"content":"背景 用户反馈系统运行一段时间后就出现了如下错误。\njava.lang.RuntimeException: java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached 用户执行了几次查询后，就发现线程疯狂飙升，而且jvm 内存释放后，线程却没释放，内存还是不够。\n这说明，jvm 虽然内存不高，但是由于线程太多，java 线程和操作系统线程有对应关系，相当于操作系统线程占了pod 内的内存，造成内存不够。\n开始我以为是监控不准，后来进入到 pod 后，发现 jvm 内存确实不高。那么就是线程泄漏。\n这是用户的 stack 信息，发现 ss-0,ss-1,ss-2 线程个有 400 +\n\"ss-0\" #721 daemon prio=5 os_prio=0 cpu=57.90ms elapsed=155563.07s tid=0x00007fc79a22dbd0 nid=0x316 waiting on condition [0x00007fc5329c8000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@17.0.2/Native Method) - parking to wait for \u003c0x000000040492a820\u003e (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(java.base@17.0.2/LockSupport.java:341) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(java.base@17.0.2/AbstractQueuedSynchronizer.java:506) at java.util.concurrent.ForkJoinPool.unmanagedBlock(java.base@17.0.2/ForkJoinPool.java:3463) at java.util.concurrent.ForkJoinPool.managedBlock(java.base@17.0.2/ForkJoinPool.java:3434) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@17.0.2/AbstractQueuedSynchronizer.java:1623) at java.util.concurrent.LinkedBlockingQueue.take(java.base@17.0.2/LinkedBlockingQueue.java:435) at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@17.0.2/ThreadPoolExecutor.java:1062) at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@17.0.2/ThreadPoolExecutor.java:1122) at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@17.0.2/ThreadPoolExecutor.java:635) at java.lang.Thread.run(java.base@17.0.2/Thread.java:833) Locked ownable synchronizers: - None \"ss-1\" #722 daemon prio=5 os_prio=0 cpu=42.84ms elapsed=155563.07s tid=0x00007fc79a22ff30 nid=0x317 waiting on condition [0x00007fc5328c7000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@17.0.2/Native Method) - parking to wait for \u003c0x000000040492a820\u003e (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(java.base@17.0.2/LockSupport.java:341) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(java.base@17.0.2/AbstractQueuedSynchronizer.java:506) at java.util.concurrent.ForkJoinPool.unmanagedBlock(java.base@17.0.2/ForkJoinPool.java:3463) at java.util.concurrent.ForkJoinPool.managedBlock(java.base@17.0.2/ForkJoinPool.java:3434) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@17.0.2/AbstractQueuedSynchronizer.java:1623) at java.util.concurrent.LinkedBlockingQueue.take(java.base@17.0.2/LinkedBlockingQueue.java:435) at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@17.0.2/ThreadPoolExecutor.java:1062) at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@17.0.2/ThreadPoolExecutor.java:1122) at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@17.0.2/ThreadPoolExecutor.java:635) at java.lang.Thread.run(java.base@17.0.2/Thread.java:833) Locked ownable synchronizers: - None \"ss-2\" #723 daemon prio=5 os_prio=0 cpu=47.49ms elapsed=155563.07s tid=0x00007fc79a230ea0 nid=0x318 waiting on condition [0x00007fc5327c6000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@17.0.2/Native Method) - parking to wait for \u003c0x000000040492a820\u003e (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(java.base@17.0.2/LockSupport.java:341) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(java.base@17.0.2/AbstractQueuedSynchronizer.java:506) at java.util.concurrent.ForkJoinPool.unmanagedBlock(java.base@17.0.2/ForkJoinPool.java:3463) at java.util.concurrent.ForkJoinPool.managedBlock(java.base@17.0.2/ForkJoinPool.java:3434) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@17.0.2/AbstractQueuedSynchronizer.java:1623) at java.util.concurrent.LinkedBlockingQueue.take(java.base@17.0.2/LinkedBlockingQueue.java:435) at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@17.0.2/ThreadPoolExecutor.java:1062) at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@17.0.2/ThreadPoolExecutor.java:1122) at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@17.0.2/ThreadPoolExecutor.java:635) at java.lang.Thread.run(java.base@17.0.2/Thread.java:833) Locked ownable synchronizers: - None 其实这里就可以推测出来是创建了 400 + 个线程池，但是没有回收。\n所以经过一番排查后，终于找到了元凶，就是线程池创建后，没有关闭。。并且由于线程池是局部变量，所以被不停的调用，但一直不释放。\n这里开始是想着会不会是程序创建了很多线程池后，没有办法创建回收的线程池的线程了。 结果后面惊喜发现有个局部变量的线程池，没回收。哈哈。\n","description":"","tags":null,"title":"java.lang.OutOfMemoryError: unable to create native thread","uri":"/posts/thread_leak/"},{"categories":null,"content":"今天同事用局域网内闲置电脑搞了个远程编译环境。本地脚本如下，效果很好，编译再也不用卡了\n#!/usr/bin/env zsh usage() { echo \"Usage: ./rb.sh [-t] [-rf \u003cmodel\u003e]\" 1\u003e\u00262; exit 1; } arg=\"-Dmaven.javadoc.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true -Drat.skip=true -Djacoco.skip=true -DskipITs -DskipTests -Prelease\" while getopts \"tr:\" o; do case \"${o}\" in t) arg=\"-Drat.skip=true -Dcheckstyle.skip=false -T1\" ;; r) rf=\"-rf ${OPTARG}\" ;; *) usage ;; esac done shift $((OPTIND-1)) rsync -azh --progress --delete --exclude={'**/target','.git','.idea'} \"$(pwd)\" chuxin@llt-mbp.local:~/dev rsync -azh --progress ~/.m2/ chuxin@llt-mbp.local:~/.m2/ # shellcheck disable=SC2087 ssh -tt -i /Users/chenchuxin/.ssh/id_rsa chuxin@llt-mbp.local \u003c\u003c EOF cd ~/dev/${PWD##*/} ./mvnw clean install $arg $rf exit 0 EOF rsync -azh --progress --include='**/target' chuxin@llt-mbp.local:~/dev/\"${PWD##*/}\"/ \"$(pwd)\" rsync -azh --progress chuxin@llt-mbp.local:~/.m2/ ~/.m2 原理其实也很简单，就是使用 rsync 来同步，先将依赖同步过去，编译完后再将文件同步回来\n","description":"","tags":null,"title":"远程编译","uri":"/posts/remote_mvn_clean/"},{"categories":null,"content":"线程池 线程池构造方法 Executors 类可以方便构造常用的线程池。\n底层真正调用的就两个\nThreadPoolExecutor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u003cRunnable\u003e workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize \u003c 0 || maximumPoolSize \u003c= 0 || maximumPoolSize \u003c corePoolSize || keepAliveTime \u003c 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } ForkJoinPool\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 public ForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, UncaughtExceptionHandler handler, boolean asyncMode, int corePoolSize, int maximumPoolSize, int minimumRunnable, Predicate\u003c? super ForkJoinPool\u003e saturate, long keepAliveTime, TimeUnit unit) { checkPermission(); int p = parallelism; if (p \u003c= 0 || p \u003e MAX_CAP || p \u003e maximumPoolSize || keepAliveTime \u003c= 0L) throw new IllegalArgumentException(); if (factory == null || unit == null) throw new NullPointerException(); this.factory = factory; this.ueh = handler; this.saturate = saturate; this.keepAlive = Math.max(unit.toMillis(keepAliveTime), TIMEOUT_SLOP); int size = 1 \u003c\u003c (33 - Integer.numberOfLeadingZeros(p - 1)); int corep = Math.min(Math.max(corePoolSize, p), MAX_CAP); int maxSpares = Math.min(maximumPoolSize, MAX_CAP) - p; int minAvail = Math.min(Math.max(minimumRunnable, 0), MAX_CAP); this.bounds = ((minAvail - p) \u0026 SMASK) | (maxSpares \u003c\u003c SWIDTH); this.mode = p | (asyncMode ? FIFO : 0); this.ctl = ((((long)(-corep) \u003c\u003c TC_SHIFT) \u0026 TC_MASK) | (((long)(-p) \u003c\u003c RC_SHIFT) \u0026 RC_MASK)); this.registrationLock = new ReentrantLock(); this.queues = new WorkQueue[size]; String pid = Integer.toString(getAndAddPoolIds(1) + 1); this.workerNamePrefix = \"ForkJoinPool-\" + pid + \"-worker-\"; } 特殊的队列 SynchronousQuery 实现分析 关于 BlockingQueue workQueue\n其中 fixedThreadPool 使用了 SynchronousQueue\n该队列其实没有容量，有线程取，那么才能允许线程放入，相当于放入会阻塞，如果没有线程取的话。总之可以提高效率。内部使用了 TransferStack 和 transferQueue 两种。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 /** * @author 一灯架构 * @apiNote SynchronousQueue示例 **/ public class SynchronousQueueDemo { public static void main(String[] args) throws InterruptedException { // 1. 创建SynchronousQueue队列 SynchronousQueue\u003cInteger\u003e synchronousQueue = new SynchronousQueue\u003c\u003e(); // 2. 启动一个线程，往队列中放1个元素 new Thread(() -\u003e { try { synchronousQueue.put(0); System.out.println(Thread.currentThread().getName() + \" 入队列 0\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); // 3. 等待1000毫秒 Thread.sleep(5000L); // 4. 启动一个线程，往队列中放1个元素 new Thread(() -\u003e { try { synchronousQueue.put(1); System.out.println(Thread.currentThread().getName() + \" 入队列 1\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); // 5. 等待1000毫秒 Thread.sleep(5000L); // 6. 再启动一个线程，从队列中取出1个元素 new Thread(() -\u003e { try { System.out.println(Thread.currentThread().getName() + \" 出队列 \" + synchronousQueue.take()); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); // 7. 等待1000毫秒 Thread.sleep(5000L); // 8. 再启动一个线程，从队列中取出1个元素 new Thread(() -\u003e { try { System.out.println(Thread.currentThread().getName() + \" 出队列 \" + synchronousQueue.take()); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } } 内部队列实际的使用，实话比较难懂。大概就是线程挂起和唤醒，其它也没看出来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 /** * Puts or takes an item. */ @SuppressWarnings(\"unchecked\") E transfer(E e, boolean timed, long nanos) { /* * Basic algorithm is to loop trying one of three actions: * * 1. If apparently empty or already containing nodes of same * mode, try to push node on stack and wait for a match, * returning it, or null if cancelled. * * 2. If apparently containing node of complementary mode, * try to push a fulfilling node on to stack, match * with corresponding waiting node, pop both from * stack, and return matched item. The matching or * unlinking might not actually be necessary because of * other threads performing action 3: * * 3. If top of stack already holds another fulfilling node, * help it out by doing its match and/or pop * operations, and then continue. The code for helping * is essentially the same as for fulfilling, except * that it doesn't return the item. */ SNode s = null; // constructed/reused as needed int mode = (e == null) ? REQUEST : DATA; for (;;) { SNode h = head; if (h == null || h.mode == mode) { // empty or same-mode if (timed \u0026\u0026 nanos \u003c= 0L) { // can't wait if (h != null \u0026\u0026 h.isCancelled()) casHead(h, h.next); // pop cancelled node else return null; } else if (casHead(h, s = snode(s, e, h, mode))) { long deadline = timed ? System.nanoTime() + nanos : 0L; Thread w = Thread.currentThread(); int stat = -1; // -1: may yield, +1: park, else 0 SNode m; // await fulfill or cancel while ((m = s.match) == null) { if ((timed \u0026\u0026 (nanos = deadline - System.nanoTime()) \u003c= 0) || w.isInterrupted()) { if (s.tryCancel()) { clean(s); // wait cancelled return null; } } else if ((m = s.match) != null) { break; // recheck } else if (stat \u003c= 0) { if (stat \u003c 0 \u0026\u0026 h == null \u0026\u0026 head == s) { stat = 0; // yield once if was empty Thread.yield(); } else { stat = 1; s.waiter = w; // enable signal } } else if (!timed) { LockSupport.setCurrentBlocker(this); try { ForkJoinPool.managedBlock(s); } catch (InterruptedException cannotHappen) { } LockSupport.setCurrentBlocker(null); } else if (nanos \u003e SPIN_FOR_TIMEOUT_THRESHOLD) LockSupport.parkNanos(this, nanos); } if (stat == 1) s.forgetWaiter(); Object result = (mode == REQUEST) ? m.item : s.item; if (h != null \u0026\u0026 h.next == s) casHead(h, s.next); // help fulfiller return (E) result; } } else if (!isFulfilling(h.mode)) { // try to fulfill if (h.isCancelled()) // already cancelled casHead(h, h.next); // pop and retry else if (casHead(h, s=snode(s, e, h, FULFILLING|mode))) { for (;;) { // loop until matched or waiters disappear SNode m = s.next; // m is s's match if (m == null) { // all waiters are gone casHead(s, null); // pop fulfill node s = null; // use new node next time break; // restart main loop } SNode mn = m.next; if (m.tryMatch(s)) { casHead(s, mn); // pop both s and m return (E) ((mode == REQUEST) ? m.item : s.item); } else // lost match s.casNext(m, mn); // help unlink } } } else { // help a fulfiller SNode m = h.next; // m is h's match if (m == null) // waiter is gone casHead(h, null); // pop fulfilling node else { SNode mn = m.next; if (m.tryMatch(h)) // help match casHead(h, mn); // pop both h and m else // lost match h.casNext(m, mn); // help unlink } } } } 为什么将任务添加到队列后，能够持续读取任务并执行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // ctl 中既存了worker 数量，又存了线程池状态 int c = ctl.get(); // 小于核心线程数 if (workerCountOf(c) \u003c corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 添加到队列中 if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) \u0026\u0026 remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 尝试增加工作线程 else if (!addWorker(command, false)) reject(command); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 /** * Checks if a new worker can be added with respect to current * pool state and the given bound (either core or maximum). If so, * the worker count is adjusted accordingly, and, if possible, a * new worker is created and started, running firstTask as its * first task. This method returns false if the pool is stopped or * eligible to shut down. It also returns false if the thread * factory fails to create a thread when asked. If the thread * creation fails, either due to the thread factory returning * null, or due to an exception (typically OutOfMemoryError in * Thread.start()), we roll back cleanly. * * @param firstTask the task the new thread should run first (or * null if none). Workers are created with an initial first task * (in method execute()) to bypass queuing when there are fewer * than corePoolSize threads (in which case we always start one), * or when the queue is full (in which case we must bypass queue). * Initially idle threads are usually created via * prestartCoreThread or to replace other dying workers. * * @param core if true use corePoolSize as bound, else * maximumPoolSize. (A boolean indicator is used here rather than a * value to ensure reads of fresh values after checking other pool * state). * @return true if successful */ private boolean addWorker(Runnable firstTask, boolean core) { retry: for (int c = ctl.get();;) { // Check if queue empty only if necessary. if (runStateAtLeast(c, SHUTDOWN) \u0026\u0026 (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) return false; // CAS 来增加线程数 for (;;) { if (workerCountOf(c) \u003e= ((core ? corePoolSize : maximumPoolSize) \u0026 COUNT_MASK)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateAtLeast(c, SHUTDOWN)) continue retry; // else CAS failed due to workerCount change; retry inner loop } } // 添加线程 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); if (isRunning(c) || (runStateLessThan(c, STOP) \u0026\u0026 firstTask == null)) { if (t.getState() != Thread.State.NEW) throw new IllegalThreadStateException(); workers.add(w); workerAdded = true; int s = workers.size(); if (s \u003e largestPoolSize) largestPoolSize = s; } } finally { mainLock.unlock(); } // 开启线程 if (workerAdded) { t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 这个方法解释了为什么只要添加到队列中，就会一直有线程处理任务，他是 loop 获取的，并且 die 之后会补充。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try { while (task != null || (task = getTask()) != null) { w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() \u0026\u0026 runStateAtLeast(ctl.get(), STOP))) \u0026\u0026 !wt.isInterrupted()) wt.interrupt(); try { beforeExecute(wt, task); try { task.run(); afterExecute(task, null); } catch (Throwable ex) { afterExecute(task, ex); throw ex; } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } } 线程池是如何优雅退出的 提供了 shutdown 和 shutdownNow 两种方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void shutdown() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { checkShutdownAccess(); advanceRunState(SHUTDOWN); interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor } finally { mainLock.unlock(); } tryTerminate(); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // 传入 false private void interruptIdleWorkers(boolean onlyOne) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { for (Worker w : workers) { Thread t = w.thread; // 获取锁的线程都调用 interrupt,如果 worker 正在运行，那么会占用该锁 if (!t.isInterrupted() \u0026\u0026 w.tryLock()) { try { t.interrupt(); } catch (SecurityException ignore) { } finally { w.unlock(); } } if (onlyOne) break; } } finally { mainLock.unlock(); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public List\u003cRunnable\u003e shutdownNow() { List\u003cRunnable\u003e tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { checkShutdownAccess(); advanceRunState(STOP); interruptWorkers(); tasks = drainQueue(); } finally { mainLock.unlock(); } tryTerminate(); return tasks; } 1 2 3 4 5 6 7 8 9 10 void interruptIfStarted() { Thread t; // worker 状态开始，未中断 if (getState() \u003e= 0 \u0026\u0026 (t = thread) != null \u0026\u0026 !t.isInterrupted()) { try { t.interrupt(); } catch (SecurityException ignore) { } } } ScheduledThreadPool 为什么能实现定时？ 1 2 3 4 5 6 public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, // 应该是它 new DelayedWorkQueue()); } 内部使用了平衡二叉树，所以先了解下平衡二叉树\n平衡二叉树是平衡因子绝对值 \u003c=1 ，并且是排序树 关于平衡二叉树的调整，找新插入方向上，不平衡的树的根节点上相邻的三个点，重新排就行。\n队列的任务则是ScheduledFutureTask，他们依据执行剩余延迟时间来构造二叉树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public \u003cV\u003e ScheduledFuture\u003cV\u003e schedule(Callable\u003cV\u003e callable, long delay, TimeUnit unit) { if (callable == null || unit == null) throw new NullPointerException(); RunnableScheduledFuture\u003cV\u003e t = decorateTask(callable, new ScheduledFutureTask\u003cV\u003e(callable, // 触发时间 triggerTime(delay, unit), // 记录进入队列的顺序 sequencer.getAndIncrement())); delayedExecute(t); return t; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public ScheduledFuture\u003c?\u003e scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) { if (command == null || unit == null) throw new NullPointerException(); if (delay \u003c= 0L) throw new IllegalArgumentException(); ScheduledFutureTask\u003cVoid\u003e sft = new ScheduledFutureTask\u003cVoid\u003e(command, null, triggerTime(initialDelay, unit), // 增加了延迟，如果有 period 那么就是周期性任务 -unit.toNanos(delay), sequencer.getAndIncrement()); RunnableScheduledFuture\u003cVoid\u003e t = decorateTask(command, sft); sft.outerTask = t; delayedExecute(t); return t; } 1 2 3 4 5 6 7 8 9 10 11 public void run() { if (!canRunInCurrentRunState(this)) cancel(false); else if (!isPeriodic()) super.run(); else if (super.runAndReset()) { setNextRunTime(); // 重复执行 reExecutePeriodic(outerTask); } } 最终还是调用 addWorker 方法\n1 2 3 4 5 6 7 void ensurePrestart() { int wc = workerCountOf(ctl.get()); if (wc \u003c corePoolSize) addWorker(null, true); else if (wc == 0) addWorker(null, false); } AQS AQS 是 AbstractQueuedSynchronizer，是个抽象类 ReentrantLock， Semaphore， SynchronousQueue 都是基于 AQS\n可以从 ReentrantLock 出发，来分析下，加锁，释放是如何跟 AQS 来交互的。 AQS 内部是使用 CLH 双向队列。\nReetrantLock lock 后如果不能立刻 CAS 获取锁，那么会调用该方法，能增加到 CLH 队列中 代码比较复杂，看不明白。。 大概是在设置 CLH 队列的节点，wait 和唤醒之类的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 /** * Main acquire method, invoked by all exported acquire methods. * * @param node null unless a reacquiring Condition * @param arg the acquire argument * @param shared true if shared mode else exclusive * @param interruptible if abort and return negative on interrupt * @param timed if true use timed waits * @param time if timed, the System.nanoTime value to timeout * @return positive if acquired, 0 if timed out, negative if interrupted */ final int acquire(Node node, int arg, boolean shared, boolean interruptible, boolean timed, long time) { Thread current = Thread.currentThread(); byte spins = 0, postSpins = 0; // retries upon unpark of first thread boolean interrupted = false, first = false; Node pred = null; // predecessor of node when enqueued /* * Repeatedly: * Check if node now first * if so, ensure head stable, else ensure valid predecessor * if node is first or not yet enqueued, try acquiring * else if node not yet created, create it * else if not yet enqueued, try once to enqueue * else if woken from park, retry (up to postSpins times) * else if WAITING status not set, set and retry * else park and clear WAITING status, and check cancellation */ for (;;) { if (!first \u0026\u0026 (pred = (node == null) ? null : node.prev) != null \u0026\u0026 !(first = (head == pred))) { if (pred.status \u003c 0) { cleanQueue(); // predecessor cancelled continue; } else if (pred.prev == null) { Thread.onSpinWait(); // ensure serialization continue; } } if (first || pred == null) { boolean acquired; try { if (shared) acquired = (tryAcquireShared(arg) \u003e= 0); else acquired = tryAcquire(arg); } catch (Throwable ex) { cancelAcquire(node, interrupted, false); throw ex; } if (acquired) { if (first) { node.prev = null; head = node; pred.next = null; node.waiter = null; if (shared) signalNextIfShared(node); if (interrupted) current.interrupt(); } return 1; } } if (node == null) { // allocate; retry before enqueue if (shared) node = new SharedNode(); else node = new ExclusiveNode(); } else if (pred == null) { // try to enqueue node.waiter = current; Node t = tail; node.setPrevRelaxed(t); // avoid unnecessary fence if (t == null) tryInitializeHead(); else if (!casTail(t, node)) node.setPrevRelaxed(null); // back out else t.next = node; } else if (first \u0026\u0026 spins != 0) { --spins; // reduce unfairness on rewaits Thread.onSpinWait(); } else if (node.status == 0) { node.status = WAITING; // enable signal and recheck } else { long nanos; spins = postSpins = (byte)((postSpins \u003c\u003c 1) | 1); if (!timed) LockSupport.park(this); else if ((nanos = time - System.nanoTime()) \u003e 0L) LockSupport.parkNanos(this, nanos); else break; node.clearStatus(); if ((interrupted |= Thread.interrupted()) \u0026\u0026 interruptible) break; } } return cancelAcquire(node, interrupted, interruptible); } ","description":"","tags":null,"title":"读深入理解高并发编程后相关总结","uri":"/posts/read_high_concurrency_programming/"},{"categories":null,"content":"Oracle 是可以支持 中文空格的（\\u3000）。因此 shardingSphere 的解析也需要支持。\n查询 antlr4 的定义，首先我们遇到中文空格要像遇到英文空格一样跳过。\nWS : [ \\t\\r\\n\\u3000] + -\u003eskip ; 其次还需要将自定义的字符中排除 \\u3000 没有找到正确的排除方法。因此考虑在范围定义中跳过 \\u3000\nIDENTIFIER_ : [A-Za-z\\u0080-\\u2FFF\\u3001-\\uFF0B\\uFF0D-\\uFFFF]+[A-Za-z_$#0-9\\u0080-\\u2FFF\\u3001-\\uFF0B\\uFF0D-\\uFFFF]* ; 完美解决\n","description":"","tags":null,"title":"Antlr4 支持中文空格","uri":"/posts/antlr4_chinese_white/"},{"categories":null,"content":"OPE 加密算法全称是 Order Preserving Encryption。加密后的内容依然具有原始数据的顺序性，如果实现了该算法后，SQL 可以支持密文的 range 查询和排序。\nhttps://github.com/aymanmadkour/ope 该开源代码是 ope 的算法实现，可以实现数字，字符串的加密有序性。\n","description":"","tags":null,"title":"OPE 加密算法","uri":"/posts/encrypt_ope/"},{"categories":["database"],"content":"问题描述 今天遇到一个奇怪的问题，我将运行中的 statement 信息缓存到内存集合中，通过另一个线程调用 statement.cancle() 接口，发现无论如何都取消不了，需要等到 statment 运行完，才能取消，可是这个取消已经没有意义了。\n我的代码如下：\n1 2 3 4 5 6 7 for (Statement each : process.getDistributedJoinProcessStatements().values()) { if (null != each) { if (!each.isClosed()) { each.cancel(); } } } 我发现 debug 无论如何也走不下去，于是我不断点，每次卡住的时候就 dump 出结果\n\"pool-12-thread-1@5031\" prio=5 tid=0x2e nid=NA waiting for monitor entry java.lang.Thread.State: BLOCKED waiting for ShardingSphere-Command-0@6041 to release lock on \u003c0x2198\u003e (a com.mysql.jdbc.JDBC4Connection) at com.mysql.jdbc.StatementImpl.isClosed(StatementImpl.java:2487) at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.isClosed(HikariProxyPreparedStatement.java:-1) at org.apache.shardingsphere.mode.manager.cluster.coordinator.subscriber.ProcessListChangedSubscriber.killProcessListId(ProcessListChangedSubscriber.java:98) - locked \u003c0x2196\u003e (a org.apache.shardingsphere.mode.manager.cluster.coordinator.subscriber.ProcessListChangedSubscriber) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:568) at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87) at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144) - locked \u003c0x2197\u003e (a com.google.common.eventbus.Subscriber$SynchronizedSubscriber) at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72) at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30) at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:67) at com.google.common.eventbus.Dispatcher$PerThreadQueuedDispatcher.dispatch(Dispatcher.java:108) at com.google.common.eventbus.EventBus.post(EventBus.java:212) at org.apache.shardingsphere.infra.util.eventbus.EventBusContext.post(EventBusContext.java:51) at org.apache.shardingsphere.mode.manager.cluster.coordinator.registry.GovernanceWatcherFactory$$Lambda$558/0x0000000800fabe28.accept(Unknown Source:-1) at java.util.Optional.ifPresent(Optional.java:178) at org.apache.shardingsphere.mode.manager.cluster.coordinator.registry.GovernanceWatcherFactory.lambda$watch$0(GovernanceWatcherFactory.java:55) at org.apache.shardingsphere.mode.manager.cluster.coordinator.registry.GovernanceWatcherFactory$$Lambda$504/0x0000000800f81bc0.onChange(Unknown Source:-1) at org.apache.shardingsphere.mode.repository.cluster.zookeeper.ZookeeperRepository.lambda$watch$0(ZookeeperRepository.java:368) at org.apache.shardingsphere.mode.repository.cluster.zookeeper.ZookeeperRepository$$Lambda$510/0x0000000800f85b90.childEvent(Unknown Source:-1) at org.apache.curator.framework.recipes.cache.TreeCacheListenerWrapper.sendEvent(TreeCacheListenerWrapper.java:71) at org.apache.curator.framework.recipes.cache.TreeCacheListenerWrapper.event(TreeCacheListenerWrapper.java:42) at org.apache.curator.framework.recipes.cache.CuratorCacheListenerBuilderImpl$2.lambda$event$0(CuratorCacheListenerBuilderImpl.java:149) at org.apache.curator.framework.recipes.cache.CuratorCacheListenerBuilderImpl$2$$Lambda$557/0x0000000800fab580.accept(Unknown Source:-1) at java.util.ArrayList.forEach(ArrayList.java:1511) at org.apache.curator.framework.recipes.cache.CuratorCacheListenerBuilderImpl$2.event(CuratorCacheListenerBuilderImpl.java:149) at org.apache.curator.framework.recipes.cache.CuratorCacheImpl.lambda$putStorage$6(CuratorCacheImpl.java:287) at org.apache.curator.framework.recipes.cache.CuratorCacheImpl$$Lambda$522/0x0000000800f8f1c0.accept(Unknown Source:-1) at org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92) at org.apache.curator.framework.listen.MappingListenerManager$$Lambda$85/0x0000000800d0a120.run(Unknown Source:-1) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.lang.Thread.run(Thread.java:833) 原来是被 mysql 连接对象锁住了。 debug 时，idea 会调用 toString 也会上锁，所以卡住。 因此直接调用 cancle 不判断就没有这个问题了。\n1 2 3 for (Statement each : process.getDistributedJoinProcessStatements().values()) { each.cancel(); } 另外 cancle 内部其实是创建一个新连接，然后发送 kill 语句来实现的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public void cancel() throws SQLException { if (!this.statementExecuting.get()) { return; } if (!this.isClosed \u0026\u0026 this.connection != null \u0026\u0026 this.connection.versionMeetsMinimum(5, 0, 0)) { Connection cancelConn = null; java.sql.Statement cancelStmt = null; try { cancelConn = this.connection.duplicate(); cancelStmt = cancelConn.createStatement(); cancelStmt.execute(\"KILL QUERY \" + this.connection.getIO().getThreadId()); this.wasCancelled = true; } finally { if (cancelStmt != null) { cancelStmt.close(); } if (cancelConn != null) { cancelConn.close(); } } } } ","description":"","tags":null,"title":"jdbc Statement 无法取消的问题","uri":"/posts/statement_cancle/"},{"categories":null,"content":"项目地址： https://github.com/brettwooldridge/HikariCP\n“Simplicity is prerequisite for reliability.” - Edsger Dijkstra\n最先强调的就是简单，扁平，性能，做了 字节码级别的优化。 配置简单，很多东西选择不做，例如 statement 缓存。\npool size 比想象的要小的多，这样更能拥有更好的性能。因为单 cpu 并非并行，上下文切换需要时间，另外查询的位置需要来回切换等等。 不过 IO wait 比较大时，那么并发时能够提升的。 另外对于大事务和短事务，比较难以协调，这时候最好搞两个 pool。\nHikariCP 怎么学习 考虑编译起来，然后运行 test 查看代码逻辑\ntestSealed1 通过 IDEA 运行时发现总会报 Error occurred during initialization of boot layer FindException: Module not found 后来发现 mvn clean 之后，可以运行，但是有部分代码是会在编译阶段被更改，因此还得 install 之后在跑 但是 idea 运行之前会 build 然后就改变了 target 下部分代码。 所以需要勾选运行前不 build 即可解决问题。\ntestSealed2\ntestSealed3\ntestSealedAccessibleMethods\n创建 hikari datasoure 的时候会设置 sealed, 如果已经 sealed 了，那么后续就不允许改了。\ntestIdleTimeout 看一下 idle 为什么被释放。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Test public void testIdleTimeout() throws InterruptedException, SQLException { HikariConfig config = newHikariConfig(); config.setMinimumIdle(5); config.setMaximumPoolSize(10); config.setConnectionTestQuery(\"SELECT 1\"); config.setDataSourceClassName(\"org.h2.jdbcx.JdbcDataSource\"); config.addDataSourceProperty(\"url\", \"jdbc:h2:mem:test;DB_CLOSE_DELAY=-1\"); System.setProperty(\"com.zaxxer.hikari.housekeeping.periodMs\", \"1000\"); try (HikariDataSource ds = new HikariDataSource(config)) { getUnsealedConfig(ds).setIdleTimeout(3000); System.clearProperty(\"com.zaxxer.hikari.housekeeping.periodMs\"); SECONDS.sleep(1); HikariPool pool = getPool(ds); assertEquals(\"Total connections not as expected\", 5, pool.getTotalConnections()); assertEquals(\"Idle connections not as expected\", 5, pool.getIdleConnections()); try (Connection connection = ds.getConnection()) { Assert.assertNotNull(connection); MILLISECONDS.sleep(1500); // 获取了一个 connection, 从 connectionBag 中借了一个，由于最小的空闲线程数是 5，因此又自动补充了一个 assertEquals(\"Second total connections not as expected\", 6, pool.getTotalConnections()); assertEquals(\"Second idle connections not as expected\", 5, pool.getIdleConnections()); } // 还没超过设置的 3000 ms,所以总空闲连接就变成了 6 个。 assertEquals(\"Idle connections not as expected\", 6, pool.getIdleConnections()); MILLISECONDS.sleep(3000); // 超时释放了 assertEquals(\"Third total connections not as expected\", 5, pool.getTotalConnections()); assertEquals(\"Third idle connections not as expected\", 5, pool.getIdleConnections()); } } 看一下是谁补充了 connectionBag.\n1 this.houseKeeperTask = houseKeepingExecutorService.scheduleWithFixedDelay(new HouseKeeper(), 100L, housekeepingPeriodMs, MILLISECONDS); debug 发现是 houseKeeperTask ,定时任务，探测到最小空闲连接已经小于 4 了，因此又补充了。\n那么又是谁释放了呢？\n依然是 houseKeeperTask 发现空闲连接已经大于最小空闲连接配置，并且时间已经超过 配置的 3000 ms,所以就释放了。\n1 2 3 4 5 6 7 8 9 10 11 12 if (idleTimeout \u003e 0L \u0026\u0026 config.getMinimumIdle() \u003c config.getMaximumPoolSize()) { logPoolState(\"Before cleanup \"); final var notInUse = connectionBag.values(STATE_NOT_IN_USE); var maxToRemove = notInUse.size() - config.getMinimumIdle(); for (PoolEntry entry : notInUse) { if (maxToRemove \u003e 0 \u0026\u0026 elapsedMillis(entry.lastAccessed, now) \u003e idleTimeout \u0026\u0026 connectionBag.reserve(entry)) { closeConnection(entry, \"(connection has passed idleTimeout)\"); maxToRemove--; } } logPoolState(\"After cleanup \"); } testIdleTimeout2 testConcurrentClose testPoolSizeAboutSameSizeAsThreadCount 测试 poolSize 的大小基本上跟线程数差不多\ntestSlowConnectionTimeBurstyWork 测试快速的任务处理下，连接创建比较慢，但是实际使用的connection 很少\ntestRaceCondition 测试30s,不停创建连接和驱逐连接\ntestAutoCommit 测试设置 autoCommit 为false ,关闭连接后，代理的原真实连接还是会恢复成原状\ntestTransactionIsolation 测试设置 isolation，关闭连接后，恢复（dirtyBits 通过位运算来提高性能）\ntestIsolation 看起来就是测试设置 isolation 成功\ntestReadOnly 测试设置 readOnly, 关闭连接后，恢复\ntestCatalog 测试设置 catalog, 关闭连接后，恢复\ntestCommitTracking 测试 isCommitStateDirty 在 autoCommit = false 时，有 SQL 执行时，就变成 true, rollback 或者 commit 就恢复\ntestException1 测试异常之后，能够自动检查 exception 并且关闭连接\ntestUseAfterStatementClose 测试关闭 statement 之后，不允许再使用 statement\ntestUseAfterClose 测试关闭连接后使用\ntestLastErrorTimeout 测试获取 connection 时，isConnectionDead 不会主动抛出异常\n","description":"","tags":null,"title":"探索 HikariCP","uri":"/posts/hikaricp/"},{"categories":["Tech"],"content":"最开始是通过官方网站上这本书来学习 https://doc.rust-lang.org/book/\n通读之后开始学习 https://github.com/rust-lang/rustlings/\n中间参与了一个开源项目 https://github.com/database-mesh/pisanix/tree/master\n最近开始学习 https://exercism.org/tracks/rust 最近 rust 学习又有了一些新突破\n从零手写文本编辑器 https://www.flenker.blog/hecto/ 这个教程里分成了多个 commit 带着你一步步的完成编辑器，看着自己的代码逐步变成可以使用的文本编辑器，真的很不错哦。\n","description":"","tags":null,"title":"rust 学习","uri":"/posts/learn_rust/"},{"categories":null,"content":"Connection is not available, request timed out after 30000ms 最近碰到一个这个异常，前端是 xxl-job 任务，后端将 shardingsphere-proxy 作为数据源来使用。\n偶发，频率比较高 网络上相关解释 从 Hikari 拿连接，但是拿不到，要不就是达到最大连接数了 https://stackoverflow.com/questions/32968530/hikaricp-connection-is-not-available\nMysql 连接数上看，两台实例 205， 206 都是没有超过 1000 最大连接数 20000\n这个是业务的报错，想拿 connection 拿不到，不确定是 proxy 报的还是业务报的， proxy 无异常日志 根据日志找到了业务的库配置\nspring.datasource.dynamic.datasource.sharding0.driver-class-name = com.mysql.jdbc.Driver spring.datasource.dynamic.datasource.sharding0.druid.initial-size = 5 spring.datasource.dynamic.datasource.sharding0.druid.max-active = 100 spring.datasource.dynamic.datasource.sharding0.druid.max-wait = 60000 spring.datasource.dynamic.datasource.sharding0.druid.min-idle = 5 spring.datasource.dynamic.datasource.sharding0.druid.validation-query = select 1 spring.datasource.dynamic.datasource.sharding0.password = root spring.datasource.dynamic.datasource.sharding0.url = jdbc:mysql://xxx?useUnicode=true\u0026characterEncoding=utf-8\u0026serverTimezone=GMT%2B8\u0026useSSL=false\u0026allowPublicKeyRetrieval=true spring.datasource.dynamic.datasource.sharding0.username = root 奇怪的是为什么是 druid 连接池，但是却没有 druid 相关信息?\n客户反馈 druid 的链接信息没有生效，实际使用的是 hikari，并且应该是采用了默认配置，那也就是最大连接数是 10\n查实修改下连接池配置\nspring.datasource.dynamic.datasource.sharding0.hikari.minimum-idle = 5 spring.datasource.dynamic.datasource.sharding0.hikari.maximum-pool-size = 100 spring.datasource.dynamic.datasource.sharding0.hikari.connection-timeout = 40000 发现报错信息变成了 sharding0 - Connection is not available, request timed out after 40000ms.\n说明参数生效了，另外也说明了的确是用户业务 hikari 连接池 报出的异常信息\n难道 100 的连接池还不够，试试 500 ，结果还是不行，难道是客户业务代码有 bug？\n还是先证明不是 proxy 的问题吧，考虑监控下发送到 proxy 的连接，看看究竟有多少\nProxy 的外部连接数，从停止服务的 2 ,启动服务后增长到 52, 启动 job 一直到 job 失败，连接数都是 52\n# 统计连接到 3307 端口的外部链接数量 netstat -n | grep 3307 | wc -l 既然确定是业务的问题，那么就从 业务 hikari 连接池找原因吧，首先打开 hikari 的 debug 日志\n# 放到 application.properties 中 logging.level.com.zaxxer.hikari.HikariConfig = DEBUG logging.level.com.zaxxer.hikari = TRACE 发现连接池参数没生效，还是 10 难道是写法错误？\n尝试修改配置项\nDynamic datasource 的文档居然要收费。。。\nhttps://www.kancloud.cn/tracy5546/dynamic-datasource/2270657\n交钱是不可能交钱的。\n下载源码后，发现有 spring 的相关提示\n原来 dynamic datasource 配置的 hikari 最大连接数是。\n我怀疑是为了赚钱，故意的。。\nspring.datasource.dynamic.datasource.sharding0.hikari.min-idle = 5 spring.datasource.dynamic.datasource.sharding0.hikari.max-pool-size = 30 问题解决。\n","description":"","tags":null,"title":"Connection is not avaible","uri":"/posts/connection_is_not_avaible/"},{"categories":null,"content":"通过如下步骤打包后发现 jar 包中的文件都是 .java 文件，而不是 .class 文件 经过了一下午的尝试，发现手动创建包，然后指定 .class 文件可以成功打包。 不过没有特别需求还是直接用 maven 吧，简单些，无需添加任何插件，直接 mvn clean install 即可。\n","description":"","tags":null,"title":"使用idea创建的 jar 文件都是 .java 而不是 .class","uri":"/posts/use_idea_to_generate_jar/"},{"categories":null,"content":"今天写代码时遇到一个异常。如下\njava.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@4b76a937 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:869) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:865) at com.mysql.jdbc.MysqlIO.checkForOutstandingStreamingData(MysqlIO.java:3172) 我推测可能是结果集没有关闭，然后想要继续创建结果集。 发现跟之前的代码唯一的区别如下\n1 2 3 4 5 // 之前 while(resultset.next()) // 之后 if(resultset.next()) 因为只有一个结果集，所以我考虑直接用下面的方式，结果就报错了。\ndebug mysql-jdbc 之后，发现 resultSet.next() 方法如果没有更多结果集之后，会关闭流式查询结果集。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // RowDataDynamic.java public ResultSetRow next() throws SQLException { this.nextRecord(); if (this.nextRow == null \u0026\u0026 !this.streamerClosed \u0026\u0026 !this.moreResultsExisted) { // 关闭流 this.io.closeStreamer(this); this.streamerClosed = true; } if (this.nextRow != null \u0026\u0026 this.index != Integer.MAX_VALUE) { ++this.index; } return this.nextRow; } ","description":"","tags":null,"title":"如果 result set 仅有一个结果集，是否可以用 if(resultset.next()) ?","uri":"/posts/resultset_next/"},{"categories":null,"content":"背景 在优化全局二级索引的过程中，有一个优化点是如果二级索引的数据量太大，那么带回主表再查可能引起一些问题，例如效率不高，OOM 或者 SQL 超长等问题，因此考虑二级索引表数据量大的时候，直接查询主表。 因此需要查询二级索引表的数据量。 这里考虑能否直接查询二级索引表的同时获取本次查询结果的数据量。\n实践 方式一 通过 ResultSet.TYPE_SCROLL_INSENSITIVE 可以将游标放到最后，然后根据 getRow() 方法获取\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public static void main(String[] args) throws ClassNotFoundException, SQLException { Class.forName(\"com.mysql.jdbc.Driver\"); Connection connection = DriverManager.getConnection(\"jdbc:mysql://127.0.0.1:3306/demo_ds_0?serverTimezone=UTC\u0026useSSL=false\u0026allowPublicKeyRetrieval=true\", \"root\", \"123456\"); // 设置游标可以滚动，默认是 forward only try (PreparedStatement preparedStatement = connection.prepareStatement(\"select * from t_single\", ResultSet.TYPE_SCROLL_INSENSITIVE, ResultSet.CONCUR_READ_ONLY); ResultSet resultSet = preparedStatement.executeQuery()) { int count = 0; // 移动到最后 if (resultSet.last()) { // 获取最后一行的行号 count = resultSet.getRow(); } System.out.println(\"total:\" + count); resultSet.beforeFirst(); int columnIndex = 1; while (resultSet.next()) { System.out.println(resultSet.getObject(columnIndex ++)); } } } 但是为什么之前的很多框架都采用的 count sql，而不用这种方式呢？例如 pageHelper 等 https://cloud.tencent.com/developer/article/1433806 搜索到相关文章，发现 TYPE_SCROLL_INSENSITIVE 会将结果集缓存起来，容易 OOM。那么这种方式可能问题就比较大了。 如果二级索引表查询出的数据量比较大，缓存结果集的风险就比较大了。\nhttps://forums.mysql.com/read.php?39,435275,435275 该主题也在说使用这种resultSet type 会导致性能很差\nHow many rows are we talking about here? In general, it’s -never- a good idea to select more rows than you need, and use fetch hints to scroll the result set. Pretty much every database now has options like MySQL’s LIMIT clause that can (and often will) involve the optimizer to make it so your query examines as few rows as possible.\nThe MySQL protocol itself doesn’t have the notion of fetch batch sizes, and the driver is optimized for OLTP type queries. Leaving large result sets open while a user paginates is very, very sub-optimal because of the locks and other resources that are needlessly taken.\nYou may want to consider re-thinking your overall strategy, because it’s more than likely consuming more resources than you think, even on databases other than MySQL.\n-Mark\nMark Matthews Consulting Member Technical Staff - MySQL Enterprise Tools Oracle http://www.mysql.com/products/enterprise/monitor.html\n看官方的回复也不建议在大数据中使用滚动游标的方式。\n结论 可以在查询结果的同时获取 count，但是性能可能不高，也有占用过多资源的可能。\n","description":"","tags":null,"title":"是否可以在 jdbc 查询数据的同时获取本次查询的数据量？","uri":"/posts/result_set_type/"},{"categories":null,"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 create table ENCRYPT.TEST1 ( A VARCHAR2(200) PRIMARY KEY, B VARCHAR2(200), C VARCHAR2(200) ) / create table ENCRYPT.TEST2 ( A VARCHAR2(200) PRIMARY KEY, B VARCHAR2(200), C VARCHAR2(200) ) / insert into TEST1 values (1,2,3); insert into TEST1 values (2,3,4); insert into TEST2 values (4,2,3); merge into TEST2 fi using TEST1 se on (fi.A = se.A and fi.A = 3) when matched then update set fi.B = se.B, fi.C = se.C when not matched then insert (A, B, C) values (1,2,3); 执行如上 SQL 发现一直报 ORA-00001: unique constraint (ENCRYPT.SYS_C007033) violated\n原来是因为我理解错了 merge insert 的含义。 这是将 TEST2 的数据痛 TEST1 进行比对，如果 TEST1 中的数据满足 ON 的条件，那么会进行 update，否则会进行 insert. 因为 TEST1 中不符合条件的有两条，所以会依次执行 insert (1,2,3) 所以就冲突了。（相当于 insert 执行了两次）\n","description":"","tags":null,"title":"Oracle merge insert 问题","uri":"/posts/oracle_merge/"},{"categories":["Tech"],"content":"背景 用户使用 shardingsphere 发现 No suitable driver 异常。但是用户的 jar 包中是有该驱动的。\n调查 发现如果增加这段代码后，可以正常执行\n1 Class.forName(\"com.mysql.cj.jdbc.Driver\"); 正常而言，按照 JDBC4.0 标准，驱动是通过 spi 机制加载。不需要手动加载。 通过 debug DriverManager 发现其中有个驱动的名称中包含了 - 不是合法的 java 名称。 但是会将异常吞掉，所以没有展示出来。 修复名称后，可以正常使用。\n相关知识 JDBC4.0 是通过 spi 加载。 - 不是合法名称，使用 shade 插件替换名称时要注意。 DriverManager 加载多个驱动过程中，如果有任何一个失败，都不会再继续加载，并不会抛出相关异常。\n","description":"","tags":null,"title":"No suitable driver 问题调查","uri":"/posts/driverclassname/"},{"categories":null,"content":"https://github.com/julianhyde/sqlline sqlline 是一个通过 jdbc 的连接工具。\n下载代码并编译 git clone git://github.com/julianhyde/sqlline.git cd sqlline ./mvnw package 将 shardingsphere-jdbc 打包，之后最好再使用 sharde PLUGIN 打成一个 jar 包\n修改 sqlline bin 目录下的 sqlline 脚本，将相关依赖添加进去包括数据库驱动、shardingsphere shade jar、 sqlline-VERSION-jar-with-dependencies.jar\n#!/bin/bash # sqlline - Script to launch SQL shell on Unix, Linux or Mac OS BINPATH=$(dirname $0) exec java -cp \"$BINPATH/../target/*\":\"/Users/chenchuxin/Documents/sqlline-sqlline-1.12.0/bin/*\" sqlline.SqlLine \"$@\" # End sqlline 使用 sqlline sqlline -d org.apache.shardingsphere.driver.ShardingSphereDriver -u jdbc:shardingsphere:absolutepath:/Users/chenchuxin/Documents/GitHub/sw-test/src/main/resources/META-INF/oracle.yaml ","description":"","tags":null,"title":"使用 sqlline 连接 sharding_jdbc","uri":"/posts/use_sql_line/"},{"categories":["Tech"],"content":"初步了解 opendal 查看 examples 了解到 opendal 是各种数据源的通用连接器\n运行接入到 mysql 试试看 查看 good first issue 的相关 pr ","description":"","tags":null,"title":"Open_dal","uri":"/posts/open_dal/"},{"categories":["Tech"],"content":"如何才能实现分布式 join？\n有三种方式：\n将数据全部抽取到内存中，进行内存计算。 将数据全部抽取到一个数据源中，直接进行查询。 内存和存储并用。 对于分析型的 SQL 而言，不需要考虑事务和性能的话，可以采用数据挪动的方式来实现。\n如下图所示，列出了可能的实现方式。 该方案的具体实现需要考虑 2 点。\n语义分析 （考虑通过 calcite 或者自行解析提取条件） 获取表之间的依赖关系，例如 A left join B on condition，那么 B 依赖 A, 可以先将 A 抽取出来，再依据 A 来抽取 B 增加条件到表中 支持数据迁移（需要支持数据迁移的管理） ","description":"","tags":null,"title":"分布式 join 实现","uri":"/posts/distributed-join/"},{"categories":["Tech"],"content":"Target 说清楚 calcite 是什么，能做什么 画出 calcite 的大体架构 明白 calcite 是如何跟 SS 整合的 未来 SS 还可以在哪些方面进一步利用 calcite calcite 优化逻辑源码级理解 在 calcite 上提交 pr Process calcite 是什么，能做什么 calcite 是动态数据的管理框架。我理解，就是数据库的计算层，去除掉了存储的部分。 calcite 可以接入任何形式的数据，只需要我们将数据注册成表的形式，那么就可以使用 SQL 来对它进行查询。\ncalcite 的大体架构 calcite 如何跟 SS 整合 ShardingSphere 可以支持分库分表，所以也自然会有跨实例查询的 SQL。 例如跨实例的 join。这些 SQL 是没办法直接下推到数据库去执行的，因此利用 calcite 来实现复杂子查询或者是跨实例 join。 我们可以从一个跨实例 join 的 select debug 来查看整合 calcite 的方式。 1. 执行查询前，会先构造 OptimizerContext，里面包含一些数据库的配置，例如 caseSensitive -\u003e false conformance -\u003e MYSQL_5 timeZone -\u003e UTC lex -\u003e MYSQL fun -\u003e mysql 以及一些默认的规则 HepPlanner\n注册 schema 将表转化为 translatable table 创建 CalciteCatalogReader 对象（参数： rootSchema, schemaName, RelDataTypeFactory, CalciteConnectionConfig） 创建 SqlValidator 创建 SqlToRelConverter 拿到之前的 RelOptPlanner （HepPlanner） 将 shardingsphere 的 sqlstatement 转化成 SqlNode 利用 SqlToRelConverter 将 sqlNode 转化为 RelNode 然后分别进行RBO 和 CBO 优化 将优化结果与 dataContext 绑定 在 FilterableTableScanExecutor 执行下推逻辑，下推到各个逻辑库上执行。 将结果集封装到 ResultSet 中 同普通查询后续流程一致 如何规划执行？默认每个表下推一次吗？ 从目前代码的 TranslatableTableScanExecutor 来看，确实是单表的下推。\n未来 SS 还可以在哪些方面进一步利用 calcite 内存的优化，目前全部都是抽取到内存中，大数据会 OOM 自定义算子的实现 利用数据的收集做代价优化 calcite 优化逻辑源码级理解 关系代数的理解 https://blog.csdn.net/QuinnNorris/article/details/70739094 calcite 解决编译报错问题 checkout 到最近 release 的tag\n注释掉 build.gradle.kts 中的 releaseArtifacts\n","description":"","tags":null,"title":"calcite 学习","uri":"/posts/learn_calcite/"},{"categories":["Tech"],"content":"","description":"","tags":null,"title":"Learnhadoop","uri":"/posts/learnhadoop/"},{"categories":["Tech"],"content":"","description":"","tags":null,"title":"LearnSpark","uri":"/posts/learnspark/"},{"categories":null,"content":"文档信息提取 https://developer.aliyun.com/article/783392 本文首先介绍了 MySQL 的 join 实现，包括以下方式\nNested-Loop Join (NL Join) Batched Key Access Join (BKA Join) Block Nested-Loop Join（版本 \u003c 8.0.20） Hash Join (版本 \u003e= 8.0.18) polardb-x 支持多种 join 方式，包括 Lookup Join、Nested-Loop Join、Hash Join、Sort-Merge Join 等，本文主要讲的是 Lookup Join\nlookup join Lookup Join 的执行过程如下（非索引回表情形）：\n从驱动侧拉取一批数据。通常情况下数据量不会很多，如果数据较多，那么每个批的大小受到 lookup 端的分片数量以及是否可以进行分片裁剪限制。批大小的选择会直接影响查询性能，如果批特别小会导致 RPC 次数太高，批太大则会导致内存中暂存的数据量膨胀，高并发情况下可能导致 OOM。默认情况下我们尽可能让每个分片平均查询 50 个值、最多不超过 300 个值。 计算 batch 内每行数据所在分片，由于 lookup 侧是一个分区表，驱动表的每行数据要 lookup 的数据位于不同的分区中。只有包含数据的分片才需要参与 Join，如果没有任何值被路由到某个分片上，那么这个分片也无需被 Lookup。 并发请求所有需要 lookup 的分片，并将查到的数据行以 Join Key 为 Key 构建成哈希表，缓存在内存中。 类似于 Hash Join，利用哈希表为驱动侧的每行找到与其 Join 的行，取决于 Join 类型，可能 Join 出 0 行、1 行或多行。 1 2 3 4 /* Query 1 */ SELECT o_orderkey, o_custkey, c_name FROM orders JOIN customer ON o_cutkey = c_custkey WHERE o_orderkey BETWEEN 1001 AND 1005 依赖全局索引的 Join 则更为复杂一些，回忆下 MySQL 的 BKA Join，需要进行两次 lookup：\n第一次用 Join key 查询全局索引表（用于 Join） 第二次用全局索引表中的主键查询主表（用于索引回表） 将回表结果以 PK 为 key 构建哈希表，与2中的查询结果 Join，得到完整的 Join 右侧数据 将完整的 Join 右侧数据以 Join Key 为 key 构建哈希表，与 1 的数据 Join，得到最终 Join 结果 1 2 3 4 /* Query 2 */ SELECT c_name, c_custkey, o_orderkey, o_totalprice FROM customer JOIN orders ON c_cutkey = o_custkey WHERE c_custkey BETWEEN 13 AND 15 文档信息提取 https://help.aliyun.com/document_detail/316601.html\nNested-Loop Join (NLJoin) 一般是含有不等式条件场景下会使用 流程如下：\n拉取内表（右表，通常是数据量较小的一边）的全部数据，缓存到内存中。 遍历外表数据，对于外表的每行： 对于每一条缓存在内存中的内表数据。 构造结果行，并检查是否满足JOIN条件，如果满足条件则输出。 支持通过 hint 来定义顺序\nHash Join Hash Join是等值JOIN最常用的算法之一。它的原理如下所示：\n拉取内表（右表，通常是数据量较小的一边）的全部数据，写进内存中的哈希表。 遍历外表数据，对于外表的每行： 根据等值条件JOIN Key查询哈希表，取出0-N匹配的行（JOIN Key相同）。 构造结果行，并检查是否满足JOIN条件，如果满足条件则输出。 当索引无法满足，不能使用 lookup join 的时候，可以使用 hash join\nSort-Merge Join Sort-Merge Join是另一种等值JOIN算法，它依赖左右两边输入的顺序，必须按JOIN Key排序。它的原理如下：\n开始Sort-Merge Join之前，输入端必须排序（借助MergeSort或MemSort）。 比较当前左右表输入的行，并按以下方式操作，不断消费左右两边的输入： 如果左表的JOIN Key较小，则消费左表的下一条数据。 如果右表的JOIN Key较小，则消费右表的下一条数据。 如果左右表JOIN Key相等，说明获得了1条或多条匹配，检查是否满足JOIN条件并输出。 关于 Join 顺序以及各种 join 的使用场景 文档信息提取 https://zhuanlan.zhihu.com/p/379967662\n面对 OLTP 场景，可以使用大表和小表进行 join ，使用前文提到的 Lookup Join 算法 但是对于 OLAP 的场景，我们可能就需要将数据从内存拉出来进行计算。\n等值 JOIN 那么一般就是 hash-join 和 sort-merge join 两种，Hash join 根据是否支持大数据量，又分为 内存 Hash-join 和 落盘版的 Hash-join\n内存版 Hash join 的实现 根据统计信息选取较小的表根据 join 条件对给定的列 build hash table，然后流式遍历，去 hash table 里面找 key，如果一致，那么就拿出一整行，然后直接向下游输出，这个过程又叫 probe table\n//build Table for row in t1: hashValue = hash_func(row) put (hashValue, row) into hash-table; //probe Table for row in t2: hashValue = hash_func(row) t1_row = lookup from hash-table if (t1_row != null) { join(t1_row, row) } polardb-x 与一般的 hash join 最大的不同，是采用了内存友好的 vector 重新实现了哈希表，对 CPU CACHE 更友好，可以提高 JOIN 性能。 具体实现没太明白。。。大概是搞了一些缓存索引结构？\n如果数据量太大，无法全部放到内存中的时候，那么就需要使用 HybridHashJoin 算法。\nHybirdHashJoin Hybird hash join 从我的角度理解，就是将原表按照关联的键做 partition，然后分别在每个分区做 hash join. SortMergeJoin sort t1, sort t2 R1 = t1.next() R2 = t2.next() while (R1 != null \u0026\u0026 R2 != null) { if R1 joins with R2 output (R1, R2) else if R1 \u003c R2 R1 = t1.next() else R2 = t2.next() } 这种场景一般外排序会比较消耗 IO，一般 hash join.不过某些极端场景下， hash join 分桶之后，数据量还是超限，那么这种场景可能 sort merge join 就比较优势了。\nShuffle join TODO\n","description":"","tags":null,"title":"Polardb-x 的 join 实现","uri":"/posts/polardbxjoin/"},{"categories":["Tech"],"content":"分布式 join 调研 Citus Data Citus Data 是一个开源的分布式数据库管理系统，它是基于 PostgreSQL 架构之上，能够允许数据库在多个服务器之间进行分布式运算，以便应对大型数据处理和高流量负载的需求。利用 Citus Data 可以为 PostgreSQL 提供横向扩展的能力，使其可以更好地应对应用中的大量数据请求，具有较高的性能和可扩展性。Citus Data 的特点还包括良好的可用性、容错性、可管理性和可扩展性。Citrus Data 可以作为云端数据库的解决方案提供，同时也适用于在本地运行的企业数据库和分析场景中。\n分布式 join 支持方式（数据复制） 单表 join 分表\n下图介绍了 单表和 sharding 表 join 时 citus 的处理方式\n它两种方式来对 local 表和 distributed 表进行关联查询\n将 distributed 表的数据从 workers 节点移动到协调器 将 local 表数据从协调器移动到 workers 节点 两种方式的选择有多种模式可以配置，例如自动模式： 因为一般来说 distributed 表的数据量比较大，所以只有当满足以下条件时才会将数据从 distributed 移动到 local 中\n分布式表包含唯一键，如主键。 唯一键包含一个常量等式过滤器，可以直接或通过传递性实现。 具体模式如下：\nAuto (default): Distributed table will be moved to coordinator if the distributed table contains a constant equality filter on a unique column, which ensures less data movement from workers to the coordinator. If not, then the local table will be moved from coordinator to workers. Prefer-local: The filtered data of local table will be moved to the workers from the coordinator, then the JOIN will be executed on the workers. Prefer-distributed: The filtered data of the distributed table will be moved to the coordinator from the workers, then the JOIN will be executed on the coordinator. Never: Citus will give an error for local and distributed table JOINs, same as before Citus 10. 分表 join 分表\n这里展示的是表的分片数量相同，并且关联条件是分片键的场景，类似于 binding table\n该图展示了 citus 的 join 方式，有本地 join，refrence table join （就是广播表），还有 repartition joins，里面提到会使用数据混洗的方式，性能不高。另外这个方式需要开关打开。\n根据作者对于 repartition joins https://github.com/citusdata/citus/issues/2321 的回复来看，目前该功能只能用于某些场景并且不建议默认打开使用。\nVitess vitess 是可扩展的兼容 mysql 的云原生数据库。\n分布式 join 支持方式（内存计算 + 数据复制） 没有找到官方明确的说法，但是从一些公开的演讲中发现了一些信息。 它实现分布式 join 的方式也是两种，一种是在内存中计算，另外一种就是数据复制，不过它的复制是通过 MySQL binlog。\nhttps://www.infoq.com/presentations/vitess/ https://vitess.io/docs/16.0/reference/vreplication/vreplication/\nPresto Presto 是一个分布式 SQL 查询引擎，可以在云计算和大数据领域广泛应用。Presto 主要用于处理大数据查询和分析，它支持从多个数据源中进行高速查询，包括 Hadoop，MySQL，Cassandra，PostgreSQL 等。 Presto 的设计目的是为了在处理大量数据时提供快速的查询和分析功能。Presto 支持 SQL 查询和复杂的分析，可以处理 PB 级别的数据，并且可以在数秒内返回查询结果。Presto 的一个优点是它可以与多个数据存储系统集成，而不需要将数据迁移到一个中心位置，从而降低数据分析的成本和复杂性。它主要处理 OLAP 场景。\n有点儿像 calcite。\n分布式 join 支持方式（内存计算 + 数据复制） 从上述文档可以看到 presto 是基于代价的查询优化器。一般也是两种方式实现分布式 join\nPartitioned: each node participating in the query builds a hash table from only a fraction of the data Broadcast: each node participating in the query builds a hash table from all of the data (data is replicated to each node) partitioned 方式是输入数据的部分生成 hash 表然后去各个节点上迭代。\nbroadcast 的方式则是基于全部数据生成 hash 表然后 join。\n参考文档 https://www.citusdata.com/blog/2021/07/02/citus-tips-joins-between-local-and-distributed-postgres-tables/\nhttps://docs.citusdata.com/en/stable/articles/outer_joins.html\nhttps://blog.bigdataboutique.com/2020/05/querying-multiple-data-sources-with-a-single-query-using-prestos-query-federation-veulwi\nhttps://prestodb.io/docs/current/optimizer/cost-based-optimizations.html\nhttps://docs.treasuredata.com/display/public/PD/About+Presto+Distributed+Query+Engine\n","description":"","tags":null,"title":"分布式 join 调研","uri":"/posts/join_learn/"},{"categories":["Tech"],"content":"Apache ShardingSphere 元数据介绍 Apache ShardingSphere 的元数据主要包括规则、数据源、表结构等信息。规则信息可能包含分片、加密、读写分离、事务、高可用等。 数据源信息存储的是需要通过 ShardingSphere 来进行管理的底层数据库资源。表结构信息主要就是底层数据源的表结构，包括表的 column 信息、索引信息等。\nApache ShardingSphere 通过这些元数据信息配合治理中心的能力，例如 zookeeper、etcd 的存储和通知能力，可以实现集群内配置的共享和变更，从而实现计算节点的水平扩展。同时元数据信息对于 ShardingSphere 而言也是至关重要的，以表的数据结构为例，ShardingSphere 利用表的数据结构可以对采用了加密规则的 SQL 进行正确的改写，内核中的 federation 引擎也会利用表结构信息进行 SQL 优化。\n既然 ShardingSphere 的元数据如此重要，那么我们该怎么入手了解元数据呢？\nApache ShardingSphere 三层元数据结构 ShardingSphere 的三层元数据结构是个了解元数据信息的很好入口。我们可以启动 ShardingSphere–proxy 的 cluster 模式，这样可以在 zookeeper 中直观的看到 ShardingSphere 的三层元数据结构。 如下结构展示了 ShardingSphere 元数据在 zookeeper 中的结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 governance_ds --metadata (元数据信息) ----sharding_db (逻辑库名称) ------active_version (当前生效的版本) ------versions --------0 ----------data_sources (底层数据库信息) ----------rules (逻辑库的规则信息，例如分片规则，加密规则等) ------schemas (表、视图信息) --------sharding_db ----------tables ------------t_order ------------t_single ----------views ----shardingsphere (内置元数据库) ------schemas --------shardingsphere ----------tables ------------sharding_table_statics (分片统计信息表) ------------cluster_information （版本信息） ----performance_schema (模拟 mysql 数据库) ------schemas --------performance_schema ----------tables ------------accounts ----information_schema (模拟 mysql 数据库) ------schemas --------information_schema ----------tables ------------tables ------------schemata ------------columns ------------engines ------------routines ------------parameters ------------views ----mysql ----sys --sys_data (内置元数据库的具体行信息) ----shardingsphere ------schemas --------shardingsphere ----------tables ------------sharding_table_statistics --------------79ff60bc40ab09395bed54cfecd08f94 --------------e832393209c9a4e7e117664c5ff8fc61 ------------cluster_information --------------d387c4f7de791e34d206f7dd59e24c1c 上图展示了 ShardingSphere 的元数据结构信息，内容非常丰富，不过我们也不用担心，只要了解了大的节点信息，就可以根据逻辑推测出其它相关信息。\n我们首先关注下 metadata 节点，下方的 active_version 中展示了当前生效的元数据版本，然后在 versions 节点下我们可以找到生效的版本 0 ， 在这个生效版本节点下存储了规则和数据库的连接信息。 schemas 节点下存储了该逻辑库下的表和视图的具体信息，值得一提的是，ShardingSphere 存储的表结构信息是经过规则装饰后的表结构信息，例如分片表只会根据其中一片真实表获取结构，然后替换表名称，如果是加密规则，也不会在表结构中展示真实的加密列信息。这样做的目的是为了让用户完全面向逻辑库来进行相关操作。\n对于 metadata 下的 shardingsphere 节点，该节点也跟逻辑库的结构类似，只不过这里存储的是 ShardingSphere 内置的一些表结构，例如 sharding_table_statics （分片统计信息表）、cluster_information （版本信息表）等。这一块内容后面的内置元数据库会再进一步展开讲讲。\n至于 performance_schema、information_schema、mysql、sys 等节点都是用来模拟 mysql 的数据字典而建立的。当然如果用户的前端协议是 PostgreSQL， 那么这些数据字典也会变更为 PostgreSQL 的数据字典。目前数据字典这边主要用于支持各种客户端工具直接连接 proxy，未来会进一步增加数据收集从而支持对于这些数据字典的查询。这一块也会在后面的元数据库内容中做进一步介绍。\n可能有同学看到我们的数据结构会比较好奇，为什么在 sharding_db 的逻辑库下还有一层 sharding_db 呢？为什么不直接把 tables 节点放置在 sharding_db 下呢？其实这也就是 ShardingSphere 三层元数据结构的由来，ShardingSphere 是一个数据库的上层平台，因此需要兼容多种数据库格式，MySQL 的确是两层结构的，但是对于 PostgreSQL 而言，它是三层结构的，由 实例 和 database 以及 schema 组成，因此为了兼容性，ShardingSphere 使用了三层数据库结构，对于 MySQL，ShardingSphere 增加了一个相同的逻辑 schema 层，从而保证逻辑的统一性。\n在了解了 metadata 的结构之后，相信细心的同学也发现了 sys_data 节点。那么这个节点的作用是什么呢？让我们走进 ShardingSphere 的内置元数据库。\nApache ShardingSphere 内置元数据库 内置元数据库简介 内置元数据库是什么？其实我们从上面的节点中就能猜出来，在 metadata 节点下有一个 shardingsphere 节点，该节点下目前存在着两张表，sharding_table_statics （分片信息收集表），cluster_information （版本信息表）。可能有同学会说他们都是 ShardingSphere 内部信息的收集表，的确 ShardingSphere 内置元数据库的设计目标之一就是存储内部收集信息服务于功能和用户，另外，内置元数据库还可以用来存储用户设置的信息（暂未实现）。\n说了这么多，肯定有同学好奇，sys_data 节点的作用是什么。ShardingSphere 将表的结构信息存储在 metadata 中，表的行信息存储在 sys_data 中。我们可以看到 sys_data 节点下的多个 id，其实他们就是该表的每一行，具体行的内容就存储在该节点之下。\n通过在 metadata 中存储表结构，在 sys_data 中存储表的内容，我们就可以实现通过 sql 直接查询内置元数据库表的信息了。例如我们可以通过如下 SQL 直接查询出当前的版本信息。\nmysql\u003e select * from shardingsphere.cluster_information; +----------------+ | version | +----------------+ | 5.3.2-SNAPSHOT | +----------------+ 1 row in set (3.35 sec) 我们还可以直接查询出某个真实表的统计信息。\nmysql\u003e select * from shardingsphere.sharding_table_statistics where actual_table_name = \"t_order_1\"\\G *************************** 1. row *************************** id: 2 logic_database_name: sharding_db logic_table_name: t_order actual_database_name: ds_0 actual_table_name: t_order_1 row_count: 0 size: 16384 *************************** 2. row *************************** id: 4 logic_database_name: sharding_db logic_table_name: t_order actual_database_name: ds_1 actual_table_name: t_order_1 row_count: 0 size: 16384 2 rows in set (0.39 sec) 那么元数据库的功能是如何实现的呢？\n内置数据库的工作原理 内置元数据库功能的实现主要依赖两个方面，一是数据收集，二是查询实现。\n数据收集主要需要考虑如何将数据采集到内存，同时还需要考虑如何将内存信息同步到治理中心来保证集群间的同步。如何将数据采集到内存中，以 sharding_table_statistics 表的数据采集为例。从 ShardingSphereDataCollector 接口出发，我们看到该类有一个数据收集的方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * ShardingSphere data collector. */ @SingletonSPI public interface ShardingSphereDataCollector extends TypedSPI { /** * Collect. * * @param databaseName database name * @param table table * @param shardingSphereDatabases ShardingSphere databases * @return ShardingSphere table data * @throws SQLException sql exception */ Optional\u003cShardingSphereTableData\u003e collect(String databaseName, ShardingSphereTable table, Map\u003cString, ShardingSphereDatabase\u003e shardingSphereDatabases) throws SQLException; 查询该接口的调用方，我们可以看到是 ShardingSphereDataCollectorRunnable 定时任务在调用。没错，当前的实现方式是在 proxy 端启动定时任务来进行数据收集，根据内置元数据表来区分不同的数据收集器进行数据采集。未来会根据社区用户反馈，可能将这一部分做成 e-job 触发的方式来进行收集。 ShardingStatisticsTableCollector 类中具体展示了收集信息的逻辑。主要就是利用底层数据源和分片规则去查询数据库信息从而获取统计信息。\n在数据收集完成后，ShardingSphereDataScheduleCollector 类会根据收集到的信息跟内存中的信息做比对，如果发现不一致，那么会通过 EVENTBUS 发送事件给治理中心模块，治理中心收到事件后，会更新其它节点的信息，并做内存同步。监听事件类的代码如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /** * ShardingSphere schema data registry subscriber. */ @SuppressWarnings(\"UnstableApiUsage\") public final class ShardingSphereSchemaDataRegistrySubscriber { private final ShardingSphereDataPersistService persistService; private final GlobalLockPersistService lockPersistService; public ShardingSphereSchemaDataRegistrySubscriber(final ClusterPersistRepository repository, final GlobalLockPersistService globalLockPersistService, final EventBusContext eventBusContext) { persistService = new ShardingSphereDataPersistService(repository); lockPersistService = globalLockPersistService; eventBusContext.register(this); } /** * Update when ShardingSphere schema data altered. * * @param event schema altered event */ @Subscribe public void update(final ShardingSphereSchemaDataAlteredEvent event) { String databaseName = event.getDatabaseName(); String schemaName = event.getSchemaName(); GlobalLockDefinition lockDefinition = new GlobalLockDefinition(\"sys_data_\" + event.getDatabaseName() + event.getSchemaName() + event.getTableName()); if (lockPersistService.tryLock(lockDefinition, 10_000)) { try { persistService.getTableRowDataPersistService().persist(databaseName, schemaName, event.getTableName(), event.getAddedRows()); persistService.getTableRowDataPersistService().persist(databaseName, schemaName, event.getTableName(), event.getUpdatedRows()); persistService.getTableRowDataPersistService().delete(databaseName, schemaName, event.getTableName(), event.getDeletedRows()); } finally { lockPersistService.unlock(lockDefinition); } } } } 如上述代码所示，在事件接受后，当前节点会更新治理中心的信息，当节点信息发生变更后，依赖 zookeeper/etcd 的通知能力，集群中的其它节点会收到治理中心的变更，并在如下代码中更新自己节点的内存信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 /** * ShardingSphere data changed watcher. */ public final class ShardingSphereDataChangedWatcher implements GovernanceWatcher\u003cGovernanceEvent\u003e { @Override public Collection\u003cString\u003e getWatchingKeys(final String databaseName) { return Collections.singleton(ShardingSphereDataNode.getShardingSphereDataNodePath()); } @Override public Collection\u003cType\u003e getWatchingTypes() { return Arrays.asList(Type.ADDED, Type.UPDATED, Type.DELETED); } @Override public Optional\u003cGovernanceEvent\u003e createGovernanceEvent(final DataChangedEvent event) { if (isDatabaseChanged(event)) { return createDatabaseChangedEvent(event); } if (isSchemaChanged(event)) { return createSchemaChangedEvent(event); } if (isTableChanged(event)) { return createTableChangedEvent(event); } if (isTableRowDataChanged(event)) { return createRowDataChangedEvent(event); } return Optional.empty(); } private boolean isDatabaseChanged(final DataChangedEvent event) { return ShardingSphereDataNode.getDatabaseName(event.getKey()).isPresent(); } private boolean isSchemaChanged(final DataChangedEvent event) { return ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()).isPresent() \u0026\u0026 ShardingSphereDataNode.getSchemaName(event.getKey()).isPresent(); } private boolean isTableChanged(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaNameBySchemaPath(event.getKey()); Optional\u003cString\u003e tableName = ShardingSphereDataNode.getTableName(event.getKey()); return databaseName.isPresent() \u0026\u0026 schemaName.isPresent() \u0026\u0026 tableName.isPresent(); } private boolean isTableRowDataChanged(final DataChangedEvent event) { return ShardingSphereDataNode.getRowUniqueKey(event.getKey()).isPresent(); } private Optional\u003cGovernanceEvent\u003e createDatabaseChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseName(event.getKey()); Preconditions.checkState(databaseName.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType()) { return Optional.of(new DatabaseDataAddedEvent(databaseName.get())); } if (Type.DELETED == event.getType()) { return Optional.of(new DatabaseDataDeletedEvent(databaseName.get())); } return Optional.empty(); } private Optional\u003cGovernanceEvent\u003e createSchemaChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Preconditions.checkState(databaseName.isPresent()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaName(event.getKey()); Preconditions.checkState(schemaName.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType()) { return Optional.of(new SchemaDataAddedEvent(databaseName.get(), schemaName.get())); } if (Type.DELETED == event.getType()) { return Optional.of(new SchemaDataDeletedEvent(databaseName.get(), schemaName.get())); } return Optional.empty(); } private Optional\u003cGovernanceEvent\u003e createTableChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Preconditions.checkState(databaseName.isPresent()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaNameBySchemaPath(event.getKey()); Preconditions.checkState(schemaName.isPresent()); return doCreateTableChangedEvent(event, databaseName.get(), schemaName.get()); } private Optional\u003cGovernanceEvent\u003e doCreateTableChangedEvent(final DataChangedEvent event, final String databaseName, final String schemaName) { Optional\u003cString\u003e tableName = ShardingSphereDataNode.getTableName(event.getKey()); Preconditions.checkState(tableName.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType()) { return Optional.of(new TableDataChangedEvent(databaseName, schemaName, tableName.get(), null)); } if (Type.DELETED == event.getType()) { return Optional.of(new TableDataChangedEvent(databaseName, schemaName, null, tableName.get())); } return Optional.empty(); } private Optional\u003cGovernanceEvent\u003e createRowDataChangedEvent(final DataChangedEvent event) { Optional\u003cString\u003e databaseName = ShardingSphereDataNode.getDatabaseNameByDatabasePath(event.getKey()); Preconditions.checkState(databaseName.isPresent()); Optional\u003cString\u003e schemaName = ShardingSphereDataNode.getSchemaNameBySchemaPath(event.getKey()); Preconditions.checkState(schemaName.isPresent()); Optional\u003cString\u003e tableName = ShardingSphereDataNode.getTableNameByRowPath(event.getKey()); Preconditions.checkState(tableName.isPresent()); Optional\u003cString\u003e rowPath = ShardingSphereDataNode.getRowUniqueKey(event.getKey()); Preconditions.checkState(rowPath.isPresent()); if (Type.ADDED == event.getType() || Type.UPDATED == event.getType() \u0026\u0026 !Strings.isNullOrEmpty(event.getValue())) { YamlShardingSphereRowData yamlShardingSphereRowData = YamlEngine.unmarshal(event.getValue(), YamlShardingSphereRowData.class); return Optional.of(new ShardingSphereRowDataChangedEvent(databaseName.get(), schemaName.get(), tableName.get(), yamlShardingSphereRowData)); } if (Type.DELETED == event.getType()) { return Optional.of(new ShardingSphereRowDataDeletedEvent(databaseName.get(), schemaName.get(), tableName.get(), rowPath.get())); } return Optional.empty(); } } 经过上述流程，我们就完成了元数据库信息的收集。\n那么内置元数据库是如何支持用户查询的呢？其实就是利用了 ShardingSphere 内置的 federation 引擎实现的。federation 引擎借助 calcite 的能力，可以将内存中的数据结构注册到 calcite 上，通过 calcite 将 sql 查询转化为内存的查询，从而实现 SQL 语句直接返回结果。 在 FilterableTableScanExecutor 类中，我们可以看到如果用户查询的表在内置元数据库中，那么会采用内存查询。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 private Enumerable\u003cObject[]\u003e executeByShardingSphereData(final String databaseName, final String schemaName, final ShardingSphereTable table) { Optional\u003cShardingSphereTableData\u003e tableData = Optional.ofNullable(data.getDatabaseData().get(databaseName)).map(optional -\u003e optional.getSchemaData().get(schemaName)) .map(ShardingSphereSchemaData::getTableData).map(shardingSphereData -\u003e shardingSphereData.get(table.getName())); return tableData.map(this::createMemoryEnumerator).orElseGet(this::createEmptyEnumerable); } private Enumerable\u003cObject[]\u003e createMemoryEnumerator(final ShardingSphereTableData tableData) { return new AbstractEnumerable\u003cObject[]\u003e() { @Override public Enumerator\u003cObject[]\u003e enumerator() { return new MemoryEnumerator\u003c\u003e(tableData.getRows()); } }; } 当然 federation 引擎还提供了其它更强大的功能，例如跨库查询、复杂子查询等，当前 federation 也在快速迭代中，欢迎社区小伙伴前来贡献。\n在了解了内置元数据库的基本实现原理后，我们就可以利用内置元数据库实现更多更丰富的功能了。例如支持 PostgreSQL 客户端的 \\d 查询。\nPostgreSQL \\d 的查询支持 PostgreSQL \\d 是 PG 客户端常用的命令之一，要实现 \\d 的查询，其实就是需要实现它对应的 SQL，并且需要对数据做一定的装饰，例如分片表替换成逻辑表。\n\\d 实际的执行语句如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT n.nspname as \"Schema\", c.relname as \"Name\", CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", c.reloptions as \"Storage\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 需要实现该语句的查询，我们需要收集 pg_catalog.pg_class pg_catalog.pg_namespace 两张表的信息。另外我们还需要模拟返回如下两个函数的返回结果 pg_catalog.pg_get_userbyid(c.relowner), pg_catalog.pg_table_is_visible(c.oid)。\n表的收集同上面的 sharding_table_statistics 表的收集逻辑类似，这里就不再赘述，由于 pg_class 内容比较多，所以我们只收集 \\d 涉及到的一些信息。另外在数据收集阶段，由于分片规则的存在，我们需要展示逻辑表名，因此需要对收集的信息做进一步的装饰，例如表名的替换。\n查询的过程我们只需要模拟函数结果就可以，很幸运，calcite 提供了注册函数的能力。当然目前只是单纯的 mock，未来可以进一步扩展成为真实数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 /** * Create catalog reader. * * @param schemaName schema name * @param schema schema * @param relDataTypeFactory rel data type factory * @param connectionConfig connection config * @return calcite catalog reader */ public static CalciteCatalogReader createCatalogReader(final String schemaName, final Schema schema, final RelDataTypeFactory relDataTypeFactory, final CalciteConnectionConfig connectionConfig) { CalciteSchema rootSchema = CalciteSchema.createRootSchema(true); rootSchema.add(schemaName, schema); registryUserDefinedFunction(schemaName, rootSchema.plus()); return new CalciteCatalogReader(rootSchema, Collections.singletonList(schemaName), relDataTypeFactory, connectionConfig); } private static void registryUserDefinedFunction(final String schemaName, final SchemaPlus schemaPlus) { if (!\"pg_catalog\".equalsIgnoreCase(schemaName)) { return; } schemaPlus.add(\"pg_catalog.pg_table_is_visible\", ScalarFunctionImpl.create(SQLFederationPlannerUtil.class, \"pgTableIsVisible\")); schemaPlus.add(\"pg_catalog.pg_get_userbyid\", ScalarFunctionImpl.create(SQLFederationPlannerUtil.class, \"pgGetUserById\")); } /** * Mock pg_table_is_visible function. * * @param oid oid * @return true */ @SuppressWarnings(\"unused\") public static boolean pgTableIsVisible(final Long oid) { return true; } /** * Mock pg_get_userbyid function. * * @param oid oid * @return user name */ @SuppressWarnings(\"unused\") public static String pgGetUserById(final Long oid) { return \"mock user\"; } 其它流程就基本跟 sharding_table_statistics 一致。那么我们来看下我们的成果吧。\n我们执行 \\d 结果如下（t_order 为分片表，t_single 为普通单表）。\n1 2 3 4 5 6 7 sharding_db=\u003e \\d List of relations Schema | Name | Type | Owner --------+----------+-------------------+----------- public | t_order | table | mock user public | t_single | table | mock user (2 rows) 目前 ShardingSphere 内置元数据库功能是试验性的功能，很多流程和实现方式还需要进一步完善，它是 ShardingSphere 社区对于元数据库的一次探索，它的进一步发展还依赖社区小伙伴的支持。读完了全完的你是不是也跃跃欲试了？欢迎来社区贡献，我们准备了丰富的任务列表等你来挑战。 https://github.com/apache/shardingsphere/issues/24378\n","description":"","tags":null,"title":"ShardingSphere 元数据能力增强解读与实战","uri":"/posts/shardingsphere_meta_learn_action/"},{"categories":["Tech"],"content":"TiDB 使用 创建表时可以设置数据过期时间，然后后台定时任务删除相关数据\n产品文档：https://docs.pingcap.com/zh/tidb/dev/time-to-live\n功能列表 功能 备注 create 语句中增加 TTL 属性，指定表中数据的过期时间 create 语句中增加注释信息(兼容 MySQL)，包含 TTL 属性 alter 语句修改 TTL 属性 TTL 可以配合表中其它列的属性来使用 可以指定 TTL 任务时间间隔 PolarDB-X 创建 TTL 表，按照时间分区，定期删除和创建相关分区表\n产品文档： https://help.aliyun.com/document_detail/403528.html\n功能列表 功能 备注 对按照时间进行 range 分区的表，定时失效过期分区，定时提前创建分区 仅用在自动模式下的分区表上 支持通过 DDL 语句来定义相关分区的 TTL TTL表支持的时间分区列类型为：date、datetime； 所有的唯一键（包括主键）必须包含TTL表的local partition by range时间分区列；所有的唯一键（包括主键）必须包含TTL表的local partition by range时间分区列 支持查看分区信息以及过期时间等 information_schema.local_partitions 支持校验物理表的物理分区的完整性 支持 Alter 语句手动创建新分区表/删除过期分区表 支持普通表和 TTL 表互相转换 支持创建、查看、删除 TTL 定时任务 Google Spanner 创建表时可以设置行删除策略，后台任务扫描表并删除相关行\n产品文档：https://cloud.google.com/spanner/docs/ttl?hl=zh-cn\n功能列表 功能 备注 create 语句中增加行删除策略 alter 语句修改行删除策略 查看表的 TTL 删除情况 ","description":"","tags":null,"title":"DataTTL 相关调研","uri":"/posts/datattl/"},{"categories":["Tech"],"content":"目标 新增 SS 默认系统库，初步支持全局静态元数据（编码、事务隔离级别）管理 设计元数据调度收集功能，支持 ShardingSphere 动态元数据管理（数据分布等信息） 现状 当前 ss 缺乏自己的元数据信息，例如分片的数据分布、编码等等。\n之前为了解决各个客户端连接报错的问题，设计了各个数据库方言的模拟库。 只有用户表会从真实数据库获取，其他表都是通过 yaml 文件来模拟存储到 zk 的。\n元数据存储调研 MySQL Mysql 将相关参数放在 variables_info 并将相关值设置在全局或 session 级别对应的表中\n1 2 3 4 5 6 7 8 9 10 11 mysql\u003e show tables like '%variables%'; +--------------------------------------------+ | Tables_in_performance_schema (%variables%) | +--------------------------------------------+ | global_variables | | persisted_variables | | session_variables | | user_variables_by_thread | | variables_by_thread | | variables_info | +--------------------------------------------+ 查询隔离级别\n1 2 3 4 5 6 7 8 9 10 11 mysql\u003e SELECT VI.VARIABLE_NAME, GV.VARIABLE_VALUE, VI.MIN_VALUE, VI.MAX_VALUE FROM performance_schema.variables_info AS VI INNER JOIN performance_schema.global_variables AS GV USING (VARIABLE_NAME) where variable_name = 'transaction_isolation' ORDER BY VARIABLE_NAME; +-----------------------+-----------------+-----------+-----------+ | VARIABLE_NAME | VARIABLE_VALUE | MIN_VALUE | MAX_VALUE | +-----------------------+-----------------+-----------+-----------+ | transaction_isolation | REPEATABLE-READ | 0 | 0 | +-----------------------+-----------------+-----------+-----------+ 1 row in set (0.01 sec) Postgres pg_settings 表\n1 2 3 4 5 postgres=# select * from pg_settings where name = 'default_transaction_isolation'; name | setting | unit | category | short_desc | extra_desc | context | vartype | source | min_val | max_val | enumvals | boot_val | reset_val | sourcefile | sourceline | pending_restart -------------------------------+----------------+------+-------------------------------------------------+---------------------------------------------------------------+------------+---------+---------+---------+---------+---------+----------------------------------------------------------------------+----------------+----------------+------------+------------+----------------- default_transaction_isolation | read committed | | Client Connection Defaults / Statement Behavior | Sets the transaction isolation level of each new transaction. | | user | enum | default | | | {serializable,\"repeatable read\",\"read committed\",\"read uncommitted\"} | read committed | read committed | | | f (1 row) PolarDB-X Polardb-x 的元数据库 Polardb-x 中维护了自己的元数据库用于内部流程的使用 它的元数据中几乎维护了所有需要的信息，例如 tables，columns，lock，ddl_job, schedule job 等。 另外在内存中也维护了一份元数据信息，主要是表、列。内存中元数据来源于存储在 mysql 中的元数据，每次 ddl 执行后，都会生成刷新对应表的任务，通过任务触发 cn 从 mysql 中查询并缓存。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 mysql\u003e show tables; +---------------------------------------+ | Tables_in_polardbx_meta_db_polardbx | +---------------------------------------+ | __test_sequence | | __test_sequence_opt | | audit_log | | backfill_objects | | baseline_info | | binlog_polarx_command | | character_sets | | checker_reports | | collation_character_set_applicability | | collations | | column_metas | | column_statistics | | columns | | complex_task_outline | | concurrency_control_rule | | concurrency_control_trigger | | config_listener | | db_group_info | | db_info | | db_priv | | ddl_engine | | ddl_engine_archive | | ddl_engine_task | | ddl_engine_task_archive | | ddl_plan | | default_role_state | | engines | | feature_usage_statistics | | file_storage_files_meta | | file_storage_info | | files | | fired_scheduled_jobs | | global_variables | | group_detail_info | | indexes | | inst_config | | inst_lock | | key_column_usage | | locality_info | | node_info | | partition_group | | partition_group_delta | | partitions | | plan_info | | quarantine_config | | read_write_lock | | recycle_bin | | referential_constraints | | role_priv | | scaleout_backfill_objects | | scaleout_checker_reports | | scaleout_outline | | scheduled_jobs | | schema_change | | schemata | | sequence | | sequence_opt | | server_info | | session_variables | | storage_info | | table_constraints | | table_group | | table_local_partitions | | table_partitions | | table_partitions_delta | | table_priv | | table_statistics | | tablegroup_outline | | tables | | tables_ext | | user_login_error_limit | | user_priv | | variable_config | | views | +---------------------------------------+ 74 rows in set (0.02 sec) 从DDL的执行来看 polardb-x 的元数据使用 简述：整个 ddl 过程中涉及了 ddl-job 任务相关表的使用（ddl_engine 、ddl_engine_task ）,表相关元数据的使用（tables、tables_ext、columns）以及锁 （read_write_lock）等。 以 DDL 语句为例，我们看一下 polardb-x 是怎么使用 metadata 的\n1 CREATE TABLE t1(id bigint not null auto_increment, name varchar(30), primary key(id)) dbpartition by hash(id); SQL语句进入PolarDB-X的CN后，将经历协议层、优化器、执行器的完整处理流程。首先经过解析、鉴权、校验，被解析为关系代数树后，在优化器中经历RBO和CBO生成执行计划，最终在DN上执行完成。 由于 DDL 涉及元数据的变更，所以可能会造成系统状态的不一致。所以 polardb-x 通过 ddl job 配合 metadataDb 以及 双版本元数据 + MDL锁来解决这个问题。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Override public Cursor handle(RelNode logicalPlan, ExecutionContext executionContext) { BaseDdlOperation logicalDdlPlan = (BaseDdlOperation) logicalPlan; initDdlContext(logicalDdlPlan, executionContext); // Validate the plan first and then return immediately if needed. boolean returnImmediately = validatePlan(logicalDdlPlan, executionContext); boolean isNewPartDb = DbInfoManager.getInstance().isNewPartitionDb(logicalDdlPlan.getSchemaName()); if (isNewPartDb) { setPartitionDbIndexAndPhyTable(logicalDdlPlan); } else { setDbIndexAndPhyTable(logicalDdlPlan); } // Build a specific DDL job by subclass. DdlJob ddlJob = returnImmediately? new TransientDdlJob(): buildDdlJob(logicalDdlPlan, executionContext); // Validate the DDL job before request. validateJob(logicalDdlPlan, ddlJob, executionContext); // Handle the client DDL request on the worker side. handleDdlRequest(ddlJob, executionContext); if (executionContext.getDdlContext().isSubJob()){ return buildSubJobResultCursor(ddlJob, executionContext); } return buildResultCursor(logicalDdlPlan, executionContext); } 将 sql 转化为执行计划后，就会创建 ddl job, ddl job 中包含了新增表信息到元数据库、创建物理表、同步元数据信息等任务。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Override protected ExecutableDdlJob doCreate() { CreateTableValidateTask validateTask = new CreateTableValidateTask(schemaName, logicalTableName, physicalPlanData.getTablesExtRecord()); CreateTableAddTablesExtMetaTask addExtMetaTask = new CreateTableAddTablesExtMetaTask(schemaName, logicalTableName, physicalPlanData.isTemporary(), physicalPlanData.getTablesExtRecord(), autoPartition); CreateTablePhyDdlTask phyDdlTask = new CreateTablePhyDdlTask(schemaName, logicalTableName, physicalPlanData); CdcDdlMarkTask cdcDdlMarkTask = new CdcDdlMarkTask(schemaName, physicalPlanData); CreateTableAddTablesMetaTask addTableMetaTask = new CreateTableAddTablesMetaTask(schemaName, logicalTableName, physicalPlanData.getDefaultDbIndex(), physicalPlanData.getDefaultPhyTableName(), physicalPlanData.getSequence(), physicalPlanData.getTablesExtRecord(), physicalPlanData.isPartitioned(), physicalPlanData.isIfNotExists(), physicalPlanData.getKind(), hasTimestampColumnDefault, binaryColumnDefaultValues); LocalityDesc locality = physicalPlanData.getLocalityDesc(); StoreTableLocalityTask storeLocalityTask = locality == null ? null : new StoreTableLocalityTask(schemaName, logicalTableName, locality.toString(), false); CreateTableShowTableMetaTask showTableMetaTask = new CreateTableShowTableMetaTask(schemaName, logicalTableName); TableSyncTask tableSyncTask = new TableSyncTask(schemaName, logicalTableName); ExecutableDdlJob4CreateTable result = new ExecutableDdlJob4CreateTable(); ... result.setCreateTableValidateTask(validateTask); result.setCreateTableAddTablesExtMetaTask(addExtMetaTask); result.setCreateTablePhyDdlTask(phyDdlTask); result.setCreateTableAddTablesMetaTask(addTableMetaTask); result.setCdcDdlMarkTask(cdcDdlMarkTask); result.setCreateTableShowTableMetaTask(showTableMetaTask); result.setTableSyncTask(tableSyncTask); return result; 创建 ddl job 之后，就会下发执行任务，storeJob 会将相关 job 写入ddl_engine_task,ddl_engine 表中。 notifyLeader 会通知相关 cn 节点执行 ddl job。ddl job 会更新元数据库中的 tables columns 等表，如果有多个 cn 节点，这里会触发节点间同步，收到同步信息的节点会从元数据中获取相关信息，并更新 cn 内缓存的元数据信息。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void execute() { ddlContext.setResources(ddlJob.getExcludeResources()); // Create a new job and put it in the queue. ddlJobManager.storeJob(ddlJob, ddlContext); // Request the leader to perform the job. DdlRequest ddlRequest = notifyLeader(ddlContext.getSchemaName(), Lists.newArrayList(ddlContext.getJobId())); // Wait for response from the leader, then respond to the client. if (ddlContext.isAsyncMode()) { return; } respond(ddlRequest, ddlJobManager, executionContext, true); } 关于元数据 GMS 三副本节点 polarDb-x 的元数据存储于内置的 GMS 三副本节点中，提供了全局时间戳来提供外部一致性读，具体可以参考：PolarDB-X 全局时间戳服务的设计 PolarDB-X 一致性共识协议 (X-Paxos)-阿里云开发者社区\nCockroachDB Cockroach db 的内置元数据库 Cockroach 的底层存储结构如下： 内置元数据设计 方案：表结构和存储分开，yaml 模拟表结构 流程图 Zk 结构 以 分片表数据量统计表 为例\ncreate table sharding_table_statistics ( id int, logic_database_name varchar(100), logic_table_name varchar(100), actual_database_name varchar(100), actual_table_name varchar(100), row_count BIGINT, size BIGINT ) 对应 zk 存储结构 红色为修改，黄色为新增，蓝色为 PG 特有（pg 同一个实例上的库是不共享元数据信息的，所以 pg 的 shardingsphere schema 增加在用户创建的逻辑库下，当然 postgres 库下也会模拟一个 shardingsphere schema）\n初始化\n通过 yaml 模拟生成表结构（同现有 information_schema 流程） 从 zk 获取 ShardingSphereData 对象 如果未获取到，根据表结构初始化 ShardingSphereData 对象 注册表结构到 calcite 方便进行查询使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public static MetaDataContexts create(final MetaDataPersistService persistService, final ContextManagerBuilderParameter parameter, final InstanceContext instanceContext) throws SQLException { Collection\u003cString\u003e databaseNames = instanceContext.getInstance().getMetaData() instanceof JDBCInstanceMetaData ? parameter.getDatabaseConfigs().keySet() : persistService.getDatabaseMetaDataService().loadAllDatabaseNames(); Map\u003cString, DatabaseConfiguration\u003e effectiveDatabaseConfigs = createEffectiveDatabaseConfigurations(databaseNames, parameter.getDatabaseConfigs(), persistService); Collection\u003cRuleConfiguration\u003e globalRuleConfigs = persistService.getGlobalRuleService().load(); ConfigurationProperties props = new ConfigurationProperties(persistService.getPropsService().load()); // 增加元数据库表结构的初始化 Map\u003cString, ShardingSphereDatabase\u003e databases = ShardingSphereDatabasesFactory.create(effectiveDatabaseConfigs, props, instanceContext); databases.putAll(reloadDatabases(databases, persistService)); ShardingSphereRuleMetaData globalMetaData = new ShardingSphereRuleMetaData(GlobalRulesBuilder.buildRules(globalRuleConfigs, databases, instanceContext, props)); ShardingSphereMetaData metaData = new ShardingSphereMetaData(databases, globalMetaData, props); ShardingSphereData shardingSphereData = initShardingSphereData(persistService, metaData, instanceContext); return new MetaDataContexts(persistService, metaData, shardingSphereData); } 1 2 3 4 5 6 7 private static ShardingSphereData initShardingSphereData(final MetaDataPersistService persistService, final ShardingSphereMetaData metaData, final InstanceContext instanceContext) { // 先从 zk 加载，没有的话就依赖之前的表结构进行初始化 ShardingSphereData result = persistService.getShardingSphereDataPersistService().load().orElse(ShardingSphereDataFactory.getInstance(metaData)); // 注册表结构到 calcite 方便后续查询使用 result.registerShardingSphereDataQueryEngine(metaData, instanceContext.getEventBusContext()); return result; } 初始化数据收集任务 （目前考虑采用监听 zk 的方式来触发相关任务的收集 + 定时任务收集） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @Slf4j public final class ShardingSphereDataContextManagerLifecycleListener implements ContextManagerLifecycleListener { @Override public void onInitialized(final ModeConfiguration modeConfig, final ContextManager contextManager) { ShardingSphereDataJobWorker.initialize(contextManager); } } public final class ShardingSphereDataJobWorker { private static final AtomicBoolean WORKER_INITIALIZED = new AtomicBoolean(false); /** * Initialize job worker. * @param contextManager context manager */ public static void initialize(final ContextManager contextManager) { if (WORKER_INITIALIZED.get()) { return; } synchronized (WORKER_INITIALIZED) { if (WORKER_INITIALIZED.get()) { return; } log.info(\"start worker initialization\"); // 开启定时收集线程 startScheduleThread(contextManager); // 监听 收集任务 zk 节点 ShardingSphereDataNodeWatcher.getInstance(); WORKER_INITIALIZED.set(true); log.info(\"worker initialization done\"); } } private static void startScheduleThread(final ContextManager contextManager) { // TODO start thread to collect data } } 内存中数据结构如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public final class MetaDataContexts implements AutoCloseable { private final MetaDataPersistService persistService; private final ShardingSphereMetaData metaData; private final ShardingSphereData shardingSphereData; /** * Sharding sphere data. */ @Getter public final class ShardingSphereData { .. // key: table name value: table data private final Map\u003cString, ShardingSphereTableData\u003e tableData = new LinkedHashMap\u003c\u003e(); .. } @RequiredArgsConstructor @Getter public class ShardingSphereTableData { private final String name; private final List\u003cShardingSphereRowData\u003e rows = new LinkedList\u003c\u003e(); } @RequiredArgsConstructor @Getter public class ShardingSphereRowData { private final List\u003cObject\u003e row; } 内存中元数据的变化通过 event 发送，并同步到 zk 中，通过 zk 监听同步刷新其它节点的元数据库 元数据库的使用 利用 calcite 进行查询 增删改，通过内存对象操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @Getter public final class ShardingSphereData { private final Map\u003cString, ShardingSphereTableData\u003e tableData = new LinkedHashMap\u003c\u003e(); private ShardingSphereDataQueryEngine queryEngine; /** * Query. * * @param sql sql * @return result set */ public ResultSet query(final String sql) { return queryEngine.query(sql); } /** * Register. * * @param metaData meta data * @param eventBusContext event bus */ public void registerShardingSphereDataQueryEngine(final ShardingSphereMetaData metaData, final EventBusContext eventBusContext) { ShardingSphereDataQueryEngine queryEngine = ShardingSphereDataQueryEngineFactory.getShardingSphereDataQueryEngine(); // 注册到 calcite queryEngine.init(metaData, eventBusContext, tableData); this.queryEngine = queryEngine; } } 查询的实现类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 public class ShardingSphereDataFederationQueryEngine implements ShardingSphereDataQueryEngine { private OptimizerContext optimizerContext; private ShardingSphereRuleMetaData globalRuleMetaData; private ConfigurationProperties props; private EventBusContext eventBusContext; private ShardingSphereMetaData metaData; private Map\u003cString, ShardingSphereTableData\u003e tableData; @Override public void init(final ShardingSphereMetaData metaData, final EventBusContext eventBusContext, final Map\u003cString, ShardingSphereTableData\u003e tableData) { this.optimizerContext = OptimizerContextFactory.create(metaData.getDatabases(), metaData.getGlobalRuleMetaData()); this.globalRuleMetaData = metaData.getGlobalRuleMetaData(); this.props = metaData.getProps(); this.eventBusContext = eventBusContext; this.tableData = tableData; this.metaData = metaData; } @SneakyThrows private Connection createConnection() { MemorySchema memorySchema = new MemorySchema(tableData, metaData.getDatabase(\"ShardingSphereData\").getSchema(\"ShardingSphereData\")); Properties info = new Properties(); info.setProperty(CalciteConnectionProperty.DEFAULT_NULL_COLLATION.camelName(), NullCollation.LAST.name()); info.setProperty(CalciteConnectionProperty.CASE_SENSITIVE.camelName(), \"false\"); Connection result = DriverManager.getConnection(\"jdbc:calcite:\", info); CalciteConnection calciteConnection = result.unwrap(CalciteConnection.class); SchemaPlus rootSchema = calciteConnection.getRootSchema(); rootSchema.add(\"memory\", memorySchema); return result; } @SneakyThrows @Override public ResultSet query(final String sql) { try (Connection connection = createConnection(); Statement statement = connection.createStatement()) { return statement.executeQuery(sql); } } @Override public boolean isDefault() { return true; } } Tasks use yaml to simulate ShardingSphere built in table add ShardingSphere data and persist add schedule thread or zk watch to trigger collect data support to use federation to query shardingsphere data ","description":"","tags":null,"title":"ShardingSphere 内置数据库设计","uri":"/posts/shardingspheresysdatabase/"},{"categories":["Tech"],"content":"PG \\d 支持 \\d 的现状 \\d 实际执行的语句如下\nSELECT n.nspname as \"Schema\", c.relname as \"Name\", CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", c.reloptions as \"Storage\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 查询结果如下\nSchema | Name | Type | Owner | Storage --------+----------------------+-------+---------+---------------------------------- public | new_t_single_view | view | gaussdb | public | student10w | table | gaussdb | {orientation=row,compression=no} public | t_broadcast_table | table | gaussdb | {orientation=row,compression=no} public | t_broadcast_view_new | view | gaussdb | public | t_encrypt | table | gaussdb | {orientation=row,compression=no} public | t_order_0 | table | gaussdb | {orientation=row,compression=no} public | t_order_1 | table | gaussdb | {orientation=row,compression=no} public | t_order_view_new_0 | view | gaussdb | public | t_order_view_new_1 | view | gaussdb | public | t_single | table | gaussdb | {orientation=row,compression=no} (10 rows) 涉及的系统表有\npg_catalog.pg_class pg_catalog.pg_namespace\n涉及的函数有\npg_catalog.pg_get_userbyid(c.relowner) pg_catalog.pg_table_is_visible(c.oid)\n方案设计 方案概述 利用之前的ShardingSphere 内置数据库设计 在系统库中增加对应的表，表结构和字段类型同原生的数据库一致，并做数据收集并装饰，通过 federation 执行内存查询\n治理中心结构 表的数据收集 SELECT -- 数据库收集值 n.nspname as \"Schema\", -- 数据库收集值 c.relname as \"Name\", -- 数据库收集值 CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", -- 通过 calcite 注册函数，返回 ss 中的用户信息 pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", -- 数据库收集值 c.reloptions as \"Storage\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' -- 通过 calcite 注册函数，mock 返回 true AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 需要收集 pg_class 和 pg_namespace 的信息，并做一些值的装饰，例如 分片表需要替换表名。\npg_class 表\nhttps://opengauss.org/zh/docs/3.1.0/docs/Developerguide/PG_CLASS.html\n黄色表示 \\d 使用到的字段\npg_namespace 表\nhttps://opengauss.org/zh/docs/3.1.0/docs/Developerguide/PG_NAMESPACE.html\n关键问题及解决方法 查询语句中的函数无法下推，如何使用 通过 federation 注册函数解决 pg_table_is_visible(c.oid) mock 返回 true，收集数据时就只收集 true 的部分；\npg_get_userbyid mock 返回 ss 中配置的用户；\npg_class 数据量比较大，只收集 \\d 使用到的数据行内容。 查询中需要根据 pg_class 的 relnamespace 关联到 pg_namespace 中的 oid， 但是由于分布式系统，因此可能存在 pg_namespace oid 相同但是 nspname 却不同。 需要新建一张系统 oid 映射关系表，通过 ss 生成 id 映射不同实例的 oid，在表存储时，需要存储 ss 生成的 oid。（需要进一步设计）\n针对 \\d+ 是否能够支持 SELECT n.nspname as \"Schema\", c.relname as \"Name\", CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'i' THEN 'index' WHEN 'I' THEN 'global partition index' WHEN 'S' THEN 'sequence' WHEN 'L' THEN 'large sequence' WHEN 'f' THEN 'foreign table' WHEN 'm' THEN 'materialized view' WHEN 'e' THEN 'stream' WHEN 'o' THEN 'contview' END as \"Type\", pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\", -- 该值暂时无法支持显示，需要获取大小 pg_catalog.pg_size_pretty(pg_catalog.pg_table_size(c.oid)) as \"Size\", c.reloptions as \"Storage\", -- 该值依赖 pg_catalog.pg_description 表的收集以及 oid 的映射维护 -- select description from pg_catalog.pg_description where objoid = $1 and classoid = (select oid from pg_catalog.pg_class where relname = $2 and relnamespace = 11) and objsubid = 0 pg_catalog.obj_description(c.oid, 'pg_class') as \"Description\" FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind IN ('r','v','m','S','L','f','e','o','') AND n.nspname \u003c\u003e 'pg_catalog' AND n.nspname \u003c\u003e 'db4ai' AND n.nspname \u003c\u003e 'information_schema' AND n.nspname !~ '^pg_toast' AND c.relname not like 'matviewmap\\_%' AND c.relname not like 'mlog\\_%' AND pg_catalog.pg_table_is_visible(c.oid) ORDER BY 1,2; 附录 系统函数分析 pg_get_userbyid\n对应源码为\n/* ---------- * pg_get_userbyid - Get a user name by roleid and * fallback to 'unknown (OID=n)' * ---------- */ Datum pg_get_userbyid(PG_FUNCTION_ARGS) { Oid roleid = PG_GETARG_OID(0); Name result; HeapTuple roletup; Form_pg_authid role_rec; /* * Allocate space for the result */ result = (Name) palloc(NAMEDATALEN); memset(NameStr(*result), 0, NAMEDATALEN); /* * Get the pg_authid entry and print the result */ roletup = SearchSysCache1(AUTHOID, ObjectIdGetDatum(roleid)); if (HeapTupleIsValid(roletup)) { role_rec = (Form_pg_authid) GETSTRUCT(roletup); *result = role_rec-\u003erolname; ReleaseSysCache(roletup); } else sprintf(NameStr(*result), \"unknown (OID=%u)\", roleid); PG_RETURN_NAME(result); } 可以用以下语句获取\n\u003cstrong\u003eselect\u003c/strong\u003e oid,rolname \u003cstrong\u003efrom\u003c/strong\u003e pg_authid where oid = 1234 pg_table_is_visible(c.oid) 函数对应源码如下\n/* * RelationIsVisible * Determine whether a relation (identified by OID) is visible in the * current search path. Visible means \"would be found by searching * for the unqualified relation name\". */ bool RelationIsVisible(Oid relid) { HeapTuple reltup; Form_pg_class relform; Oid relnamespace; bool visible; reltup = SearchSysCache1(\u003cem\u003eRELOID\u003c/em\u003e, ObjectIdGetDatum(relid)); if (!HeapTupleIsValid(reltup)) elog(ERROR, \"cache lookup failed for relation %u\", relid); relform = (Form_pg_class) GETSTRUCT(reltup); recomputeNamespacePath(); /* * Quick check: if it ain't in the path at all, it ain't visible. Items in * the system namespace are surely in the path and so we needn't even do * list_member_oid() for them. */ relnamespace = relform-\u003erelnamespace; if (relnamespace != PG_CATALOG_NAMESPACE \u0026\u0026 !list_member_oid(activeSearchPath, relnamespace)) visible = false; else { /* * If it is in the path, it might still not be visible; it could be * hidden by another relation of the same name earlier in the path. So * we must do a slow check for conflicting relations. */ char *relname = NameStr(relform-\u003erelname); ListCell *l; visible = false; foreach(l, activeSearchPath) { Oid namespaceId = lfirst_oid(l); if (namespaceId == relnamespace) { /* Found it first in path */ visible = true; break; } if (OidIsValid(get_relname_relid(relname, namespaceId))) { /* Found something else first in path */ break; } } } ReleaseSysCache(reltup); return visible; } ","description":"","tags":null,"title":"ShardingSphere PostgreSQL openGauss \\d 支持方案","uri":"/posts/shardingsphere_sys_data_d/"},{"categories":["Tech"],"content":"最近想写一下 mysql 前后端协议。\n看看能否参照 pisanix 来写一个简单版本的协议。\n发现一个同事写的最小化 MySQL Rust 代理实现\n","description":"","tags":null,"title":"Pisanix protocol","uri":"/posts/join-pisanix/"},{"categories":["Tech"],"content":"https://yeasy.gitbook.io/docker_practice/\n使用 dockerFile 创建镜像 FROM nginx RUN echo '\u003ch1\u003eHello, Docker!\u003c/h1\u003e' \u003e /usr/share/nginx/html/index.html 容器的使用 ","description":"","tags":null,"title":"Docker","uri":"/posts/docker/"},{"categories":["Tech"],"content":"构建 yat 下载源码\ngit clone https://gitee.com/opengauss/Yat.git 构建 yat\ncd Yat/yat-master chmod +x gradlew ./gradlew pack cd pkg chmod +x install ./install -F 根据源码中的 dockerFile 构建 dockerImage\n使用 yat 测试 测试 shardingSphere proxy 利用构建的 yat image 运行相关测试 需要挂在到对应目录\ndocker run --name yat0 -i -t -v /Users/chenchuxin/Documents/GitHub/Yat/openGaussBase:/root/openGaussBase -w /root/openGaussBase --entrypoint=bash --privileged=true yat-v1 修改 yat 项目 openGaussBase/conf 下的 configure.yml 文件\nyat.limit.case.size.max: '200K' yat.limit.case.count.max: 100000 yat.limit.case.depth.max: 10 yat.limit.case.name.pattern: .* 修改 yat 项目 openGaussBase/conf 下的 nodes.yml 文件\ndefault: host: 'host.docker.internal' db: url: 'jdbc:opengauss://host.docker.internal:3307/sharding_db?loggerLevel=OFF' driver: 'org.opengauss.Driver' username: 'root' password: 'root' port: 3307 name: 'sharding_db' ssh: port: 22 username: root password: '********' 创建 conf/env.sh\ntouch conf/env.sh 在 openGaussBase/schedule 目录创建 schedule.schd 文件，增加你需要测试的 case\ntest: SQL/DDL/view/Opengauss_Function_DDL_View_Case0034 进入 docker bash 模式 运行 yat suite\n[+] [2022-06-22 09:06:15] [05.394 s] [v] SQL/DDL/view/Opengauss_Function_DDL_View_Case0034 .... : ok ################# Testing Result 1/1 Using Time PT5.46011S At 2022-06-22 09:06:20 ################## Run test suite finished 也可能出现相关异常，可参考 https://gitee.com/opengauss/Yat/blob/master/yat-master/docs/index.md#yat-quick-start 来解决\n","description":"","tags":null,"title":"Add Yat Test for ShardingSphere","uri":"/posts/add-yat-test-for-shardingsphere/"},{"categories":["Tech"],"content":"https://zhuanlan.zhihu.com/p/515688555\nmac 开发环境启动 galaxysql https://hub.docker.com/r/polardbx/polardb-x\ndocker pull polardbx/polardb-x docker run -d --name some-dn-and-gms --env mode=dev -p 4886:4886 -p 32886:32886 polardbx/polardb-x docker exec -it 41d8a027 bash mysql -h127.0.0.1 -P4886 -uroot -padmin -D polardbx_meta_db_polardbx -e \"select passwd_enc from storage_info where inst_kind=2\" 获取密码后修改 server.properties\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 serverPort=8528 managerPort=3406 rpcPort=9090 charset=utf-8 processors=4 processorHandler=16 processorKillExecutor=128 timerExecutor=8 managerExecutor=256 serverExecutor=1024 idleTimeout= trustedIps=127.0.0.1 slowSqlTime=1000 maxConnection=20000 allowManagerLogin=1 allowCrossDbQuery=true galaxyXProtocol=1 metaDbAddr=127.0.0.1:4886 metaDbXprotoPort=32886 metaDbUser=my_polarx # 前文查看的存储节点密码 metaDbPasswd=qEJWtCdgsOIie4j4mKP2Bvg2dsFHzdIhTaqMiq86N1QQU1HHL7olKb60pxz5hp/4 #?? E2+jB0*0\u0026gM9)9$4+6)E4@1$lO9%G8+jA4_ metaDbName=polardbx_meta_db_polardbx instanceId=polardbx-polardbx 注释掉 CobarServer.java 中 tryStartCdcManager(); 代码\n启动配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u003ccomponent name=\"ProjectRunConfigurationManager\"\u003e \u003cconfiguration default=\"false\" name=\"TddlLauncher\" type=\"Application\" factoryName=\"Application\" singleton=\"false\" nameIsGenerated=\"true\"\u003e \u003cenvs\u003e \u003cenv name=\"dnPasswordKey\" value=\"asdf1234ghjk5678\" /\u003e \u003c/envs\u003e \u003coption name=\"MAIN_CLASS_NAME\" value=\"com.alibaba.polardbx.server.TddlLauncher\" /\u003e \u003cmodule name=\"polardbx-server\" /\u003e \u003coption name=\"VM_PARAMETERS\" value=\"-Dserver.conf=$PROJECT_DIR$/polardbx-server/src/main/conf/server.properties\" /\u003e \u003cextension name=\"coverage\"\u003e \u003cpattern\u003e \u003coption name=\"PATTERN\" value=\"com.alibaba.polardbx.server.*\" /\u003e \u003coption name=\"ENABLED\" value=\"true\" /\u003e \u003c/pattern\u003e \u003c/extension\u003e \u003cmethod v=\"2\"\u003e \u003coption name=\"Make\" enabled=\"true\" /\u003e \u003c/method\u003e \u003c/configuration\u003e \u003c/component\u003e 连接 polar-dbx cn\nmysql -h127.0.0.1 -P8528 -upolardbx_root -p123456 polardb-x debug ddl create ddl 一直不能成功原来是因为，docker 镜像中没有启动 cdc，DDL 流程中涉及到 notify cdc，所以注释 CdcManager.notifyDdl 方法中的代码可以执行成功。 这篇文档写得挺好，说明了 ddl 执行的流程 https://zhuanlan.zhihu.com/p/515688555\n","description":"","tags":null,"title":"Polar Db","uri":"/posts/polar-db/"},{"categories":["Tech"],"content":"Step 1: 克隆项目\nStep 2: 添加新的远程仓库 为了修改他人 Fork 的仓库，你需要将其添加到自己的远程仓库列表中\n$ git remote add sirmin https://github.com/SirMin/shardingsphere.git 现在，当你执行 git remote -v 指令时，就可以看到他人 Fork 的仓库，出现在你的远程仓库列表中：\n% git remote -v origin\thttps://github.com/tuichenchuxin/shardingsphere.git (fetch) origin\thttps://github.com/tuichenchuxin/shardingsphere.git (push) sirmin\thttps://github.com/SirMin/shardingsphere.git (fetch) sirmin\thttps://github.com/SirMin/shardingsphere.git (push) upstream\thttps://github.com/apache/shardingsphere.git (fetch) upstream\thttps://github.com/apache/shardingsphere.git (push) Step 3: 拉取新的远程仓库\n$ git fetch sirmin Step 4: 切换到对应的分支 你需要确认一下，贡献者提出 Pull Request 时所用的分支（如果你不确定他们使用的哪个分支，请看 Pull Request 顶部的信息）：\n在本地，给该分支起一个不重复的名字，例如 EvanOne0616-patch，然后切换到贡献者提出 Pull Request 所用的分支：\n$ git checkout -b sirmin-patch sirmin/sirmin/resultSet Step 5: 提交修改，推送远程\n$ git commit -m \"Fix the wrong spelling of the word\" $ git push sirmin HEAD:sirmin/resultSet ","description":"","tags":null,"title":"How to Modify Pr","uri":"/posts/modifypr/"},{"categories":["Tech"],"content":"docker pull registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g docker run --privileged --restart=always --name oracle_11g -p 1521:1521 -d registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g docker exec -it 容器ID /bin/bash source /home/oracle/.bash_profile sqlplus nologconnect as sysdba 1 2 3 4 create user oracle identified by oracle#123; alter user system identified by system; alter user system identified by 123456; grant connect,resource,dba to oracle; 参考：https://baijiahao.baidu.com/s?id=1709232831349390844\u0026wfr=spider\u0026for=pc\n","description":"","tags":null,"title":"mac docker 安装 oracle 11g","uri":"/posts/installoracle11g/"},{"categories":["Tech"],"content":"Freemarker freemarker 是一款开源的模版引擎，可以基于模版方便的生成结果。 https://freemarker.apache.org/\nFreemarker 的使用 编写 ftl 模版 以生成 postgres 查询的 sql 语句为例 编写 delete.ftl 文件，${} 中的字段是参数\n1 DROP TABLE IF EXISTS ${schema}.${name}; 当然实际使用中的模版可能复杂的多，以部分创建表模版为例 我们可以在模版中使用 import 引入其它模版 使用 assign 设置变量 使用 if, list 等 使用 ?? 判断是否为空，使用 !false 如果为空，默认 false\n\u003c#import \"../../macro/constraints.ftl\" as CONSTRAINTS\u003e \u003c#assign with_clause = false\u003e \u003c#if fillfactor!false || parallel_workers!false || toast_tuple_target!false || (autovacuum_custom!false \u0026\u0026 add_vacuum_settings_in_sql!false) || autovacuum_enabled == 't' || autovacuum_enabled == 'f' || (toast_autovacuum!false \u0026\u0026 add_vacuum_settings_in_sql!false) || toast_autovacuum_enabled == 't' || toast_autovacuum_enabled == 'f' \u003e \u003c#assign with_clause = true\u003e \u003c/#if\u003e \u003c#list columns as c \u003e 其它模版使用可以参考 https://freemarker.apache.org/docs/dgui_template.html\njava 使用 freemarker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @NoArgsConstructor(access = AccessLevel.PRIVATE) public final class FreemarkerManager { private static final FreemarkerManager INSTANCE = new FreemarkerManager(); private final Configuration templateConfig = createTemplateConfiguration(); /** * Get freemarker manager instance. * * @return freemarker manager instance */ public static FreemarkerManager getInstance() { return INSTANCE; } @SneakyThrows private Configuration createTemplateConfiguration() { Configuration result = new Configuration(Configuration.VERSION_2_3_31); result.setDirectoryForTemplateLoading(new File(Objects.requireNonNull(FreemarkerManager.class.getClassLoader().getResource(\"template\")).getFile())); result.setDefaultEncoding(\"UTF-8\"); return result; } /** * Get sql from template. * * @param data data * @param path path * @return sql */ @SneakyThrows public static String getSqlFromTemplate(final Map\u003cString, Object\u003e data, final String path) { Template template = FreemarkerManager.getInstance().templateConfig.getTemplate(path); try (StringWriter result = new StringWriter()) { template.process(data, result); return result.toString(); } } } ","description":"","tags":null,"title":"Freemarker 的使用","uri":"/posts/use_free_marker/"},{"categories":["Tech"],"content":"下载源码 https://github.com/postgres/pgadmin4\n安装环境 brew install node brew install yarn cd runtime yarn install node_modules/nw/nwjs/nwjs.app/Contents/MacOS/nwjs . sudo mkdir \"/var/log/pgadmin\" sudo chmod a+wrx \"/var/log/pgadmin\" sudo mkdir \"/var/lib/pgadmin\" sudo chmod a+wrx \"/var/lib/pgadmin\" pip install --upgrade pip pip install psycopg2-binary make install-node 最后运行 pgAdmin4.py 过程中会有一些问题，参考 https://github.com/postgres/pgadmin4 readme. 和 stack overflow\n","description":"","tags":null,"title":"How To Debug PgAdmin4","uri":"/posts/howtodebugpgadmin4/"},{"categories":["Tech"],"content":"PgAmdin4 展示 DDL 语句 通过 PgAdmin4 可以获取 table 的 DDL 语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 -- Table: public.t_order_0 -- DROP TABLE IF EXISTS public.t_order_0; CREATE TABLE IF NOT EXISTS public.t_order_0 ( order_id integer NOT NULL, user_id integer NOT NULL, status character varying(45) COLLATE pg_catalog.\"default\", CONSTRAINT t_order_0_pkey PRIMARY KEY (order_id) ) TABLESPACE pg_default; ALTER TABLE IF EXISTS public.t_order_0 OWNER to postgres; COMMENT ON TABLE public.t_order_0 IS 'haha'; COMMENT ON COLUMN public.t_order_0.order_id IS 'haha'; PgAdmin4 是如何展示对应的 DDL 语句的呢 https://github.com/postgres/pgadmin4 翻阅源码发现 DDL 语句的展示，主要是通过以下步骤来获取 SQL 语句的。\n查询表基础信息（区分 pg 不同版本） 获取相关权限信息 查询列相关信息 检查 of_type 和 继承表，如果存在需要获取 获取该表的所有列信息 对列做格式化 查询 constriant 信息，并添加到 列上 检查 table is partitions 查询 index 信息 查询 ROW SECURITY POLICY 查询 Triggers 查询 Rules 相关代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def sql(self, gid, sid, did, scid, tid): \"\"\" This function will creates reverse engineered sql for the table object Args: gid: Server Group ID sid: Server ID did: Database ID scid: Schema ID tid: Table ID \"\"\" main_sql = [] status, res = self._fetch_table_properties(did, scid, tid) if not status: return res if len(res['rows']) == 0: return gone(gettext(self.not_found_error_msg())) data = res['rows'][0] return BaseTableView.get_reverse_engineered_sql( self, did=did, scid=scid, tid=tid, main_sql=main_sql, data=data, add_not_exists_clause=True) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_reverse_engineered_sql(self, **kwargs): \"\"\" This function will creates reverse engineered sql for the table object Args: kwargs \"\"\" did = kwargs.get('did') scid = kwargs.get('scid') tid = kwargs.get('tid') main_sql = kwargs.get('main_sql') data = kwargs.get('data') json_resp = kwargs.get('json_resp', True) diff_partition_sql = kwargs.get('diff_partition_sql', False) if_exists_flag = kwargs.get('add_not_exists_clause', False) # Table \u0026 Schema declaration so that we can use them in child nodes schema = data['schema'] table = data['name'] is_partitioned = 'is_partitioned' in data and data['is_partitioned'] # Get Reverse engineered sql for Table self._get_resql_for_table(did, scid, tid, data, json_resp, main_sql, add_not_exists_clause=if_exists_flag) # Get Reverse engineered sql for Table self._get_resql_for_index(did, tid, main_sql, json_resp, schema, table, add_not_exists_clause=if_exists_flag) # Get Reverse engineered sql for ROW SECURITY POLICY self._get_resql_for_row_security_policy(scid, tid, json_resp, main_sql, schema, table) # Get Reverse engineered sql for Triggers self._get_resql_for_triggers(tid, json_resp, main_sql, schema, table) # Get Reverse engineered sql for Compound Triggers self._get_resql_for_compound_triggers(tid, main_sql, schema, table) # Get Reverse engineered sql for Rules self._get_resql_for_rules(tid, main_sql, table, json_resp) # Get Reverse engineered sql for Partitions partition_main_sql = \"\" if is_partitioned: sql = render_template(\"/\".join([self.partition_template_path, self._NODES_SQL]), scid=scid, tid=tid) status, rset = self.conn.execute_2darray(sql) if not status: return internal_server_error(errormsg=rset) self._get_resql_for_partitions(data, rset, json_resp, diff_partition_sql, main_sql, did) sql = '\\n'.join(main_sql) if not json_resp: return sql, partition_main_sql return ajax_response(response=sql.strip('\\n')) pgAdmin 展示表结构反向 SQL 执行了哪些 SQL 语句呢？ table propeties 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 SELECT rel.oid, rel.relname AS name, rel.reltablespace AS spcoid,rel.relacl AS relacl_str, (CASE WHEN length(spc.spcname::text) \u003e 0 OR rel.relkind = 'p' THEN spc.spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END) as spcname, (CASE rel.relreplident WHEN 'd' THEN 'default' WHEN 'n' THEN 'nothing' WHEN 'f' THEN 'full' WHEN 'i' THEN 'index' END) as replica_identity, (select nspname FROM pg_catalog.pg_namespace WHERE oid = 2200::oid ) as schema, pg_catalog.pg_get_userbyid(rel.relowner) AS relowner, rel.relkind, (CASE WHEN rel.relkind = 'p' THEN true ELSE false END) AS is_partitioned, rel.relhassubclass, rel.reltuples::bigint, des.description, con.conname, con.conkey, EXISTS(select 1 FROM pg_catalog.pg_trigger JOIN pg_catalog.pg_proc pt ON pt.oid=tgfoid AND pt.proname='logtrigger' JOIN pg_catalog.pg_proc pc ON pc.pronamespace=pt.pronamespace AND pc.proname='slonyversion' WHERE tgrelid=rel.oid) AS isrepl, (SELECT count(*) FROM pg_catalog.pg_trigger WHERE tgrelid=rel.oid AND tgisinternal = FALSE) AS triggercount, (SELECT ARRAY(SELECT CASE WHEN (nspname NOT LIKE 'pg\\_%') THEN pg_catalog.quote_ident(nspname)||'.'||pg_catalog.quote_ident(c.relname) ELSE pg_catalog.quote_ident(c.relname) END AS inherited_tables FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid ORDER BY inhseqno)) AS coll_inherits, (SELECT count(*) FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid) AS inherited_tables_cnt, (CASE WHEN rel.relpersistence = 'u' THEN true ELSE false END) AS relpersistence, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'fillfactor=([0-9]*)') AS fillfactor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'parallel_workers=([0-9]*)') AS parallel_workers, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'toast_tuple_target=([0-9]*)') AS toast_tuple_target, (substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS autovacuum_enabled, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS autovacuum_freeze_table_age, (substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS toast_autovacuum_enabled, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS toast_autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS toast_autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS toast_autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS toast_autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS toast_autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS toast_autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS toast_autovacuum_freeze_table_age, rel.reloptions AS reloptions, tst.reloptions AS toast_reloptions, rel.reloftype, CASE WHEN typ.typname IS NOT NULL THEN (select pg_catalog.quote_ident(nspname) FROM pg_catalog.pg_namespace WHERE oid = 2200::oid )||'.'||pg_catalog.quote_ident(typ.typname) ELSE typ.typname END AS typname, typ.typrelid AS typoid, rel.relrowsecurity as rlspolicy, rel.relforcerowsecurity as forcerlspolicy, (CASE WHEN rel.reltoastrelid = 0 THEN false ELSE true END) AS hastoasttable, (SELECT pg_catalog.array_agg(provider || '=' || label) FROM pg_catalog.pg_seclabels sl1 WHERE sl1.objoid=rel.oid AND sl1.objsubid=0) AS seclabels, (CASE WHEN rel.oid \u003c= 13756::oid THEN true ElSE false END) AS is_sys_table -- Added for partition table , (CASE WHEN rel.relkind = 'p' THEN pg_catalog.pg_get_partkeydef(16410::oid) ELSE '' END) AS partition_scheme FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=rel.oid AND des.objsubid=0 AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND NOT rel.relispartition AND rel.oid = 16410::oid ORDER BY rel.relname; 表总数 'SELECT COUNT(*)::text FROM public.t_order_1;' 权限 SELECT 'relacl' as deftype, COALESCE(gt.rolname, 'PUBLIC') grantee, g.rolname grantor, pg_catalog.array_agg(privilege_type) as privileges, pg_catalog.array_agg(is_grantable) as grantable FROM (SELECT d.grantee, d.grantor, d.is_grantable, CASE d.privilege_type WHEN 'CONNECT' THEN 'c' WHEN 'CREATE' THEN 'C' WHEN 'DELETE' THEN 'd' WHEN 'EXECUTE' THEN 'X' WHEN 'INSERT' THEN 'a' WHEN 'REFERENCES' THEN 'x' WHEN 'SELECT' THEN 'r' WHEN 'TEMPORARY' THEN 'T' WHEN 'TRIGGER' THEN 't' WHEN 'TRUNCATE' THEN 'D' WHEN 'UPDATE' THEN 'w' WHEN 'USAGE' THEN 'U' ELSE 'UNKNOWN' END AS privilege_type FROM (SELECT rel.relacl FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) acl, (SELECT (d).grantee AS grantee, (d).grantor AS grantor, (d).is_grantable AS is_grantable, (d).privilege_type AS privilege_type FROM (SELECT aclexplode(rel.relacl) as d FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) a ORDER BY privilege_type) d ) d LEFT JOIN pg_catalog.pg_roles g ON (d.grantor = g.oid) LEFT JOIN pg_catalog.pg_roles gt ON (d.grantee = gt.oid) GROUP BY g.rolname, gt.rolname reverse engine for table\nSELECT 'relacl' as deftype, COALESCE(gt.rolname, 'PUBLIC') grantee, g.rolname grantor, pg_catalog.array_agg(privilege_type) as privileges, pg_catalog.array_agg(is_grantable) as grantable FROM (SELECT d.grantee, d.grantor, d.is_grantable, CASE d.privilege_type WHEN 'CONNECT' THEN 'c' WHEN 'CREATE' THEN 'C' WHEN 'DELETE' THEN 'd' WHEN 'EXECUTE' THEN 'X' WHEN 'INSERT' THEN 'a' WHEN 'REFERENCES' THEN 'x' WHEN 'SELECT' THEN 'r' WHEN 'TEMPORARY' THEN 'T' WHEN 'TRIGGER' THEN 't' WHEN 'TRUNCATE' THEN 'D' WHEN 'UPDATE' THEN 'w' WHEN 'USAGE' THEN 'U' ELSE 'UNKNOWN' END AS privilege_type FROM (SELECT rel.relacl FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) acl, (SELECT (d).grantee AS grantee, (d).grantor AS grantor, (d).is_grantable AS is_grantable, (d).privilege_type AS privilege_type FROM (SELECT aclexplode(rel.relacl) as d FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND rel.oid = 16410::oid ) a ORDER BY privilege_type) d ) d LEFT JOIN pg_catalog.pg_roles g ON (d.grantor = g.oid) LEFT JOIN pg_catalog.pg_roles gt ON (d.grantee = gt.oid) GROUP BY g.rolname, gt.rolname 获取列\nSELECT att.attname as name, att.atttypid, att.attlen, att.attnum, att.attndims, att.atttypmod, att.attacl, att.attnotnull, att.attoptions, att.attstattarget, att.attstorage, att.attidentity, pg_catalog.pg_get_expr(def.adbin, def.adrelid) AS defval, pg_catalog.format_type(ty.oid,NULL) AS typname, pg_catalog.format_type(ty.oid,att.atttypmod) AS displaytypname, pg_catalog.format_type(ty.oid,att.atttypmod) AS cltype, CASE WHEN ty.typelem \u003e 0 THEN ty.typelem ELSE ty.oid END as elemoid, (SELECT nspname FROM pg_catalog.pg_namespace WHERE oid = ty.typnamespace) as typnspname, ty.typstorage AS defaultstorage, description, pi.indkey, (SELECT count(1) FROM pg_catalog.pg_type t2 WHERE t2.typname=ty.typname) \u003e 1 AS isdup, CASE WHEN length(coll.collname::text) \u003e 0 AND length(nspc.nspname::text) \u003e 0 THEN pg_catalog.concat(pg_catalog.quote_ident(nspc.nspname),'.',pg_catalog.quote_ident(coll.collname)) ELSE '' END AS collspcname, EXISTS(SELECT 1 FROM pg_catalog.pg_constraint WHERE conrelid=att.attrelid AND contype='f' AND att.attnum=ANY(conkey)) As is_fk, (SELECT pg_catalog.array_agg(provider || '=' || label) FROM pg_catalog.pg_seclabels sl1 WHERE sl1.objoid=att.attrelid AND sl1.objsubid=att.attnum) AS seclabels, (CASE WHEN (att.attnum \u003c 1) THEN true ElSE false END) AS is_sys_column, (CASE WHEN (att.attidentity in ('a', 'd')) THEN 'i' WHEN (att.attgenerated in ('s')) THEN 'g' ELSE 'n' END) AS colconstype, (CASE WHEN (att.attgenerated in ('s')) THEN pg_catalog.pg_get_expr(def.adbin, def.adrelid) END) AS genexpr, tab.relname as relname, (CASE WHEN tab.relkind = 'v' THEN true ELSE false END) AS is_view_only, seq.* FROM pg_catalog.pg_attribute att JOIN pg_catalog.pg_type ty ON ty.oid=atttypid LEFT OUTER JOIN pg_catalog.pg_attrdef def ON adrelid=att.attrelid AND adnum=att.attnum LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=att.attrelid AND des.objsubid=att.attnum AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN (pg_catalog.pg_depend dep JOIN pg_catalog.pg_class cs ON dep.classid='pg_class'::regclass AND dep.objid=cs.oid AND cs.relkind='S') ON dep.refobjid=att.attrelid AND dep.refobjsubid=att.attnum LEFT OUTER JOIN pg_catalog.pg_index pi ON pi.indrelid=att.attrelid AND indisprimary LEFT OUTER JOIN pg_catalog.pg_collation coll ON att.attcollation=coll.oid LEFT OUTER JOIN pg_catalog.pg_namespace nspc ON coll.collnamespace=nspc.oid LEFT OUTER JOIN pg_catalog.pg_sequence seq ON cs.oid=seq.seqrelid LEFT OUTER JOIN pg_catalog.pg_class tab on tab.oid = att.attrelid WHERE att.attrelid = 16410::oid AND att.attnum \u003e 0 AND att.attisdropped IS FALSE ORDER BY att.attnum; SELECT t.main_oid, pg_catalog.ARRAY_AGG(t.typname) as edit_types FROM (SELECT pc.castsource AS main_oid, pg_catalog.format_type(tt.oid,NULL) AS typname FROM pg_catalog.pg_type tt JOIN pg_catalog.pg_cast pc ON tt.oid=pc.casttarget WHERE pc.castsource IN (23,1043) AND pc.castcontext IN ('i', 'a') UNION SELECT tt.typbasetype AS main_oid, pg_catalog.format_type(tt.oid,NULL) AS typname FROM pg_catalog.pg_type tt WHERE tt.typbasetype IN (23,1043) ) t GROUP BY t.main_oid; SELECT 'attacl' as deftype, COALESCE(gt.rolname, 'PUBLIC') grantee, g.rolname grantor, pg_catalog.array_agg(privilege_type order by privilege_type) as privileges, pg_catalog.array_agg(is_grantable) as grantable FROM (SELECT d.grantee, d.grantor, d.is_grantable, CASE d.privilege_type WHEN 'CONNECT' THEN 'c' WHEN 'CREATE' THEN 'C' WHEN 'DELETE' THEN 'd' WHEN 'EXECUTE' THEN 'X' WHEN 'INSERT' THEN 'a' WHEN 'REFERENCES' THEN 'x' WHEN 'SELECT' THEN 'r' WHEN 'TEMPORARY' THEN 'T' WHEN 'TRIGGER' THEN 't' WHEN 'TRUNCATE' THEN 'D' WHEN 'UPDATE' THEN 'w' WHEN 'USAGE' THEN 'U' ELSE 'UNKNOWN' END AS privilege_type FROM (SELECT attacl FROM pg_catalog.pg_attribute att WHERE att.attrelid = 16410::oid AND att.attnum = 1::int ) acl, (SELECT (d).grantee AS grantee, (d).grantor AS grantor, (d).is_grantable AS is_grantable, (d).privilege_type AS privilege_type FROM (SELECT pg_catalog.aclexplode(attacl) as d FROM pg_catalog.pg_attribute att WHERE att.attrelid = 16410::oid AND att.attnum = 1::int) a) d ) d LEFT JOIN pg_catalog.pg_roles g ON (d.grantor = g.oid) LEFT JOIN pg_catalog.pg_roles gt ON (d.grantee = gt.oid) GROUP BY g.rolname, gt.rolname ORDER BY grantee SELECT cls.oid, cls.relname as name, indnkeyatts as col_count, CASE WHEN length(spcname::text) \u003e 0 THEN spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END as spcname, CASE contype WHEN 'p' THEN desp.description WHEN 'u' THEN desp.description WHEN 'x' THEN desp.description ELSE des.description END AS comment, condeferrable, condeferred, conislocal, substring(pg_catalog.array_to_string(cls.reloptions, ',') from 'fillfactor=([0-9]*)') AS fillfactor FROM pg_catalog.pg_index idx JOIN pg_catalog.pg_class cls ON cls.oid=indexrelid LEFT OUTER JOIN pg_catalog.pg_tablespace ta on ta.oid=cls.reltablespace LEFT JOIN pg_catalog.pg_depend dep ON (dep.classid = cls.tableoid AND dep.objid = cls.oid AND dep.refobjsubid = '0' AND dep.refclassid=(SELECT oid FROM pg_catalog.pg_class WHERE relname='pg_constraint') AND dep.deptype='i') LEFT OUTER JOIN pg_catalog.pg_constraint con ON (con.tableoid = dep.refclassid AND con.oid = dep.refobjid) LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=cls.oid AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_description desp ON (desp.objoid=con.oid AND desp.objsubid = 0 AND desp.classoid='pg_constraint'::regclass) WHERE indrelid = 16410::oid AND contype='u' ORDER BY cls.relname SELECT c.oid, conname as name, relname, nspname, description as comment, pg_catalog.pg_get_expr(conbin, conrelid, true) as consrc, connoinherit, NOT convalidated as convalidated, conislocal FROM pg_catalog.pg_constraint c JOIN pg_catalog.pg_class cl ON cl.oid=conrelid JOIN pg_catalog.pg_namespace nl ON nl.oid=relnamespace LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=c.oid AND des.classoid='pg_constraint'::regclass) WHERE contype = 'c' AND conrelid = 16410::oid SELECT cls.oid, cls.relname as name, indnkeyatts as col_count, amname, CASE WHEN length(spcname::text) \u003e 0 THEN spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END as spcname, CASE contype WHEN 'p' THEN desp.description WHEN 'u' THEN desp.description WHEN 'x' THEN desp.description ELSE des.description END AS comment, condeferrable, condeferred, substring(pg_catalog.array_to_string(cls.reloptions, ',') from 'fillfactor=([0-9]*)') AS fillfactor, pg_catalog.pg_get_expr(idx.indpred, idx.indrelid, true) AS indconstraint FROM pg_catalog.pg_index idx JOIN pg_catalog.pg_class cls ON cls.oid=indexrelid LEFT OUTER JOIN pg_catalog.pg_tablespace ta on ta.oid=cls.reltablespace JOIN pg_catalog.pg_am am ON am.oid=cls.relam LEFT JOIN pg_catalog.pg_depend dep ON (dep.classid = cls.tableoid AND dep.objid = cls.oid AND dep.refobjsubid = '0' AND dep.refclassid=(SELECT oid FROM pg_catalog.pg_class WHERE relname='pg_constraint') AND dep.deptype='i') LEFT OUTER JOIN pg_catalog.pg_constraint con ON (con.tableoid = dep.refclassid AND con.oid = dep.refobjid) LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=cls.oid AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_description desp ON (desp.objoid=con.oid AND desp.objsubid = 0 AND desp.classoid='pg_constraint'::regclass) WHERE indrelid = 16410::oid AND contype='x' ORDER BY cls.relname Table 结束，接下来是 index\nSELECT DISTINCT ON(cls.relname) cls.oid, cls.relname as name, (SELECT (CASE WHEN count(i.inhrelid) \u003e 0 THEN true ELSE false END) FROM pg_inherits i WHERE i.inhrelid = cls.oid) as is_inherited FROM pg_catalog.pg_index idx JOIN pg_catalog.pg_class cls ON cls.oid=indexrelid JOIN pg_catalog.pg_class tab ON tab.oid=indrelid LEFT OUTER JOIN pg_catalog.pg_tablespace ta on ta.oid=cls.reltablespace JOIN pg_catalog.pg_namespace n ON n.oid=tab.relnamespace JOIN pg_catalog.pg_am am ON am.oid=cls.relam LEFT JOIN pg_catalog.pg_depend dep ON (dep.classid = cls.tableoid AND dep.objid = cls.oid AND dep.refobjsubid = '0' AND dep.refclassid=(SELECT oid FROM pg_catalog.pg_class WHERE relname='pg_constraint') AND dep.deptype='i') LEFT OUTER JOIN pg_catalog.pg_constraint con ON (con.tableoid = dep.refclassid AND con.oid = dep.refobjid) WHERE indrelid = 16410::OID AND conname is NULL ORDER BY cls.relname ROW SECURITY POLICY\n'SELECT pl.oid AS oid, pl.polname AS name FROM pg_catalog.pg_policy pl WHERE pl.polrelid\t= 16410 ORDER BY pl.polname;' trigger\n1 2 3 4 5 SELECT t.oid, t.tgname as name, t.tgenabled AS is_enable_trigger FROM pg_catalog.pg_trigger t WHERE NOT tgisinternal AND tgrelid = 16410::OID ORDER BY tgname; rules\n1 2 3 4 5 6 7 8 9 10 11 12 SELECT rw.oid AS oid, rw.rulename AS name, CASE WHEN rw.ev_enabled != \\'D\\' THEN True ELSE False END AS enabled, rw.ev_enabled AS is_enable_rule FROM pg_catalog.pg_rewrite rw WHERE rw.ev_class = 16410 ORDER BY rw.rulename ","description":"","tags":null,"title":"PgAmdin4 展示 DDL 语句逻辑分析","uri":"/posts/analysispgamdin4ddl/"},{"categories":["Tech"],"content":"获取 postgres 的建表语句的几种方法 使用 pg_dump 可以使用 pg_dump 直接查看建表语句 以本机 docker 的 postgres 为例\ndocker run -it --rm postgres pg_dump -h host.docker.internal -p 5432 -U postgres -d demo_ds_0 -s -t t_order_0 结果展示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 -- -- PostgreSQL database dump -- -- Dumped from database version 14.2 (Debian 14.2-1.pgdg110+1) -- Dumped by pg_dump version 14.2 (Debian 14.2-1.pgdg110+1) SET statement_timeout = 0; SET lock_timeout = 0; SET idle_in_transaction_session_timeout = 0; SET client_encoding = 'UTF8'; SET standard_conforming_strings = on; SELECT pg_catalog.set_config('search_path', '', false); SET check_function_bodies = false; SET xmloption = content; SET client_min_messages = warning; SET row_security = off; SET default_tablespace = ''; SET default_table_access_method = heap; -- -- Name: t_order_0; Type: TABLE; Schema: public; Owner: postgres -- CREATE TABLE public.t_order_0 ( order_id integer NOT NULL, user_id integer NOT NULL, status character varying(45) ); ALTER TABLE public.t_order_0 OWNER TO postgres; -- -- Name: TABLE t_order_0; Type: COMMENT; Schema: public; Owner: postgres -- COMMENT ON TABLE public.t_order_0 IS 'haha'; -- -- Name: COLUMN t_order_0.order_id; Type: COMMENT; Schema: public; Owner: postgres -- COMMENT ON COLUMN public.t_order_0.order_id IS 'haha'; -- -- Name: t_order_0 t_order_0_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres -- ALTER TABLE ONLY public.t_order_0 ADD CONSTRAINT t_order_0_pkey PRIMARY KEY (order_id); -- -- PostgreSQL database dump complete -- 使用自定义函数 创建函数如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 CREATE OR REPLACE FUNCTION tabledef(oid) RETURNS text LANGUAGE sql STRICT AS $$ /* snatched from https://github.com/filiprem/pg-tools */ WITH attrdef AS ( SELECT n.nspname, c.relname, pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts, c.relpersistence, a.attnum, a.attname, pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype, (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault, a.attnotnull, (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation \u003c\u003e t.typcollation) as attcollation, a.attidentity, a.attgenerated FROM pg_catalog.pg_attribute a JOIN pg_catalog.pg_class c ON a.attrelid = c.oid JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid) WHERE a.attrelid = $1 AND a.attnum \u003e 0 AND NOT a.attisdropped ORDER BY a.attnum ), coldef AS ( SELECT attrdef.nspname, attrdef.relname, attrdef.relopts, attrdef.relpersistence, pg_catalog.format( '%I %s%s%s%s%s', attrdef.attname, attrdef.atttype, case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %I', attrdef.attcollation) end, case when attrdef.attnotnull then ' NOT NULL' else '' end, case when attrdef.attdefault is null then '' else case when attrdef.attgenerated = 's' then pg_catalog.format(' GENERATED ALWAYS AS (%s) STORED', attrdef.attdefault) when attrdef.attgenerated \u003c\u003e '' then ' GENERATED AS NOT_IMPLEMENTED' else pg_catalog.format(' DEFAULT %s', attrdef.attdefault) end end, case when attrdef.attidentity\u003c\u003e'' then pg_catalog.format(' GENERATED %s AS IDENTITY', case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end) else '' end ) as col_create_sql FROM attrdef ORDER BY attrdef.attnum ), tabdef AS ( SELECT coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence, string_agg(coldef.col_create_sql, E',\\n ') as cols_create_sql FROM coldef GROUP BY coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence ) SELECT format( 'CREATE%s TABLE %I.%I%s%s%s;', case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end, tabdef.nspname, tabdef.relname, coalesce( (SELECT format(E'\\n PARTITION OF %I.%I %s\\n', pn.nspname, pc.relname, pg_get_expr(c.relpartbound, c.oid)) FROM pg_class c JOIN pg_inherits i ON c.oid = i.inhrelid JOIN pg_class pc ON pc.oid = i.inhparent JOIN pg_namespace pn ON pn.oid = pc.relnamespace WHERE c.oid = $1), format(E' (\\n %s\\n)', tabdef.cols_create_sql) ), case when tabdef.relopts \u003c\u003e '' then format(' WITH (%s)', tabdef.relopts) else '' end, coalesce(E'\\nPARTITION BY '||pg_get_partkeydef($1), '') ) as table_create_sql FROM tabdef $$; 通过 pgAdmin4 获取 抓包后，发现 pgAdmin4 实际查询的 sql 如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 SELECT rel.oid, rel.relname AS name, rel.reltablespace AS spcoid,rel.relacl AS relacl_str, (CASE WHEN length(spc.spcname::text) \u003e 0 OR rel.relkind = 'p' THEN spc.spcname ELSE (SELECT sp.spcname FROM pg_catalog.pg_database dtb JOIN pg_catalog.pg_tablespace sp ON dtb.dattablespace=sp.oid WHERE dtb.oid = 16384::oid) END) as spcname, (CASE rel.relreplident WHEN 'd' THEN 'default' WHEN 'n' THEN 'nothing' WHEN 'f' THEN 'full' WHEN 'i' THEN 'index' END) as replica_identity, (select nspname FROM pg_catalog.pg_namespace WHERE oid = 2200::oid ) as schema, pg_catalog.pg_get_userbyid(rel.relowner) AS relowner, rel.relkind, (CASE WHEN rel.relkind = 'p' THEN true ELSE false END) AS is_partitioned, rel.relhassubclass, rel.reltuples::bigint, des.description, con.conname, con.conkey, EXISTS(select 1 FROM pg_catalog.pg_trigger JOIN pg_catalog.pg_proc pt ON pt.oid=tgfoid AND pt.proname='logtrigger' JOIN pg_catalog.pg_proc pc ON pc.pronamespace=pt.pronamespace AND pc.proname='slonyversion' WHERE tgrelid=rel.oid) AS isrepl, (SELECT count(*) FROM pg_catalog.pg_trigger WHERE tgrelid=rel.oid AND tgisinternal = FALSE) AS triggercount, (SELECT ARRAY(SELECT CASE WHEN (nspname NOT LIKE 'pg\\_%') THEN pg_catalog.quote_ident(nspname)||'.'||pg_catalog.quote_ident(c.relname) ELSE pg_catalog.quote_ident(c.relname) END AS inherited_tables FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid ORDER BY inhseqno)) AS coll_inherits, (SELECT count(*) FROM pg_catalog.pg_inherits i JOIN pg_catalog.pg_class c ON c.oid = i.inhparent JOIN pg_catalog.pg_namespace n ON n.oid=c.relnamespace WHERE i.inhrelid = rel.oid) AS inherited_tables_cnt, (CASE WHEN rel.relpersistence = 'u' THEN true ELSE false END) AS relpersistence, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'fillfactor=([0-9]*)') AS fillfactor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'parallel_workers=([0-9]*)') AS parallel_workers, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'toast_tuple_target=([0-9]*)') AS toast_tuple_target, (substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS autovacuum_enabled, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(rel.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS autovacuum_freeze_table_age, (substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_enabled=([a-z|0-9]*)'))::BOOL AS toast_autovacuum_enabled, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_threshold=([0-9]*)') AS toast_autovacuum_vacuum_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_vacuum_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_threshold=([0-9]*)') AS toast_autovacuum_analyze_threshold, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_analyze_scale_factor=([0-9]*[.]?[0-9]*)') AS toast_autovacuum_analyze_scale_factor, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_delay=([0-9]*)') AS toast_autovacuum_vacuum_cost_delay, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_vacuum_cost_limit=([0-9]*)') AS toast_autovacuum_vacuum_cost_limit, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_min_age=([0-9]*)') AS toast_autovacuum_freeze_min_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_max_age=([0-9]*)') AS toast_autovacuum_freeze_max_age, substring(pg_catalog.array_to_string(tst.reloptions, ',') FROM 'autovacuum_freeze_table_age=([0-9]*)') AS toast_autovacuum_freeze_table_age, rel.reloptions AS reloptions, tst.reloptions AS toast_reloptions, rel.reloftype, CASE WHEN typ.typname IS NOT NULL THEN (select pg_catalog.quote_ident(nspname) FROM pg_catalog.pg_namespace WHERE oid = 2200::oid )||'.'||pg_catalog.quote_ident(typ.typname) ELSE typ.typname END AS typname, typ.typrelid AS typoid, rel.relrowsecurity as rlspolicy, rel.relforcerowsecurity as forcerlspolicy, (CASE WHEN rel.reltoastrelid = 0 THEN false ELSE true END) AS hastoasttable, (SELECT pg_catalog.array_agg(provider || '=' || label) FROM pg_catalog.pg_seclabels sl1 WHERE sl1.objoid=rel.oid AND sl1.objsubid=0) AS seclabels, (CASE WHEN rel.oid \u003c= 13756::oid THEN true ElSE false END) AS is_sys_table -- Added for partition table , (CASE WHEN rel.relkind = 'p' THEN pg_catalog.pg_get_partkeydef(16396::oid) ELSE '' END) AS partition_scheme FROM pg_catalog.pg_class rel LEFT OUTER JOIN pg_catalog.pg_tablespace spc on spc.oid=rel.reltablespace LEFT OUTER JOIN pg_catalog.pg_description des ON (des.objoid=rel.oid AND des.objsubid=0 AND des.classoid='pg_class'::regclass) LEFT OUTER JOIN pg_catalog.pg_constraint con ON con.conrelid=rel.oid AND con.contype='p' LEFT OUTER JOIN pg_catalog.pg_class tst ON tst.oid = rel.reltoastrelid LEFT JOIN pg_catalog.pg_type typ ON rel.reloftype=typ.oid WHERE rel.relkind IN ('r','s','t','p') AND rel.relnamespace = 2200::oid AND NOT rel.relispartition AND rel.oid = 16396::oid ORDER BY rel.relname; https://github.com/postgres/pgadmin4\n","description":"","tags":null,"title":"How to get postgres create table sql","uri":"/posts/get_postgres_create_table_sql/"},{"categories":["Tech"],"content":"EventBus 的使用及注意点 使用 创建全局实例 1 static final EventBus INSTANCE = new EventBus(); 将含有 @Subscribe 的方法注册到全局实例上 1 INSTANCE.register(this); 发送 event 1 INSTANCE.post(event); 流程分析 采用上述方式时有以下几个注意点\n发送事件并且接受后处理事件是同步执行的，即同一个线程执行 如果在接受事件方法中有嵌套发送事件，那么该嵌套发送的事件不会立即执行，会等到第一个事件的接收方法完成后，再执行第二个事件。 EventBus 调用流程分析 创建 EventBus 实例 将订阅者注册到 EventBus 实例上 创建 EventBus 实例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public EventBus() { this(\"default\"); } /** * Creates a new EventBus with the given {@code identifier}. * * @param identifier a brief name for this bus, for logging purposes. Should be a valid Java * identifier. */ public EventBus(String identifier) { this( identifier, // 同步执行器 MoreExecutors.directExecutor(), Dispatcher.perThreadDispatchQueue(), LoggingHandler.INSTANCE); } 1 2 3 4 5 6 7 8 9 10 11 12 private static final class PerThreadQueuedDispatcher extends Dispatcher { // This dispatcher matches the original dispatch behavior of EventBus. /** Per-thread queue of events to dispatch. */ private final ThreadLocal\u003cQueue\u003cEvent\u003e\u003e queue = new ThreadLocal\u003cQueue\u003cEvent\u003e\u003e() { @Override protected Queue\u003cEvent\u003e initialValue() { return Queues.newArrayDeque(); } }; 注册 EventBus 订阅者 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 /** * Registers all subscriber methods on {@code object} to receive events. * * @param object object whose subscriber methods should be registered. */ public void register(Object object) { subscribers.register(object); } /** Registers all subscriber methods on the given listener object. */ void register(Object listener) { Multimap\u003cClass\u003c?\u003e, Subscriber\u003e listenerMethods = findAllSubscribers(listener); for (Entry\u003cClass\u003c?\u003e, Collection\u003cSubscriber\u003e\u003e entry : listenerMethods.asMap().entrySet()) { Class\u003c?\u003e eventType = entry.getKey(); Collection\u003cSubscriber\u003e eventMethodsInListener = entry.getValue(); CopyOnWriteArraySet\u003cSubscriber\u003e eventSubscribers = subscribers.get(eventType); if (eventSubscribers == null) { CopyOnWriteArraySet\u003cSubscriber\u003e newSet = new CopyOnWriteArraySet\u003c\u003e(); eventSubscribers = MoreObjects.firstNonNull(subscribers.putIfAbsent(eventType, newSet), newSet); } eventSubscribers.addAll(eventMethodsInListener); } } /** * Returns all subscribers for the given listener grouped by the type of event they subscribe to. */ private Multimap\u003cClass\u003c?\u003e, Subscriber\u003e findAllSubscribers(Object listener) { Multimap\u003cClass\u003c?\u003e, Subscriber\u003e methodsInListener = HashMultimap.create(); Class\u003c?\u003e clazz = listener.getClass(); for (Method method : getAnnotatedMethods(clazz)) { Class\u003c?\u003e[] parameterTypes = method.getParameterTypes(); Class\u003c?\u003e eventType = parameterTypes[0]; methodsInListener.put(eventType, Subscriber.create(bus, listener, method)); } return methodsInListener; } /** Creates a {@code Subscriber} for {@code method} on {@code listener}. */ static Subscriber create(EventBus bus, Object listener, Method method) { return isDeclaredThreadSafe(method) ? new Subscriber(bus, listener, method) // 调用方法采用 synchronized : new SynchronizedSubscriber(bus, listener, method); } 发送 event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 /** * Posts an event to all registered subscribers. This method will return successfully after the * event has been posted to all subscribers, and regardless of any exceptions thrown by * subscribers. * * \u003cp\u003eIf no subscribers have been subscribed for {@code event}'s class, and {@code event} is not * already a {@link DeadEvent}, it will be wrapped in a DeadEvent and reposted. * * @param event event to post. */ public void post(Object event) { Iterator\u003cSubscriber\u003e eventSubscribers = subscribers.getSubscribers(event); if (eventSubscribers.hasNext()) { dispatcher.dispatch(event, eventSubscribers); } else if (!(event instanceof DeadEvent)) { // the event had no subscribers and was not itself a DeadEvent post(new DeadEvent(this, event)); } } // 这里需要注意，如果当前线程在调用的 post 方法中，嵌套调用 post，第二个 post方法不会立刻执行，而是会加入 队列中，等到队列中第一个任务处理完，才会继续处理。 @Override void dispatch(Object event, Iterator\u003cSubscriber\u003e subscribers) { checkNotNull(event); checkNotNull(subscribers); Queue\u003cEvent\u003e queueForThread = queue.get(); queueForThread.offer(new Event(event, subscribers)); if (!dispatching.get()) { dispatching.set(true); try { Event nextEvent; while ((nextEvent = queueForThread.poll()) != null) { while (nextEvent.subscribers.hasNext()) { nextEvent.subscribers.next().dispatchEvent(nextEvent.event); } } } finally { dispatching.remove(); queue.remove(); } } } ","description":"","tags":null,"title":"guava EventBus 使用以及注意点","uri":"/posts/second/"},{"categories":["Tech"],"content":"背景 Apache ShardingSphere 基于用户的实际使用场景，为用户打造了多种实用功能，包括数据分片、读写分离等。在数据分片功能中，Apache ShardingSphere 提供了标准分片、复合分片等多种实用的分片策略，在各种分片策略中，用户又可以配置相关分片算法，从而解决数据分片的问题。在读写分离功能中，ShardingSphere 为用户提供了静态和动态的两种读写分离类型以及丰富的负载均衡算法以满足用户实际需求。 可以看到 ShardingSphere 的分片和读写分离功能已经非常丰富，不过用户的真实使用场景是千变万化的。以多租户场景为例，用户期望按照登录账号所属租户进行分片，但是租户信息却并不是存在于每条业务 SQL 中，这时从 SQL 中提取分片字段的算法将无法发挥作用。再以读写分离为例，大部分场景下用户都希望能够将查询操作路由到从库上执行，但是在某些实时性要求很高的场景下，用户希望将 SQL 强制路由到主库执行，这时读写分离就无法满足业务要求。 基于以上痛点 Apache ShardingSphere 为用户提供了 Hint 功能，用户可以结合实际业务场景，利用 SQL 外部的逻辑进行强制路由或者分片。目前 ShardingSphere 为用户提供了两种 Hint 方式，一种通过 Java API 手动编程，利用 HintManager 进行强制路由和分片，这种方式对采用 JDBC 编程的应用非常友好，只需要少量的代码编写，就能够轻松实现不依赖 SQL 的分片或者强制路由功能。另外一种方式对于不懂开发的 DBA 而言更加友好，ShardingSphere 基于分布式 SQL 提供的使用方式，利用 SQL HINT 和 DistSQL HINT， 为用户提供了无需编码就能实现的分片和强制路由功能。接下来，让我们一起了解下这两种使用方式。\n基于 HintManager 的手动编程 ShardingSphere 主要通过 HintManager 对象来实现强制路由和分片的功能。利用 HintManager，用户的分片将不用再依赖 SQL。它可以极大地扩展用户的使用场景，让用户可以更加灵活地进行数据分片或者强制路由。目前通过 HintManager，用户可以配合 ShardingSphere 内置的或者自定义的 Hint 算法实现分片功能，还可以通过设置指定数据源或者强制主库读写，实现强制路由功能。在学习 HintManager 的使用之前，让我们先来简单地了解一下它的实现原理，这有助于我们更好地使用它。\nHintManager 实现原理 其实通过查看 HintManager 代码，我们可以快速地了解它的原理。\n1 2 3 4 5 @NoArgsConstructor(access = AccessLevel.PRIVATE) public final class HintManager implements AutoCloseable { private static final ThreadLocal\u003cHintManager\u003e HINT_MANAGER_HOLDER = new ThreadLocal\u003c\u003e(); } 正如你所看到的，ShardingSphere 通过 ThreadLocal 来实现 HintManager 的功能，只要在同一个线程中，用户的分片设置都会得以保留。因此，只要用户在执行 SQL 之前调用 HintManager 相关功能，ShardingSphere 就能在当前线程中获取用户设置的分片或强制路由条件，从而进行分片或者路由操作。了解了 HintManager 的原理之后，让我们一起来学习一下它的使用。\nHintManager 的使用 使用 Hint 分片 Hint 分片算法需要用户实现 org.apache.shardingsphere.sharding.api.sharding.hint.HintShardingAlgorithm 接口。 Apache ShardingSphere 在进行路由时，将会从 HintManager 中获取分片值进行路由操作。\n参考配置如下： rules: - !SHARDING tables: t_order: actualDataNodes: demo_ds_${0..1}.t_order_${0..1} databaseStrategy: hint: algorithmClassName: xxx.xxx.xxx.HintXXXAlgorithm tableStrategy: hint: algorithmClassName: xxx.xxx.xxx.HintXXXAlgorithm defaultTableStrategy: none: defaultKeyGenerateStrategy: type: SNOWFLAKE column: order_id props: sql-show: true 获取 HintManager 实例 HintManager hintManager = HintManager.getInstance();\n添加分片键 使用 hintManager.addDatabaseShardingValue 来添加数据源分片键值。 使用 hintManager.addTableShardingValue 来添加表分片键值。 分库不分表情况下，强制路由至某一个分库时，可使用 hintManager.setDatabaseShardingValue 方式添加分片。 清除分片键值 分片键值保存在 ThreadLocal 中，所以需要在操作结束时调用 hintManager.close() 来清除 ThreadLocal 中的内容。 完整代码示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.addDatabaseShardingValue(\"t_order\", 1); hintManager.addTableShardingValue(\"t_order\", 2); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.setDatabaseShardingValue(3); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } 使用 Hint 强制主库路由 获取 HintManager 与基于 Hint 的数据分片相同。\n设置主库路由 使用 hintManager.setWriteRouteOnly 设置主库路由。 清除分片键值 与基于 Hint 的数据分片相同。\n完整代码示例 1 2 3 4 5 6 7 8 9 10 11 String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.setWriteRouteOnly(); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } 使用 Hint 路由至指定数据库 获取 HintManager 与基于 Hint 的数据分片相同。\n设置路由至指定数据库 使用 hintManager.setDataSourceName 设置数据库名称。 完整代码示例 1 2 3 4 5 6 7 8 9 10 11 String sql = \"SELECT * FROM t_order\"; try (HintManager hintManager = HintManager.getInstance(); Connection conn = dataSource.getConnection(); PreparedStatement preparedStatement = conn.prepareStatement(sql)) { hintManager.setDataSourceName(\"ds_0\"); try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { // ... } } } 清除强制路由值 与基于 Hint 的数据分片相同。\n在了解了基于 HintManager 的手动编程方式之后，让我们一起来了解 ShardingSphere 基于分布式 SQL 提供的另一种 Hint 的解决方案。\n基于分布式 SQL 的 Hint Apache ShardingSphere 的分布式 SQL HINT 主要由两种功能组成，一种叫做 SQL HINT，即基于 SQL 注释的方式提供的功能，另外一种是通过 DistSQL 实现的作用于 HintManager 的功能。\nSQL HINT SQL HINT 就是通过在 SQL 语句上增加注释，从而实现强制路由的一种 Hint 方式。它降低了用户改造代码的成本，同时完全脱离了 Java API 的限制，不仅可以在 ShardingSphere-JDBC 中使用，也可以直接在 ShardingSphere-Proxy 上使用。 以下面 SQL 为例，即使用户配置了针对 t_order 的相关分片算法，该 SQL 也会直接在数据库 ds_0 上原封不动地执行，并返回执行结果。\n1 2 /* ShardingSphere hint: dataSourceName=ds_0 */ SELECT * FROM t_order; 通过注释的方式我们可以方便地将 SQL 直接送达指定数据库执行而无视其它分片逻辑。以多租户场景为例，用户不用再配置复杂的分库逻辑，也无需改造业务逻辑，只需要将指定库添加到注释信息中即可。在了解了 SQL HINT 的基本使用之后，让我们一起来了解一下 SQL HINT 的实现原理。\nSQL HINT 的实现原理 其实了解 Apache ShardingSphere 的读者朋友们一定对 SQL 解析引擎不会感到陌生。SQL HINT 实现的第一步就是提取 SQL 中的注释信息。利用 antlr4 的通道功能，可以将 SQL 中的注释信息单独送至特定的隐藏通道，ShardingSphere 也正是利用该功能，在生成解析结果的同时，将隐藏通道中的注释信息一并提取出来了。具体实现如下方代码所示。\n将 SQL 中的注释送入隐藏通道。 lexer grammar Comments; import Symbol; BLOCK_COMMENT: '/*' .*? '*/' -\u003e channel(HIDDEN); INLINE_COMMENT: (('-- ' | '#') ~[\\r\\n]* ('\\r'? '\\n' | EOF) | '--' ('\\r'? '\\n' | EOF)) -\u003e channel(HIDDEN); 访问语法树后增加对于注释信息的提取： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public \u003cT\u003e T visit(final ParseContext parseContext) { ParseTreeVisitor\u003cT\u003e visitor = SQLVisitorFactory.newInstance(databaseType, visitorType, SQLVisitorRule.valueOf(parseContext.getParseTree().getClass()), props); T result = parseContext.getParseTree().accept(visitor); appendSQLComments(parseContext, result); return result; } private \u003cT\u003e void appendSQLComments(final ParseContext parseContext, final T visitResult) { if (!parseContext.getHiddenTokens().isEmpty() \u0026\u0026 visitResult instanceof AbstractSQLStatement) { Collection\u003cCommentSegment\u003e commentSegments = parseContext.getHiddenTokens().stream().map(each -\u003e new CommentSegment(each.getText(), each.getStartIndex(), each.getStopIndex())) .collect(Collectors.toList()); ((AbstractSQLStatement) visitResult).getCommentSegments().addAll(commentSegments); } } 提取出用户 SQL 中的注释信息之后，我们就需要根据注释信息来进行相关强制路由了。既然是路由，那么自然就需要使用 Apache ShardingSphere 的路由引擎，我们在路由引擎上做了一些针对 HINT 的改造。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public RouteContext route(final LogicSQL logicSQL, final ShardingSphereMetaData metaData) { RouteContext result = new RouteContext(); Optional\u003cString\u003e dataSourceName = findDataSourceByHint(logicSQL.getSqlStatementContext(), metaData.getResource().getDataSources()); if (dataSourceName.isPresent()) { result.getRouteUnits().add(new RouteUnit(new RouteMapper(dataSourceName.get(), dataSourceName.get()), Collections.emptyList())); return result; } for (Entry\u003cShardingSphereRule, SQLRouter\u003e entry : routers.entrySet()) { if (result.getRouteUnits().isEmpty()) { result = entry.getValue().createRouteContext(logicSQL, metaData, entry.getKey(), props); } else { entry.getValue().decorateRouteContext(result, logicSQL, metaData, entry.getKey(), props); } } if (result.getRouteUnits().isEmpty() \u0026\u0026 1 == metaData.getResource().getDataSources().size()) { String singleDataSourceName = metaData.getResource().getDataSources().keySet().iterator().next(); result.getRouteUnits().add(new RouteUnit(new RouteMapper(singleDataSourceName, singleDataSourceName), Collections.emptyList())); } return result; } ShardingSphere 首先发现了符合定义的 SQL 注释，再经过基本的校验之后，就会直接返回用户指定的路由结果，从而实现强制路由功能。在了解了 SQL HINT 的基本原理之后，让我们一起学习如何使用 SQL HINT。\n如何使用 SQL HINT SQL HINT 的使用非常简单，无论是 ShardingSphere-JDBC 还是 ShardingSphere-Porxy，都可以使用。 第一步打开注释解析开关。将 sqlCommentParseEnabled 设置为 true。 第二步在 SQL 上增加注释即可。目前 SQL HINT 支持指定数据源路由和主库路由。\n指定数据源路由：目前只支持路由至一个数据源。 注释格式暂时只支持/* /，内容需要以ShardingSphere hint:开始，属性名为 dataSourceName。 / ShardingSphere hint: dataSourceName=ds_0 */ SELECT * FROM t_order;\n主库路由：注释格式暂时只支持/* /，内容需要以ShardingSphere hint:开始，属性名为 writeRouteOnly。 / ShardingSphere hint: writeRouteOnly=true */ SELECT * FROM t_order;\nDistSQL HINT Apache ShardingSphere 的 DistSQL 也提供了 Hint 相关功能，让用户可以通过 ShardingSphere-Proxy 来实现分片和强制路由功能。\nDistSQL HINT 的实现原理 同前文一致，在学习使用 DistSQL HINT 功能之前，让我们一起来了解一下 DistSQL Hint 的实现原理。DistSQL HINT 的实现原理非常简单，其实就是通过操作 HintManager 实现的 HINT 功能。以读写分离 Hint 为例，当用户通过 ShardingSphere-Proxy 执行以下 SQL 时，其实 ShardingSphere 内部对 SQL 做了如下方代码所示的操作。 – 强制主库读写\n1 set readwrite_splitting hint source = write 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @RequiredArgsConstructor public final class SetReadwriteSplittingHintExecutor extends AbstractHintUpdateExecutor\u003cSetReadwriteSplittingHintStatement\u003e { private final SetReadwriteSplittingHintStatement sqlStatement; @Override public ResponseHeader execute() { HintSourceType sourceType = HintSourceType.typeOf(sqlStatement.getSource()); switch (sourceType) { case AUTO: HintManagerHolder.get().setReadwriteSplittingAuto(); break; case WRITE: HintManagerHolder.get().setWriteRouteOnly(); break; default: break; } return new UpdateResponseHeader(new EmptyStatement()); } } @NoArgsConstructor(access = AccessLevel.PRIVATE) public final class HintManagerHolder { private static final ThreadLocal\u003cHintManager\u003e HINT_MANAGER_HOLDER = new ThreadLocal\u003c\u003e(); /** * Get an instance for {@code HintManager} from {@code ThreadLocal},if not exist,then create new one. * * @return hint manager */ public static HintManager get() { if (HINT_MANAGER_HOLDER.get() == null) { HINT_MANAGER_HOLDER.set(HintManager.getInstance()); } return HINT_MANAGER_HOLDER.get(); } /** * remove {@code HintManager} from {@code ThreadLocal}. */ public static void remove() { HINT_MANAGER_HOLDER.remove(); } } 用户执行 SQL 之后，DistSQL 解析引擎会首先识别出该 SQL 是读写分离 Hint 的 SQL，同时会提取出用户想要自动路由或者强制到主库的字段。之后它会采用 SetReadwriteSplittingHintExecutor 执行器去执行 SQL，从而将正确操作设置到 HintManager 中，进而实现强制路由主库的功能。\nDistSQL HINT 的使用 下表为大家展示了 DistSQL Hint 的相关语法。\n语句 说明 示例 set readwrite_splitting hint source = [auto / write] 针对当前连接，设置读写分离的路由策略（自动路由或强制到写库） set readwrite_splitting hint source = write set sharding hint database_value = yy 针对当前连接，设置 hint 仅对数据库分片有效，并添加分片值，yy：数据库分片值 set sharding hint database_value = 100 add sharding hint database_value xx = yy 针对当前连接，为表 xx 添加分片值 yy，xx：逻辑表名称，yy：数据库分片值 add sharding hint database_value t_order= 100 add sharding hint table_value xx = yy 针对当前连接，为表 xx 添加分片值 yy，xx：逻辑表名称，yy：表分片值 add sharding hint table_value t_order = 100 clear hint 针对当前连接，清除 hint 所有设置 clear hint clear [sharding hint / readwrite_splitting hint] 针对当前连接，清除 sharding 或 readwrite_splitting 的 hint 设置 clear readwrite_splitting hint show [sharding / readwrite_splitting] hint status 针对当前连接，查询 sharding 或 readwrite_splitting 的 hint 设置 show readwrite_splitting hint status 本文详细介绍了 Hint 使用的两种方式以及基本原理，相信通过本文，读者朋友们对 Hint 都有了一些基本了解了，大家可以根据自己的需求来选择使用合适的方式。如果在使用过程中遇到任何问题，或者有任何建议想法，都欢迎来社区反馈。\n","description":"","tags":null,"title":"ShardingSphere Hint 实用指南","uri":"/posts/my-first-post/"},{"categories":null,"content":"\u003c!DOCTYPE html\u003e 老婆生日终极狂欢夜PLUS版！ ❤️ 菜宝同志万岁万岁万万岁 ❤️ 皇家御赐土味大礼包 ✨ 御膳房神秘盲盒 ✨ 点击抽取今日御膳 🎁 皇家恩赐大奖 🎁 点击抽取帝王恩典 💌 钦定爱情圣旨 点击生成皇家契约 🎉🎂🍰🎈💐💃🕺🎊 生日快乐 🎉🎂🍰🎈💐💃🕺🎊 ","description":"","tags":null,"title":"Hugo Theme MemE","uri":"/posts/birthday/"}]
